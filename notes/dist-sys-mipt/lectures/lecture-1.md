## Lecture 1. Модель распределённой системы

Невозможно держать высоконагруженный сетевой сервис на одном сервере из-за огромных объёмов данных/вычислений и возможности отказов. Нужна распределённая система на множестве серверов

#### Типы распределённых систем

- **KV Store** - огромная распределённая хэш-таблица с типовыми операциями $Set \left( key, value \right)$ и $Get \left( key \right)$, со случайным доступом поверх последовательных дисков и упорядоченностью элементов (like `std::map`). Весь диапазон ключей шардируется некоторым способом, не обязательно хэш-функцией, и в пределах шардов ключи упорядочиваются. Пример: `Cassandra`, `BigTable`
- **File System** - распределённая система хранения файлов огромных размеров или количеств, не умещающихся на одной машине, с последовательным доступом к данным. Отличается от KV Store паттерном доступа (последовательный против случайного). Пример: `GoogleFS`, `Colossus`, `HDFS`
- **Coordination Service** - распределённая система для построения других распределённых систем. Предоставляет примитивы синхронизации и согласованности: выбор лидера, lease и т.д. Выполняет аналогичную роль что и атомик для concurrency. Пример: `ZooKeeper`, `Chubby`
- **Message Queue** - распределённый асинхронный транспорт данных для многих одновременных producer-ов и consumer-ов. Пример: `Apache Kafka`
- **Database** - таблицы/колонки, индексы, транзакции, только распределённо. Самый сложный тип распределённых систем. Пример: `YDB`, `Google Spanner`, `CockroachDB`

Прежде всего, это разные модели данных, которые имеют различия в построении. Также все эти системы могут быть нераспределёнными, например `LevelDB`, `RocksDB` как KV Store на диске одной машины

#### Причины использовать распределённые системы

- **Масштабируемость (Scalability)** - возможность свободно добавлять новые вычислительные ресурсы и/или дисковые ёмкости, или же расти *горизонтально*, когда операций/данных стало слишком много для текущего количества машин, адаптируясь к нагрузке и при этом используя стандартное оборудование (свобода от Vendor Lock)
- **Отказоустойчивость (Durability + Reliability + Fault-tolerance + Availability)** - машина/диск может сбоить или даже отказать, поэтому в большом кластере всегда есть машины на обслуживании. Необходимо переживать сбои узлов, проблемы сети, электросети и человеческие ошибки, автоматически (без помощи человека) адаптироваться и скрывать всё это от пользователя
  - **Durability (Надёжность хранения)** - успешно завершённые операции не будут потеряны даже при сбое системы. **Persistent state** - надёжная запись на диск, которой противопоставляется **Volatile state** - хранение в памяти
  - **Reliability (Надёжность работы)** - система выполняет свои функции корректно и стабильно в течение длительного времени. Думаем как предотвратить сбой узла (обслуживание оборудования, логи и мониторинги, тестирование, профилирование, стабилные версии библиотек и ОС и т.д.)
  - **Fault-tolerance (Устойчивость к сбоям)** - корректность работы системы не нарушается при отказе любого из компонентов (узлов, сети, дисков и т.п.). ~~Если~~ Когда сбой произойдёт, думаем как не пострадать. Достигается некоторой степенью **избыточности (Redundancy)** - увеличенным использованием компонента. Например отказ диска можно пережить используя *RAID-массив*
  - **Availability (Доступность)** - даже если часть компонентов недоступна, система продолжает отвечать на запросы в разумное время, возможно с ограничениями (например, только на чтение). $4/5$ девяток ($99.99\\%$ или $99.999\\%$)- сколько времени в год сервер должен быть доступен
- **Безопасность (Safity)** - нельзя доверять машине/группе машин, поскольку они могут оказаться злоумышленниками и обманывать остальные машины. Особенно важно в блокчейне. Довольно сложный аспект

Эти причины независимы. К примеру, отказоустойчивость может быть необходима там, где данные помещаются на одну машину (`ZooKeeper`). Аналогично с безопасностью (`Bitcoin`)

- **Согласованность (Consistency)** - ещё одно свойство распределённых систем, гарантировать в той или иной степени, что геораспределённые клиенты будут способны конкурентно взаимодейтсовать с системой и видеть непротиворечивые данные

Реализация вышеописанных свойств составляет ядро распределённой системы, и описывается простыми в постановке задачами, рассматриваемыми в курсе. Однако промышленные распределённые системы, помимо этого, обладают множеством других инженерных аспектов, таких как
- кэши
- авторизация
- конфигурирование
- логирование
- мониторинг
- рейтлимитинг

которые в курсе не рассмотрены

#### Описание модели мира

Как и при изучении "классических алгоритмов", для построения эффективных и полезных распределённых отказоустойчивых алгоритмов необходимо определить модель физического мира, которая отражает реальный мир без излишних сложных подробностей. Доказав корректность алгоритмов в этой модели, можно будет утверждать, что они верны и в реальном мире

Распределённая система в модели состоит из
- узлов
- сети (проводов и сообщений)
- клиентов

**Узел (node, process в старых статьях/книгах)** - это автономная единица вычислений и/или хранения, участвующая в совместной работе системы. Это машина, процесс, программная единица, например актор (таблетка в `YDB`), или даже сетевой сервис. Узлы взаимодействуют посредством отправки односторонних сообщений (message) без ожидания ответов (reply), используя для этого сеть

**Сеть** есть совокупность проводов. Для простоты, между каждой парой узлов есть прямой и надёжный провод (link, TCP), по которому движутся сообщения, и узлы видят только его. Это модель **Message Passing**

Цель такой простой модели предоставить не *стоимости*, а *гарантии*. Допустим, асинхронно отправили сообщение $m$ узлу $p$ - $Send \left( m, p \right)$. Как быстро оно дойдёт? Не моментально, даже неизвестна верхняя граница и дойдёт ли оно вообще, но возможно eventually на узле $p$ сработает обработчик сообщения $m$. Модель показывает, что нам стоит ожидать. В реальности вместо Response-ов применяется механизм Retry-ев, сети более сложные чем прямой провод, а скорость передачи ограничена физически (скорость света + среда/материал) и географически (удалённые узлы, локальность), что вносит задержки. Реализации систем должны это учитывать, и здесь возникают стоимости

Также есть **клиенты**, которые взаимодействуют с узлами по модели **Client-Server** - посылают запросы и ждут ответы. Есть несколько способов взаимодействия
- http api
- RPC - Remote Procedure Call
- client library - плюсовые обёртки над другими API + полезная функциональность, например `Asio`

Клиенты (почти) не знают об узлах. Они отправляют запросы на единый сетевой адрес, которые проксируются в конкретные узлы. Альтернативно балансировка может осуществляться в клиентской библиотеке

Клиент выполняет запрос $Set \left( k, X \right)$ и ждёт подтверждение, система надёжно сохраняет $X$ и посылает подтверждение. Если затем клиент сообщит об этом по внешнему каналу связи другому, второй при $Get \left( k \right)$ ожидает увидеть $X$, т.е. увидеть *наличие причинности*. Клиенты не отправляют сообщения в систему (явно), не видят сотни узлов, не наблюдают распределённость и в целом не думают о ней. Для них система выглядит как бесконечно ёмкий, бесконечно мощный, бесконечно устойчивый компьютер, и в коде это переменная с операциями над ней, которые разные клиенты выполняют конкурентно

Внутри системы удобно использовать модель Message Passing, которая описывает общение узлов, а снаружи разумнее модель **Shared Memory** (Consistency models), учитывающая клиентов. Благодаря использованию последней в алгоритмах, где разные клиенты рассматриваются как разные потоки в concurrency, переменная становится потокобезопасной, и от клиента удаётся скрыть распределённость

При наличии причинности система должна учитывать возможность внешней коммуникации при обеспечинии согласованности. Также возможны распределённые системы, которые заставляют пользователей передавать друг другу служебную информацию для обеспечения причинности по side-channel (за пределами системы). Пример: `Cassandra`, `Dynamo`

В реальности для реализации пересылки узлами друг другу сообщений могут использоваться более высокоуровневые абстракции, например RPC, которые ближе к клиент-сервер модели

#### Гарантии факта доставки сообщения

Мы говорим о прямом проводе, поскольку узлы наблюдают TCP, а не сложную сеть, и именно такой абстракцией TCP и является. Модели каналов представляют абстракции соединений с разными гарантиями на доставку сообщений, которые удобно использовать в алгоритмах. Они не допускают сообщений из воздуха и позволяют не думать о деталях реализации, хотя и отличаются в них

- **Fair-loss link** - модель честного канала: если узел $A$ будет бесконечно отправлять сообщения узлу $B$, то второй будет бесконечно их получать, то есть они не перестанут доходить. Однако, сообщения могут теряться, дублироваться и приходить в ином порядке. Предоставляет наименьшие гарантии. С такой абстракцией неудобно работать
- **Reliable (or perfect) link** - модель надёжного (или идеального) канала: каждое однократно отправленное сообщение eventually будет доставлено и ровно один раз, при условии, что узлы живы. Не гарантирует порядка. Реализуется поверх fair-loss. Пример: TCP - отслеживание дублей, повторная отправка до подтверждения, упорядочение на стороне доставки. Используется в литературе
- **FIFO link** - модель канала, гарантирующая порядок. Ортогональна предыдущим

Узел может перезагрузиться/подвиснуть, и соединение порвётся. Разрыв соединения не так просто обработать, что является отдельным инженерным вызовом. Поскольку неизвестно, выполнилась операция или нет, возможны два плохих сценария
- операция не выполнилась, и, если её не повторить, она потеряется
- операция выполнилась, и, если её повторить, и она не *идемпотентная*, система сломается

**Идемпотентность** - свойство операции в случае многократного выполнения с одним и тем же входом не менять результат первого применения

Так Reliable модель довольна сложна в реализации, поэтому будем работать в её модификации, где вместо физического провода соединение, которое либо гарантирует доставку, либо сообщит о разрыве

#### Гарантии времени доставки сообщения

- **Synchronous model (Cинхронная модель)** - время доставки сообщений ограничено сверху $delay \left( m \right) \le \delta$. Непрактична, поскольку в реальности система ненадёжна (разрывы, баги в коммутаторах, высокий трафик, обслуживание, человеческие ошибки и т.д.), и гарантия не может обеспечиваться постоянно
- **Asynchronous model (Асинхронная модель)** - время доставки сообщений не ограничено. В этой модели доказывается корректность алгоритмов, т.е. что система обладает свойством **Safity** - не уходит в некорректные состояния (не делает ничего плохого). Но такие алгоритмы/системы бесполезны, поскольку могут ничего не делать в принципе. Наиболее часто используется в курсе
- **Partially Synchronous model (Частично синхронная модель)** - асинхронная модель, которая в некоторый момент времени $t^\*$ становится синхронной (например, сеть перестала сбоить). Это может случаться периодически. Алгоритму неизвестен $t^\*$, но когда он настанет, алгоритм адаптируется и совершит прогресс. В этой модели доказывается полезность алгоритмов, т.е. что система обладает свойством **Liveness** - eventually совершает прогресс (делает что-то хорошее)

Говоря иначе, частично синхронная модель означает, что сеть работает непредсказуемо и не даёт никаких гарантий о времени доставки, но периодически сообщения доставляются. В зависимости от ситуации будем пользоваться асинхронной или частично синхронной моделью

#### Моделирование сети

- **Partition** - разбиение системы на два связных, полностью изолированных друг от друга кластера, а также их клиентов, в результате нарушений сети (отказов link-ов)
- **Split Brain** - ситуация, когда при partition кластеры думают, что они вся система, и продолжают работать независимо. Её нельзя допускать, поскольку узлами она ненаблюдаема до и невосстановима после починки сети - консистентность системы нарушится, и клиенты пронаблюдают наличие узлов. Вариант решения: одна из частей должна остановиться до починки (CAP теорема), или обе части дожны перестать работать на запись

Модель уже учитывает partition как случай бесконечно медленных link-ов на некотором срезе

#### Моделирование узлов

Для целей модели узел есть однопоточный автомат, который получает из сети сообщения и вызывает для них обработчики, меняет своё состояние и, возможно, отвечает сообщениями в сеть/систему. Многопоточность в реализации не влияет на свойства системы, но увеличивает производительность и удобство построения. Для доказательства теорем/алгоритмов можно думать, что все действия узла атомарны, что не повлияет на корректность реальных (неатомарных) систем

Скорость работы узлов произвольная (например может уменьшиться из-за начала сборки мусора/удаления большой структуры объектов). Узел может выйти из строя (Crash, "взорваться", отказ диска). Узел может перезагрузиться (Restart), что требует добавить персистентное состояние (диск). Также возможны Византийские отказы (Byzantine), когда узел ведёт себя непредсказуемым образом, например является злоумышленником и отвечает всем по разному. Последнее сильно усложняет алгоритмы (помимо количества отказов нужно закладывать процент возможных злоумышленников), но и учитывается лишь в безопасных системах (например, криптовалюты). При реализации $Crash \subset Restart \subset Byzantine$

В зависимости от алгоритма/архитектуры узлы могут быть как равноправными, так и иметь разные роли и даже менять их (например, лидер)

#### Время

**Время (time)** абсолютно для всех узлов, и каждое событие в системе имеет координату на общей оси времени. Время не влияет на Safity, но должно уважать его при учёте в Liveness. Однако узлы не имеют доступа ко времени, только к **часам (clock)**, которые отображают абсолютное время в **timestamp** $c \left( t \right) \to ts$. В идеале это тождество, но в действительности это оценка $Now \left( \right) \to ts$ от физического процесса в часовом механизме, например кварцевом (quartz, в ПК) или атомном (atomic, точные и дорогие, на специальных машинах)

Принципиально все использования часов сводятся к двум действиям
- $t_1 \lt t_2$ - сравнение timestamp-ов для упорядочивания событий (согласование порядка действий с внешней причинностью). Чуть сложнее
- $t_2 - t_1$ - вычисление интервала времени для установки timeout-а или организации failure-detection (время ожидания подтверждения доставки сообщения до узла, время отчёта лидера об его активности - heartbeat-ы)

#### Погрешности часов

Часы строят из объектов реального мира, поэтому они имеют погрешности, что необходимо учитывать в алгоритмах
- **Skew** - рассинхронизация часов на разных узлах. Проблема для упорядочивания, когда порядок по часам не соответствует порядку во времени. Пример: клиент $A$ выполнил $Set \left( key, X \right)$ на узле со временем $12:00$, после чего сообщил об этом клиенту $B$, который выполнил $Set \left( key, Y \right)$ на узле со временем $11:59$, но в системе остался $X$ - записи переупорядочились
- **Drift (дрейф, скитание)** - явление нестабильности скорости измерения времени часами, когда они начинают спешить или отставать. Проблема для точных интервалов. **ppm (parts per million, миллионные доли)** - на сколько микросекунд часы могут отклониться за одну секунду. У кварцевых часов ppm равен $20$ ($\approx 1.7$ секунд в сутки)

***Конец описания модели***

#### Синхронизация часов (невозможна)

Drift неизбежен, но можно ли устранить Skew сведя часы к одному показанию, не обязательно к "реальному" времени? Докажем, что это невозможно

Попытаемся синхронизировать два узла. На одном узле определим время $t_1 = Now \left( \right)$, запросим время $t_3$ у другого и при получении ответа снова узнаем время $t_2 = Now \left( \right)$. Предположим, сеть симметрична, тогда примерная задержка ответа $\delta \approx \frac{t_2 - t_1}{2}$, и на другом узле сейчас $t_3 + \delta$

Пусть время доставки сообщений $delay \left( m \right) \in \left[ \Delta - u, \Delta \right]$, где $u$ - uncertainty (неопределённость), и дрейфа нет. Синхронизировать часы означает выбрать для каждого узла некоторую поправку $O_k$ к текущему показанию его часов $SC_i \left( t \right) = C_i \left( t \right) + O_i$. Предположим, есть произвольный алгоритм, который после запуска за некоторое время даст $\left| SC_i \left( t \right) - SC_j \left( t \right) \right| \le \varepsilon$. Определим нижнюю оценку $\varepsilon$, т.е. насколько точно можно синхронизировать часы

Пусть мы управляем миром, в том числе работой сети (задержками) и начальным отклонением часов, иными словами, можем вообразить любые возможные исполнения (эта техника полезна при доказательстве алгоритмов). Построим два различных исполнения, в которых поправки вычисляются независимо и в общем случае должны различаться, но таких, что алгоритм не сможет различить исполнения и выберет в обоих случаях одинаковые поправки

$t_{i \to j}$ - время доставки сообщения от $i$ к $j$. Пусть $t_{1 \to 2} = \Delta$, $t_{2 \to 1} = \Delta - u$. На такой конфигурации запускаем алгоритм и вычисляем $\varepsilon$. Затем выполним операцию **shift** - оставим события на первом узле на тех же местах, но перевернём сеть, так что $t^{shift}_{1 \to 2} = \Delta - u$, $t^{shift}_{2 \to 1} = \Delta$, и переведём часы второго узла на $+u$, чтобы ответы остались такими же. Оба узла не видят разницы, значит в обоих случаях алгоритм выберет одинаковые поправки

```math
O^{shift}_1 = O_1, O^{shift}_2 = O_2
```

Так, для первого узла

```math
C^{shift}_1 \left( t \right) = C_1 \left( t \right) \Rightarrow C^{shift}_1 \left( t \right) + O_1 = C_1 \left( t \right) + O_1 \Rightarrow SC^{shift}_1 \left( t \right) = SC_1 \left( t \right)
```

для второго узла

```math
C^{shift}_2 \left( t \right) = C_2 \left( t \right) + u \Rightarrow C^{shift}_2 \left( t \right) + O_2 = C_2 \left( t \right) + O_2 + u \Rightarrow SC^{shift}_2 \left( t \right) = SC_2 \left( t \right) + u \Rightarrow SC^{shift}_2 \left( t \right) - SC_2 \left( t \right) = u
```

и поскольку после синхронизации разница между двумя часами может быть от $-\varepsilon$ до $\varepsilon$, то

```math
SC_2 \left( t \right) \in \left[ SC_1 \left( t \right) - \varepsilon , SC_1 \left( t \right) + \varepsilon \right], SC^{shift}_2 \left( t \right) \in \left[ SC_1 \left( t \right) - \varepsilon , SC_1 \left( t \right) + \varepsilon \right] \Rightarrow 2\varepsilon \ge u \Rightarrow \varepsilon \ge \frac{u}{2}
```

В случае n узлов

```math
\varepsilon \ge \left( 1 - \frac{1}{n} \right) u
```

Таким образом, даже в мире с идеальными часами без дрейфа для синхронизации часов нужно оценить асимметрию сети $u$, что невозможно без уже синхронизированных часов. Т.е. мы не можем узнать насколько наши часы синхронизированы. Мы можем лишь измерить время roundtrip-а - путишествия сообщения туда-обратно. Стоит отказаться от синхронизации времени, поскольку время это разделяемое между узлами состояние, которое не является когерентным. Конечно, алгоритмы используют время (таймауты), но от них не должно зависеть safety, только liveness

Всё становится лучше, если отказаться от асимметрии, например, синхронизировать одни часы по другим, но не обратно. Это применяется в GPS

#### Почему GPS синхронизирует часы

Из соображений геометрии нужны три спутника для точного определения положения в пространстве. Достаточно пересечь сферы расстояний и взять точку на поверхности Земли. Это выливается в систему трёх неизвестных $\left| X - X_i \right| = d_i$. Однако расстояние до спутников измеряется при помощи локальных часов, которые имеют погрешность $\delta$ относительно времени GPS, так что система на самом деле выглядит так $\left| X - X_i \right| = \upsilon * \left( \Delta t_i + \delta \right)$, что складывается в четыре неизвестных и четыре уравнения - *навигационные уравнения GPS*. Поэтому синхронизация часов является побочным результатом. Чтобы это работало, часы на спутниках имеют огромную точность (у них очень маленький drift, избыточность кристаллов, и их постоянно синхронизируют)

#### TrueTime

**Google.TrueTime** - локальный сервис для синхронизации часов, придуманный для использования в `Google Cloud Spanner`. Вызов $TT.Now \left( \right)$ возвращает диапазон $\left[ e \left( arliest \right) , l \left( atest \right) \right]$, в пределах которого находится настоящее время. Гарантируется, что $\left[ t_0, t_1 \right] \cup \left[ e, l \right] \ne \emptyset$, где $t_0$ - время запроса и $t_1$ - время получения ответа, и что $l - e \le 6ms$

Для этого на узле хранится неизменное локальное состояние, обновляемое демоном сервиса ТТ каждые $30$ секунд запросом к одному из *тайм мастеров* кластера (в пределах одного датацентра). ppm устанавливается в $200$, что заведомо больше, чем в часах узла. При запросе $TT.Now \left( \right)$ сервис берёт копию локального состояния и используя монотонные часы сдвигает границы, имитируя дрейф, нижнюю c $ppm = -200$, а верхнюю с $ppm = 200$. Так интервал с настоящим дрейфом заведомо попадает в вычисленный

Тайм мастеров два вида, либо узел с *GPS антеной* (для синхронизации своих часов по GPS), либо *Armagedon Master* с атомными часами. Они независимы и имеют разные сценарии отказа

TrueTime используется для замены долгого межконтинентального общения ($\sim10-100ms$) ожиданием не дольше $6ms$ (в настоящее время сильно меньше). [Применение описано в 6 семинаре](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-6.md#TrueTime)

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[Семинар 1 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-1.md)
