## Lecture 3. Atomic Broadcast, State Machine Replication, Consensus

#### Репликация регистра = репликация множества

Полезное свойство линеаризуемости состоит в том, что история взаимодействия с двумя линеаризуемыми объектами в совокупности также является линеаризуемой, в отличие от, например, *SC*. Благодаря ему мы и можем свести задачу репликации многих значений к репликации регистра, а далее произвести обратный переход

#### ABD и CAS

Построенный в лекции 2 [алгоритм](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-2.md#задача-репликации-регистра) называется **ABD (Attiya, Bar-Noy, Dolev)**. Он позволяет работать только с "одной ячейкой", т.е. с данными фиксированного объёма. Для наших целей его недостаточно
- он не может варьировать объём изменяемых данных ("атомарно изменять несколько значений"), в том числе затрагивать несколько узлов
- read/write мало для транзакций (TODO: (?) решим в лекции 11)
- он не предусматривает реконфигурации (решим в [лекции 8](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-8.md#rsm-в-проде))

В действительности он достаточен для некоторых систем

Хотим операцию $CAS \left \lparen expected, desired \right \rparen : bool$. Для согласования значения разрозненных реплик мы использовали $ts$ для линеаризации. С ними запоздавшую запись легко встроить в глобальную историю - просто отбросить её, будто она была раньше и уже перетёрта. Аналогичный подход не работает с CAS. Рассмотрим сценарий $x \overset{CAS_{t_2} \left \lparen y, z \right \rparen}{\longrightarrow} x  \overset{CAS_{t_1} \left \lparen x, y \right \rparen}{\longrightarrow} y$. В нём из-за запоздавшей операции вместо $z$ получили $y$, т.е. порядки $rt$ и $ts$ отличаются. Откладывать пришедшие CAS-ы и выстраивать их по $ts$ не выйдет, поскольку $ts$ не строго монотонные, т.е. нельзя понять, сколько ещё нужно дождаться CAS-ов до применения отложенного

Таким образом, порядок $ts$, задающий порядок применения операций, не соответствует произвольному порядку получения операций репликами, и неясно, когда команда готова к применению. Необходим механизм транспорта команд в одинаковом для всех узлов порядке, который и будет порядком применения. Придумаем более удобную абстракцию без $ts$ и с произвольными операциями

#### Totally Ordered Broadcast

**Totally Ordered Broadcast (Atomic Broadcast)** - примитив/протокол синхронизации или транспорт команд для решения задачи репликации. Имеет единственную операцию $` A-BCast \left \lparen m \right \rparen `$ - посредством некоторого распределённого алгоритма доставить *всем* узлам, в том числе себе, сообщение $m$, после получения которого на каждом узле eventually сработает обработчик $` A-Deliver \left \lparen m \right \rparen `$. Хотим, чтобы при рассылке каждый несбойный узел получил сообщение, и чтобы на каждом узле обработчики вызывались в одинаковом порядке, независимо от порядка отправки. Порядок обработки и будет заменой $ts$. При этом операцию может стартовать любой узел

Формально хотим три свойства
- **Validity** - если несбойный узел стартовал $AB$, то eventually на этом узле сработает $AD$
- **Agreement** - если несбойный узел стартовал $AB$, то eventually на остальных несбойных узлах сработает $AD$
- **Total Order** - есть общий порядок обработки. Физически сообщения могут доставляться в разном порядке, но со временем на каждом узле будет формироваться одинаковый префикс логически доставленных, т.е. обработанных сообщений. Должно выполняться и для сбойных узлов
- **(Integrity)** - сообщения не возникают из воздуха и доставляются ровно один раз. Простое свойство, которое опускаем

Несбойность означает, что если узел бесконечно долго работает без отказа, то посланное сообщение не может бесконечно долго быть необработанным им. Eventually оно дойдёт и будет обработано - liveness. Т.о., первые два свойства говорят, что каждое отправленное сообщение будет обработано, и что несбойные узлы обработают одинаковый набор сообщений, а третье, что порядок обработки этих сообщений будет единым

#### Replicated State Machine

Рассмотрим использование Atomic Broadcast в задаче **репликации произвольного автомата (State Machine Replication)**. Имеем реплицированный объект $s$ с единственной мета-операцией $s.Apply \left \lparen cmd \right \rparen \to \left \lparen s', r \right \rparen $, которая применяет к объекту произвольную команду $cmd$ с неизвестной семантикой, переводя его в новое состояние, и что-то возвращает. Реплики хранят копию объекта в начальном состоянии. Очередная команда $c$ попадает на некоторый узел, который становится координатором и инициирует $` A-BCast \left \lparen c \right \rparen `$. Eventually на каждом узле, внутри обработчика $` A-Deliver \left \lparen c \right \rparen `$ происходит вызов $s.Apply \left \lparen c \right \rparen $. Когда это случится на координаторе, он (сразу) возвращает результат клиенту, и операция считается завершённой. Благодаря Atomic Broadcast изменения на всех репликах согласованы

Получили **Replicated State Machine (RSM)** - отказоустойчивый распределённый автомат, который снаружи выглядит как один узел, применяющий операции в порядке AB. Он позволяет изменять сразу множество значений, на одном узле просто через $Apply$ и на нескольких посредством транзакций. RSM активно применяется в проде (например в `YDB` система вместо ненадёжных узлов состоит целиком из RSM - *Tablets*, название пошло из `BigTable`). Это высокоуровневая схема репликации, сложности сокрыты в реализации AB

В описанном RSM координатор может вернуть результат раньше, чем любой другой узел увидит рассылку. В действительности такое невозможно, поскольку в реализации Atomic Broadcast будет использоваться алгоритм кворумов

#### Докозательство линеаризуемости

Необходимо найти линеаризацию для любой конкурентной истории. Предположим, это порядок применения команд в Atomic Broadcast. Тогда нужно доказать, что он уважает $rt$

От противного. Рассмотрим две операции $` o_1, o_2: \hspace{0.5em} o_1 \prec_{rt} o_2 `$, и для координатора $` o_1 `$

$$ A-Deliver \left \lparen o_1 \right \rparen \prec_{rt} A-BCast \left \lparen o_2 \right \rparen $$

Пусть на некоторой реплике, а значит на всех

$$ A-Deliver \left \lparen o_2 \right \rparen \prec_{rt} A-Deliver \left \lparen o_1 \right \rparen $$

Тогда для координатора $o_1$

$$ A-Deliver \left \lparen o_2 \right \rparen \prec_{rt} A-Deliver \left \lparen o_1 \right \rparen \prec_{rt} A-BCast \left \lparen o_2 \right \rparen \Rightarrow A-Deliver \left \lparen o_2 \right \rparen \prec_{rt} A-BCast \left \lparen o_2 \right \rparen $$

Противоречие, ч.т.д.

#### И целого AB мало

Даже для реализации RSM мало лишь построить AB
1. Для консистентности истории изменений реплик мета-операция $Apply$ должна быть детерминированной. Здесь могут повлиять локальные для узла ГПСЧ, время (часы), операции с плавающей точкой (зависят от установок процессора), хэш таблицы (сид/соль может быть на основе адреса + ASLR). Также могут повлиять разные версии библиотек, в том числе стандартных (например, стали использовать другой компаратор в сортировке), о чём нужно помнить при реконфигурации
2. Когда клиент не получает ответа на запрос, он по таймауту должен сделать Retry. Однако неясно, применилась ли операция, или, например, координатор умер до рассылки. Если первое, получим *At-Least-Once* семантику и, если операция не идемпотентна, неконсистентное состояние системы. Нужна *Exactly-Once* семантика через сквозной порядок операций в системе при поддержке и AB, и клиентов
3. RSM по построению реплицирует состояние объёма одной машины. Для больших объёмов нужны несколько RSM, по одной на range. Допустим, есть две операции, затрагивающие одинаковые диапазоны данных в нескольких RSM. Если операции упорядочены, то по свойству линеаризуемости всё хорошо. Конкурентные же операции на разных RSM могут выполниться в разном порядке. В системе нет общего ТО или "атомарности" операций. Нужны *распределённые транзакции*
4. Нельзя пускать read-only операции в обход AB ради ускорения, поскольку, например, можно прочесть с отстающей реплики и потерять линеаризуемость

#### Реализация Atomic Broadcast

Задача AB является бесконечной в том смысле, что обеспечивает применение всё возникающих команд. Упростим её до применения одной команды. Для этого нужно решить две независимые задачи
- **Reliable Broadcast** *(Validity + Agreement)* - гарантированная доставка сообщения всем несбойным узлам в произвольном порядке
- **Consensus** *(TO)* - согласование этими узлами применения команды, откуда возникает порядок

#### Reliable Broadcast

Нельзя просто отправлять узлам сообщение поочерёдно, поскольку в процессе координатор может отказать, нарушив *Agreement*. Снабдим сообщение меткой, и пусть каждый узел при получении его в первый раз ретрансмитит остальным узлам. В действительности применяются более практичные протоколы, например *Gossip*

#### Consensus

Пусть есть три узла, и каждому из них на вход дали своё значение $v_i$. Каждый узел должен реализовать синхронный отказоустойчивый алгоритм $` Propose \left \lparen v_i \right \rparen \to v^* `$, в вызове которого предложить своё значение $v_i$ и совместно с другими узлами выбрать общее значение $` v^* `$. *Свойства (гарантии) алгоритма*
- **Validity** - $` v^* `$ одно из предложенных $v_i$
- **Agreement** - все узлы получат один и тот же $` v^* `$
- **Termination** - алгоритм должен eventually завершаться

Consensus является одним шагом в бесконечной задаче AB, поэтому Consensus $` \sim `$ AB, и можно одно выразить через другое

#### AB через Consensus

Выразим AB через серию консенсусов $C^1 \to C^2 \to C^3$, где очередным консенсусом $C^n$ выбирается пачка сообщений $B^n$, которую нужно обработать (или, в терминах AB, доставить)

Пусть имеем Reliable Broadcast и умеем решать Consensus. Каждый узел имеет два изначально пустых локальных множества сообщений, полученных $R_i$ и обработанных $A_i$. При очередном сообщении координатор выполняет $` A-BCast \left \lparen m \right \rparen : \hspace{0.5em} R-BCast \left \lparen m \right \rparen `$ - разослать сообщение $m$ всем узлам. Каждый узел при получении сообщения запускает обработчик $` R-Deliver \left \lparen m \right \rparen : \hspace{0.5em} R_i \gets R_i \cup \{ m \} `$ - поместить сообщение $m$ в $R_i$. Сообщения из $R_i$ предстоит обработать и поместить в $A_i$. Для этого на каждом узле запущен фоновый процесс со следующим алгоритмом, состоящим из раундов

$r \gets 0$\
$while \hspace{0.5em} R_i - A_i \neq \emptyset $\
$\hspace{2em} r \gets r + 1$\
$\hspace{2em} B^r_i \gets R_i - A_i$\
$\hspace{2em} B^r \gets Propose \left \lparen r, B^r_i \right \rparen $\
$\hspace{2em} A-Deliver \left \lparen B^r \right \rparen $\
$\hspace{2em} A_i \gets A_i \cup B^r$

Предлагается сразу пачка сообщений, поскольку старое сообщение может бесконечно долго проигрывать более новым и никогда не быть доставленым (голодание сообщения, таймаут клиента). Альтернатива, предлагать самое старое сообщение, тогда в крайнем случае все узлы вместе его предложат и гарантированно выберут

Порядком обработки становится порядок выбора консенсусами пачек сообщений $B^r$, внутри которых порядок определяется порядком прихода сообщений на предложивший пачку узел. То есть $R_i$ скорее очередь. Обработчик $` A-Deliver `$ является для алгоритма внешним с произвольной семантикой

На практике множества $R_i$ и $A_i$ очищают, но поскольку сообщения могут приходить на узлы позже, чем их выбрали консенсусом от другого узла, это нужно делать аккуратно

#### Итог

В итоге свели RSM к AB, а его к задаче консенсуса, и отказоустойчивость RSM целиком зависит от отказоустойчивости реализации Consensus. В реальности RSM не строят через AB, который строят через Consensus как из отдельных компонентов. Реализуют единый алгоритм репликации на этих основах

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Семинар 2](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-2.md)
[Семинар 3 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-3.md)
