## Lecture 9. Exabyte-scale KV & DFS

До этого мы изучали отказоустойчивость и рассматривали слои Local Storage (рестарты, LSM) и Replication (отказы, Multi-Paxos, RSM). Теперь займёмся вопросом *Scalability* и рассмотрением слоя *Distribution*. И если две первые задачи почти целиком *алгоритмические*, за исключением инженерного вопроса учёта файловых систем устройства хранения и ОС для корректной работы LS при рестартах (см. [семинар 8](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-8.md)), задача масштабируемости имеет *сложности другого рода* - поиск узких мест и их устранение изменением дизайна

Научимся масштабировать KV и DFS до объёмов экзабайт данных. Есть *две причины*, почему KV и DFS
- *Историческая.* Изначально было два типа обработки данных - **batch** и **realtime**. Для batch обработки, например, `MapReduce`, требовалось хранить огромные объёмы данных, т.е. использовать DFS. Для realtime обработки, например, корзины `Amazon`, подходили KV системы
- *Актуальная.* Крупным компаниям с миллионами машин удобно хранить огромные объёмы своих данных в DFS (например, `Google` и `Colossus`). KV же используются для реализации DDB поверх них

Т.о. и KV, и DFS использовались раньше и сейчас, и задача их масштабирования является главной

#### KV

Представим данные в виде **Big table** (большой таблицы), где строку занимает одна пара $` \left \lparen key, value \right \rparen `$. KV как многопользовательская система управляет множеством таких таблиц. Big table растёт бесконечно и не помещается в одну машину, поэтому её нужно шардировать - разбить на **Tablets** (ранее range), каждый один RSM.

Аккуратно выберем размер. *Таблетки растут*, поэтому не могут быть объёма близкого к целой машине. Более того, какие-то таблетки будут *горячими* и расти очень активно, что можно узнать только при работе системы. Значит нужно уметь *динамически разбивать* таблетки на несколько и *балансировать* их (распределять по узлам). Чтобы это было легко делать, выберем *небольшой размер* порядка $` \sim10-100MB `$. Например, в `CockroachDB`, open-source аналоге `Spanner`, используют $` 64MB `$

Так одна машина обслуживает сразу несколько таблеток и может не справляться с нагрузкой при большом количестве запросов или данных всех таблеток или конкретной. Нагрузку/таблеты нужно балансировать. Этим занимается **Shard Manager** - один актор, RSM, отслеживающий распределение таблеток по узлам, их здоровье и нагрузку, разбивающий и двигающий таблетки (**Hive** в `YDB`). Также Shard Manager может следить за лидерами таблеток и в случае его смерти выбрать нового, централизованно решая проблему определения наиболее полного лога и выбора очередного $n$. Так, например, делают в `ZippyDB`

Для доступа к таблетке клиент узнаёт у Shard Manager-а её лидера - Tablet Server. Таблеток миллиарды (при экзабайтах данных), и объём их метаданных велик. С одной стороны, Shard Manager не способен их вместить, и с другой, ему тяжело обслуживать всех клиентов одному. Наивно, можно взять несколько Shard Manager-ов и статически по хэшу распределить по ним таблетки. Поместим все метаданные в служебную Big table, для таблеток которой будем поддерживать маленькое отображение, возможно, помещающееся в одну таблетку, которая размещена в отдельном RSM или системе (например, `Chubby` в `BigTable` или `Apache ZooKeeper` в `HBase` - open-source аналог `BigTable`). Чтобы не нагружать обслуживающий RSM, клиенты кэшируют таблетки (лидеров) метаданных, с данными таблеток которых работают, а также служебные таблетки на уровнях выше

TODO: Такое кэширование работает благодаря локальности обращений?

Получившаяся схема идентична системе страниц виртуальной памяти, а верхняя таблетка аналог регистра CR3. Поэтому при увеличении масштабов достаточно добавить ещё один слой отображения

#### DFS

Вспомним дизайн DFS из [семинара 4](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-4.md#дизайны-chunk-и-meta-store). В нём Meta Store на RSM является узким местом, которое не позволяет заводить файлы бесконечных размеров и/или количеств. И то, и другое приходится ограничивать, ровно как и масштабируемость. По опыту `Facebook`, такой дизайн позволяет хранить до $` \sim 10PB `$ данных, сверх чего метаданных станет слишком много для хранения в одном RSM. Шардировать метаданные сложно и неудобно, поскольку иерархия файлов, дерево, является логически цельным объектом, и допустимо лишь как-то разнести Inode

Возможное, но ограниченное решение, разбить иерархию файлов на поддеревья (**Namespaces**, Федерация в `HDFS`), полагая, что операций, затрагивающих разные поддеревья, не бывает. Более разумно хранить метаданные DFS в шардированном KV Store. Директория это множество составных ключей $` \left \lparen dir\_id , <subdirname\_or\_filename> \right \rparen `$, файл это множество составных ключей $` \left \lparen file\_id , block\_id \right \rparen `$, блок это список чанков $` disk\_id `$ и множество ключей $` \left \lparen disk\_id, block\_id \right \rparen `$. Такая структура используется в `Tectonic` - exascale DFS от `Facebook`. Следствие такого подхода, нельзя легко посчитать размер поддерева, нужен рекурсивный обход и много запросов. Другое следствие, разные файлы одной директории распределяются по разным таблеткам, что позволяет распределить нагрузку по KV при параллельном считывании директории

Шардировав метаданные, мы потеряли атомарность любых операций. Теперь нельзя выполнять, например, перемещения файлов. *Частичное решение*, разместить все ключи с одинаковым префиксом в одной таблетке, тогда вернётся атомарность части операций над одним файлом и одной директорией. *Полное решение* - транзакции

#### LSM over DFS

В KV слой Local Storage (LSM) есть на каждом узле и отвечает за контроль рестартов и случайный доступ. Чтобы переживать отказы узлов, поверх него есть слой Replication. Однако можно наоборот - построить абстракцию отказоустойчивого диска и случайный доступ поверх него. Это **LSM over DFS**. Единый LSM хранит Log и SSTables каждой таблетки в DFS в виде файлов, а MemTable хранится в памяти единственного обслуживающего узла - Tablet Server

Такой подход имеет ряд преимуществ. Во-первых, теперь хранилище реплицирует иммутабельные данные, что намного проще и эффективнее. Они просто добавляются в G-Set, где можно использовать Erasure codes и сократить объём физически хранимых данных, что при экзабайтах данных позволяет сэкономить большие деньги. Во-вторых, мы разделили хранение данных и обслуживание операций и системы. Стало намного проще управлять порядком команд, транзакциями и таблетками, в том числе балансировать их, поскольку теперь не нужно задействовать данные/диски узлов

Поскольку Tablet Server один, необходимо уметь заменять его, снова с учётом неотличимости отказа от зависания/задержек. Нужно применить **Fencing**, т.е. чтобы очередной Server на уровне эпох понимал своё место относительно других, что осуществимо лишь на уровне DFS. Добавим Lease. Пусть в начале работы с файлом, при его открытии, DFS запоминает клиента, либо же выдаёт ему идентификатор, и при повторном открытии кем-то ещё, предыдущий клиент или идентификатор забывается и игнорируется

Использование DFS вместо Local Storage приводит к появлению дополнительных сетевых вызовов. Чтобы нивелировать это, нужно научить KV и DFS работать в тандеме, чтобы менеджер узлов правильно их распределял, и запись в DFS оказывалась локальной записью

#### Циклическая зависимость. KV over DFS over KV

Мы построили KV через DFS, который построили через KV, получив таким образом цикл. `Google` утверждает, что в DFS объём метаданных в $` \sim 10000 `$ раз меньше, чем объём данных. Воспользуемся идеей **Bootstrapping**. Пусть в цикле DFS $\to$ KV $\to$ DFS, очередной DFS будет аналогичен предыдущему и делить с ним узлы Chunk Store. Тогда объём данных в DFS уровнем ниже будет уменьшен в $10000$ раз. Продолжим делать так, пока объём метаданных не станет равен $` \sim 100-1000KB `$, и тогда, вместо KV, воспользуемся просто RSM/Таблеткой (или Chubby, как описано [выше](#kv)). Такая схема является автоматически масштабируемой. Когда RSM переполнен, можно заменить его ещё одним KV over DFS. Такой дизайн используется в `Colossus` от `Google`. Как альтернативный, но не масштабируемый вариант, можно просто использовать KV предыдущего дизайна

Разумеется, такой дизайн не работает из-за большого оверхэда. Простой $Append$ потребовал бы целое множество записей в Chunk Store и запись в RSM. Хотелось бы иметь возможность дописывать в конец чанков, не задействуя Meta Store, т.е. чтобы Chunk Store допускал изменение чанков. Для этого сделаем чанки **Single-Writer**, и в случае отказа или смены писателя, чанк закрывается

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 8](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-8.md)
