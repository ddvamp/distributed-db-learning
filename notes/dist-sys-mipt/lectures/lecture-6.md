## Lecture 6. Multi-Paxos

#### Replicated log

Цель - построить RSM. Мы сделали это через Atomic Broadcast, который реализовали через Reliable Broadcast + Consensus, где последний решается через Single-Decree Paxos. В действительности такая декопмозиция слишком избыточна и неэффективна, поэтому заменим её более практичной - алгоритмом **Multi-Paxos**. Multi-Paxos реализует Atomic Broadcast, т.е. позволяет проводить серию консенсусов $` C_1 \to C_2 \to C_3 `$, каждый из которых выбирает очередную команду в RSM. Однако сменим терминологию, и вместо Atomic Broadcast-а придумаем его промышленный аналог - алгоритм репликации лога, при помощи которого построим RSM

**Replicated log** - append-only структура данных, состоящая из независимых нумерованных слотов, хранящих произвольные наборы байт. Каждая реплика RSM имеет свою копию лога и помещает в него команды пользователей. В процессе заполнения копии лога отличаются и даже могут хранить разные команды в одном слоте, но eventually они синхронизируются при помощи консенсуса. Согласованный префикс копии лога представляет собой материализацию порядка применения команд (TO в AB). На случай рестартов лог надёжно хранится на диске

Multi-Paxos, в отличие от других алгоритмов, не имеет канонического описания. Статья *The Part-Time Parliament* не определяет количество типов сообщений, следовательно неясно как на них реагировать, а также многие другие детали. По сути это конструктор или framework для построения своей реализации алгоритма путём заполнения пробелов в конструкции из статьи, главное не нарушить при этом safity. Чтобы построить что-то разумное и эффективное достаточно понимания Single-Decree Paxos

#### Построение лога

Произвольный автомат как чёрный ящик использует лог. Построим его копию. Каждая реплика имеет изначально пустую копию лога и может получать команды. Состояния копий согласуются при помощи Multi-Paxos. Когда реплика получает команду, она ищет в логе очередной свободный слот и пытается поместить команду туда, конкурируя за него с другими репликами и их командами. Каждый слот независим и связан с отдельной задачей консенсуса, решаемой для него

Реплика, чтобы поместить команду в слот, становится Proposer-ом и инициирует для слота консенсус, отправляя всем репликам $` Prepare \left \lparen k, n \right \rparen `$, где $k$ - slot number, посылается в каждом сообщении протокола, $n$ - Ballot number. Если консенсус выбрал другую команду, реплика повторяет со следующим свободным слотом. Когда реплика получает $` Prepare \left \lparen k, n \right \rparen `$, она начинает выполнять роль Acceptor-а на слоте $k$, поэтому удобно также хранить в слоте состояние Acceptor-а $` \left \lparen n_p, \left \lparen n_a, v_a \right \rparen \right \rparen `$. Реплика может одновременно быть и Proposer-ом для нескольких команд, и Acceptor-ом для нескольких слотов, т.е. иметь множество конкурентных задач в обоих ролях

#### Нюансы заполнения слотов

*Физические состояния слота в конкретной копии лога*
- **empty** - команды нет, слот ещё не использован
- **accepted** - команда есть (отданный $Vote$), но может измениться
- **committed (или chosen)** - команда зафиксирована (univalent config) и не изменится

Переход в состояние committed не означает, что команда может быть применена к RSM, поскольку более ранние слоты могут быть в другом состоянии, и позже получить команды, влияющие на текущую. Команду можно будет применить как только перед ней образуется committed префикс, т.е. примут команды из всех предшествующих слотов. Это и означает eventually одинаковый лог

Поскольку слоты независимы, и с ними можно работать параллельно, команды при параллельной обработке могут фиксироваться не в том порядке, в котором приходят на реплику. Например, слот первой команды оказался занят, а вторая была принята. Это неважно при отсутствии $rt$ ordering-а между ними

В логе возможны временные пропуски, когда для последующего слота $Prepare$ приходит быстрее, например от реплики, в логе которой больше команд. Для пропущенного слота $Prepare$ может в принципе не прийти. К примеру, где-то в сети маршрутизатор оказался перегружен, и в это время Proposer собрал кворум и отменил фьючу. Пропуск можно закрыть попыткой записать в слот команду, либо успешно, либо получив результат консенсуса. Если команды нет, нужно использовать метакоманду $nop$, возможно "испортив" слот

Аналогичным образом определяется очередной свободный слот - просто инициируется консенсус. Нет специальных случаев для обработки. Благодаря консенсусу это и многие другие вещи выполняются автоматически

На практике, реплики собирают команды в пачку, например, ожидая некоторую долю секунды, определяемую конфигом, и предлагают на консенсус сразу batch. Подробности в [лекции 8](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-8.md)

#### Эффективность и выбор лидера

Описанное решение получилось крайне неэффективным из-за конкуренции за каждый отдельный слот. Команды могут голодать, а реплики биться в livelock-е, даже возможно, предлагая одно и то же значение, к которому они сошлись. По теореме FLP нужно использовать время + рандомизацию, но, в отличие от Single-Deecre Paxos, Exponential Backoff здесь неэффективен. Можно проще и лучше

Применим детектор сбоев **выбор лидера (Leader Election)** $` \Omega \left \lparen t \right \rparen \to P `$ - **Leader (Distingushed Proposer)**. Так мы сократим количество Proposer-ов до одного и избавимся от конкуренции. Реплика, определив своего лидера, отправляет ему команды/клиентов, и тот отвечает за формирование порядка применения команд. При этом у конкретной реплики лидер может меняться (из-за отказов/задержек в сети), а разные реплики могут считать лидером разные узлы. И теперь, вместо множественных попыток преодолеть FLP для каждой отдельной команды, мы делаем это многократно реже при выборе лидера

Paxos, разделяя свойства safety и liveness, очень сильно упрощает рассуждения о лидере. В отличие от алгоритма репликации регистра, алгоритм детектора может быть абсолютно любым, использовать время и рандомизацию, давать неконсистентные ответы и даже назначать лидером саму реплику. Всё это благодаря тому, что лидер(ы) использует консенсус (Paxos) для принятия решений. Изначально лидером является каждая реплика, но если алгоритм разумен и eventually выбирает единственного лидера, получим оптимальный уровень liveness. Лидер в некотором смысле узкое место, однако алгоритм способен переживать его смерть без потери safety, но, возможно, с ухудшением liveness

#### Алгоритм выбора лидера

Каждая реплика в фоне через равные промежутки времени $` \Delta `$ (по монотонным часам) отправляет остальным $` Heartbeat \left \lparen id \right \rparen `$, где $id$ - постоянный уникальный идентификатор реплики. По умолчанию реплика считает лидером себя, но если до неё доходят $Heartbeat$, она выбирает лидера по наибольшему $id$. Лидер выбирается на ограниченное время, например, $30с$. Если за это время от лидера не пришло ни одного $Heartbeat$, он забывается. В таком алгоритме периодически будет несколько лидеров, что не влияет на safety

В действительности, в случае шардирования узлы могут поддерживать сразу несколько RSM для различных range, и, чтобы не перегружать сеть, $Heartbeat$ от разных RSM узла склеивают или в целом реализуют на уровне машины, а не RSM

Такой алгоритм имеет недостаток. Пусть случился partition, и некоторый узел, который долго не участвовал в принятии команд, после восстановления стал лидером. Он начнёт предлагать команды в занятые слоты, получая отказы и последовательно восстанавливая лог. Это может остановить прогресс системы на длительное время. Решением является выбор лидера с наиболее актуальной версией лога, т.е. с наибольшим зафиксированным префиксом

#### Оптимизации

Рассмотрим две *оптимизации*
- **кэширование лидера**
- **pipelining**

*Кэширование лидера.* Построенному алгоритму нужно 4RTT на ответ: общение клиент-реплика, общение реплика-лидер, и две фазы Paxos. Это довольно медленно, и можно лучше. Лидер в проде живёт недели, поэтому его можно кэшировать, и сократить запрос до 3RTT. Когда клиент приходит не к лидеру, ему отвечают $` Redirect \left \lparen id \right \rparen `$, он запоминает лидера и начинает слать запросы ему. Такая схема не требует отслеживания и обработки смены лидера

*Pipelining.* Представим, что лидер заполняет лог последовательно и видит ряд сообщений $` Prepare_1 \to Promise_1 \to Accept_1 \to Accepted_1 \to Prepare_2 \to Promise_2 \to Accept_2 \to Accepted_2 \to \dots `$ Здесь можно заметить *две вещи*
- фазы $` Accept_{k} `$ и $` Prepare_{k+1} `$ соседних команд независимы и могут иметь одинаковый $n$
- $Prepare$ можно делать без команды, заранее "прогревая" слот

Так $` Accept_{k} + Prepare_{k+1} `$ можно склеить в одно сообщение, как и ответы, сэкономив на сети и на записи на диск. На старте или при неудаче любой из фаз начинаем с одиночного $Prepare$

Заметим, что прогревать можно сколько угодно слотов вперёд, и заменим $Prepare$ на $` Prepare^{+} \left \lparen k, n \right \rparen `$ - захватить все слоты лога, начиная с позиции $k$, если они все пустые, иначе только слот $k$ (поведение может быть и другим). При малом $n$ получает отказ. Пустые, поскольку иначе каждый слот с $Vote$ требовал бы отослать его лидеру. Acceptor запоминает Proposer-а и $k$, пока лог не перезахватят (TODO: полагаю, Acceptor-у нужен $k$, чтобы не допускать записи в предыдущие слоты)

В итоге получили 2 RTT с возможностью параллельной обработки

#### Интуиция для Single-Decree Paxos

Когда выбран новый лидер, он в $` Prepare^{+} `$ заявляет о себе, а реплики присягают ему. $n$ - эпоха, королевский номер или номер заседания, $` n \gt n_p `$ означает отвержение старого лидера, а $` n_p \gets n `$ суть признание нового. На быстром пути выполняется лишь репликация команд, осведомление реплик о "приказах" лидера

Таким образом Single-Decree Paxos нужен не для выбора очередного сообщения, а для избрания лидера, что происходит достаточно редко, и только когда появляются проблемы с сетью и/или узлами

#### Открытые вопросы

Приведённого описания недостаточно для реализации алгоритма. Возникают вопросы
- как Proposer-у реагировать на отказ $` Prepare^{+} `$ или $Accept$
- как Acceptor реагирует на $` Prepare^{+} `$
- как удерживать команды на случай отказа
- как реплике понять, что команда committed, что префикс committed
- как лидер посылает ответ клиенту
- exactly once семантика для клиента
- как понять, что лидерство потеряно, т.е. предотвратить борьбу лидеров
- как выбираются $n$
- как новый лидер выбирает стартовый $n$
- и т.д.

Алгоритм не отвечает на них, и именно поэтому Multi-Paxos имеет массу вариаций

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Семинар 5](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-5.md)
[Семинар 6 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-6.md)
