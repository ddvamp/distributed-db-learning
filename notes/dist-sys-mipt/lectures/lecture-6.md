## Lecture 6. Multi-Paxos

Цель - построить RSM. Мы сделали это через A-BCast, который реализовали через R-BCast + Consensus, где последний есть реализация Single-Decree Paxos. Такая декопмозиция слишком избыточна и неэффективна в реальности, поэтому сведём её к более практичной - алгоритм Multi-Paxos. Multi-Paxos реализует Atomic Broadcast, т.е. позволяет проводить серию консенсусов C1 -> C2 -> C3, каждый из которых выбирает очередную команду в RSM. Однако сменим терминологию, и вместо A-BCast-а придумаем его промышленный аналог - алгоритм репликации лога, при помощи которого построим RSM

Replicated log - append-only структура данных, состоящая из независимых номерованных слотов, в которые помещаются команды пользователей. Для лога команды просто набор байт, поскольку их семантику знает лишь стейт машина (разные уровни абстракции). Порядок применения команд (TO) материализован в виде копии лога на каждой реплике, а точнее его некоторого префикса. Копии лога на разных репликах в процессе заполнения отличаются, даже могут иметь разные команды в одинаковых слотах, однако eventually синхронизируются при помощи консенсуса. На случай рестартов лог надёжно хранится на диске

При построении RSM через Replicated log каждая реплика имеет свои копии стейт машины (?текущего состояния) и лога (?последовательности изменений). На высоком уровне описания, одна из реплик получает команду от пользователя, отправляет её через консенсус в логи всех реплик в один и тот же слот, даже если по предыдущим слотам и командам логи в моменте несогласованы, и когда параллельный процесс дочитает лог до этой операции, он применит её к автомату и ответит пользователю. Реплика отслеживает текущий размер префикса применённых команд, которые определяют локальное состояние RSM

Multi-Paxos не имеет канонического описания, есть лишь статья про греков. Она не определяет количество типов сообщений, следовательно неясно как на них реагировать, а также многие другие детали. По сути это конструктор или framework для построения своей реализации алгоритма путём заполнения пробелов в статье, главное не нарушить при этом safity. Чтобы построить что-то разумное и эффективное достаточно понимания Single-Decree Paxos

Строим копию лога

Каждая реплика имеет изначально пустой лог и может получить команду, в случае чего ищет в логе первый свободный слот и пытается поместить команду туда, конкурируя за этот слот с другими репликами и их командами. Каждый слот независим и связан с отдельной задачей консенсуса, решаемой для него

Когда реплика хочет вставить команду в слот, она становится Proposer-ом и инициирует консенсус - отправляет всем репликам Prepare(k, n), где k - номер свободного слота, n - Ballot number. Если консенсус выбрал другую команду, реплика выбирает новый свободный слот и повторяет процесс. При получении же Prepare(k, n) реплика начинает выполнять роль Acceptor-а на слоте k, поэтому в слоте удобно также хранить состояние Acceptor-а (np, (na, va)). Реплика может одновременно быть и Proposer-ом для нескольких команд, и Acceptor-ом для нескольких слотов, т.е. конкурентно выполнять задачи этих ролей

Физические состояния слота в конкретной копии лога
- empty - реплика ещё не использовала слот
- accepted - в слоте записан vote Acceptor-а
- chosen/committed - команда зафиксирована, т.е. является результатом консенсуса. np может меняться, (na, va) нет

Состояние committed не означает, что команда может быть применена к RSM, поскольку более ранние слоты могут быть в состоянии accepted. Команду можно применить только если перед ней committed префикс. Это и означает eventually одинаковый лог

Из-за параллельности порядок фиксации команд может не совпадать с порядком их получения репликой, если клиент отправил команды конкурентно без RT-ordering-а. Также возможны пропуски в логе, когда первее пришёл Prepare для слота с большим номером, или даже приняли команду. Prepare может в принципе не прийти для пропущенного слота, и чтобы его закрыть, необходимо попытаться записать в слот пустую метакоманду nop, либо получив обратно информацию о команде, либо испортив слот. Если есть своя команда, вместо nop можно использовать её

Благодаря консенсусу многие вещи выполняются автоматически. Чтобы понять, что слот уже не пуст, нужно попытаться записать в него команду, и, если на первой фазе получили vote, слот уже не пуст, но в переходном состоянии от empty к accepted

На практике, реплики собирают пачку команд, например ожидая некоторую долю секунды, и предлагают на консенсус сразу batch (см. Paxos Made Live ниже)

Эффективность и выбор лидера

Описанное решение является крайне неэффективным из-за конкуренции за каждый слот. Отдельные реплики/команды могут голодать (starvation), а разные реплики могут биться в livelock-е, пытаясь поместить одну и ту же команду (подразумевается, что команда с одной реплики разошлась по остальным, и они пытаются поместить её с разными n). По теореме FLP нужно использовать время, но Exponential Backoff как в Single-Deegre Paxos не помогает. Можно проще и лучше

Сократим количество Proposer-ов до одного - используем детектор выбора лидера (Leader Election, Distingushed Proposer) Omega(t) -> P - Leader. Реплика, определив для себя лидера, отправляет ему команды/клиентов, и тот отвечает за формирование порядка применения команд. При этом лидер может сменяться (из-за смерти/задержек в сети), и разные реплики даже могут видеть лидером разные узлы

Paxos очень сильно упрощает рассуждения о лидере, поскольку разделяет свойства safety и liveness. Алгоритм детектора может быть абсолютно любым, использовать время и рандом, давать неконсистентные ответы, даже назначать лидером саму реплику, поскольку лидер(ы) всё ещё использует консенсус (Paxos) для принятия решений. Изначально каждая реплика является лидером. Но если выбирать лидера разумным образом, eventually при корректно выбранном едином лидере всё станет лучше, получим оптимальный уровень liveness. Это в некотором смысле узкое место, однако алгоритм способен переживать смерть лидера

Алгоритм выбора лидера

Каждая реплика через равные промежутки времени Delta (по монотонным часам) отправляет остальным Heartbeat(id), где id - постоянный уникальный идентификатор реплики. По умолчанию реплика считает лидером себя, но если до неё доходят Heartbeat-ы, максимальный обозначает лидера на некоторое время (например 30 сек), по истечению которого он забывается, если от него не было ни одного Hb. В таком алгоритме периодически будет несколько лидеров, что не влияет на safety

В реальности узлы могут быть сразу несколькими репликами из различных replica set, например в случае шардирования. Поэтому Heartbeat-ы существуют на уровне узлов, а не реплик, чтобы не перегружать сеть возможно кратными сообщениями

Такой алгоритм имеет недостаток. Пусть случился partition, и некоторый узел долго не видел команд, а после восстановления стал лидером. Он начнёт предлагать команду в занятые слоты, получая отказы и последовательно восстанавливая лог. Это может остановить прогресс системы на длительное время. Решением является выбор лидера с наиболее актуальной версией лога, т.е. с наибольшим зафиксированным префиксом

Две оптимизации Multi-Paxos
- кэширование лидера
- pipelining

Кэширование лидера

Построенному алгоритму нужно 4RTT (Roundtrip time) на ответ: общение клиент-реплика, общение реплика-лидер, и две фазы Paxos. Это довольно медленно, и можно лучше. Лидер в проде живёт недели, поэтому его можно кэшировать, и сократить до 3RTT. Когда клиент приходит не к лидеру, ему отвечают Redirect(id), он запоминает лидера и начинает слать запросы ему

Pipelining

Пусть лидер заполняет лог последовательно, слот за слотом, команда за командой. Он видит ряд сообщений Prepare(k, n), Promise, Accept(k, n, c), Accepted, Prepare(k + 1, n'), Promise, Accept(k + 1, n', c')... Можно заметить две вещи. Во-первых, фазы Accept и Prepare соседних команд независимы, и им можно дать одинаковый Ballot number. Во-вторых, Prepare можно сделать заранее без команды. Так Accept + Prepare можно объединить в одно сообщение, сэкономив и на сети, и на записи Acceptor-ом на диск. Мы "прогреваем" слот для следующей команды, чтобы можно было сразу отправить Accept + Prepare. При неудаче начинаем по новой с начального Prepare

Из-за появившейся зависимости слотов невозможна параллельность. Заметим, что захватывать можно сколько угодно слотов вперёд, и заменим Prepare на Prepare+(k, n) - попытка захватить все слоты с позиции k, если они пустые, иначе только слот k. Acceptor запоминает Proposer-а пока лог не перезахватят. Получили 2 RTT, а также вернули возможность параллельной обработки

Так Single-Decree Paxos нужен для выбора не очередного сообщения, а лидера, причём достаточно редко, когда появляются проблемы с сетью и/или узлами

Открытые вопросы
- что именно такое пустой слот
- как понять, что Proposer проиграл
- как и где удерживать команду на случай, если в её слот выбрали другую
- как не лидеру понять, что команда committed
- как понять, что префикс commited, и команду можно применить
- когда и кем посылается ответ клиенту
- что если клиент не получил ответа и повторно послал команду
- как понять, что лидерство потеряно
- как организовать смену лидера
- как новый лидер выбирает стартовый n
- неужели Prepare+ возвращает потенциально бесконечное число голосов впереди
- как Acceptor реагирует на Prepare+
- как отвечать на неудачу Prepare+
- как возвращаться к первой фазе
Алгоритм не отвечает на них, и именно поэтому Multi-Paxos является рецептом

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Семинар 5](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-5.md)
[Семинар 6 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-6.md)
