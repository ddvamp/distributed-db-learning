## Lecture 8. Paxos Made Live

Рассмотрим, как довести Multi-Paxos до production-ready состояния. Что еще нужно сделать помимо replicated log для получения RSM. Что можно ускорить или промасштабировать. Также посмотрим, как консенсус выглядит в коде

Lock Service

Lock Service (сервис блокировок) - consensus as a service (CaaS), сетевой сервис для обеспечения консенсуса, предоставляющий систему распределённых блокировок с API acquire/release (или lock/unlock). Пример: Google Chubby, используемый в BigTable. Сервис используется заместо подключения в коде локальной библиотеки консенсуса, а интерфейс блокировок призван быть более интуитивным и простым. По сути сервис блокировок есть отказоустойчивый RSM, доступ к консенсусу внутри которого и предоставляется клиентам

Сервисом пользуются и клиенты, и другие сервисы, поэтому для адресации блокировки выстраиваются в иерархию имён - дерево. Ветвь может относиться к клиенту/сервису. Для взятия блокировки используются пути до узлов - Lock(path). Взятая клиентом блокировка не теряется при отказе узлов сервиса, но отдаётся при смерти самого клиента. Последнее обеспечивается выдачей клиенту **Lease = Lock + Timeout**. Клиент устанавливает на уровне библиотеки Session - логическое подключение, которое заключается в посылке Heartbeat-ов. При timeout-е блокировка отнимается

Пользователи при смерти своего узла повторно берут блокировку на другом. Но поскольку сбойный и медленный узлы неотличимы, в моменте несколько узлов могут считать себя владельцем и вместе менять внешнее для системы состояние. Distributed lock by design не может гарантировать Mutual Exclusion. Необходимо снабдить Lock Epoch-ой, которую использовать в операциях (аналог Ballot number в Paxos). Это изменение на практике дополняется ещё некоторыми деталями

В действительности при помощи сервиса блокировок выбирают лидера. Альтернатива lock-ам - распределённые atomic-и, которые позволяют строить распределённые lock-free и wait-free алгоритмы (Пример: ZooKeeper)

RSM в проде

Какие задачи нужно решить для развёртывания RSM
- роли узлов: Proposer - определяет порядок операций, Acceptor - обеспечивает согласованность этого порядка, Learner (Replica) - появляющаяся на практике роль, реплики в которой отвечают только за хранение состояния и применение операций. Learner реплики похожи на Data реплики в DFS, их можно масштабировать отдельно, и они не влияют на safety. При этом разные узлы играют разный набор ролей, кто-то одну, кто-то две, кто-то все три. Такое разделение позволяет лучше масштабировать, обеспечивать отказоустойчивость и оптимизировать
- число реплик: если хотим пережить f отказов, необходимо взять n = 2f + 1 реплик. Если рассматривать оценки по ролям, то необходимы f + 1 Proposer, 2f + 1 Acceptor и f + 1 Learner. При той же отказоустойчивости Acceptor-ов мы можем пережить отказы почти всех Proposer-ов и Learner-ов. Если для системы достаточно размещения всех реплик в одном ДЦ, то хватик 3-7 штук. Больше нет смысла, поскольку лишь увеличивается вероятность медленных машин
- зависимость отказов: нельзя (не стоит) помещать реплики в один Failure Domain (node -> rack -> availability zone (az, зона доступности, например здание) -> region). Важно не просто выбрать число отказов, но и на каком уровне. Чем выше уровень Failure Domain-а, тем ниже шанс коррелированного отказа при разнесении реплик по разным доменам этого уровня, но тем больше задержки на коммуникацию. В действительности чисел отказа несколько, на каждый домен отказа, и их нужно выбирать отдельно
- реконфигурации: нельзя наивно менять состав реплик. Например, при добавлении можно получить непересекающиеся кворумы, даже если медленно добавлять реплики по одной. Здесь время начинает взаимодействовать с safity, чего нельзя допускать. Выключить систему для переконфигурации также не годится. Естественное решение - добавить любые реконфигурации в протокол команд. Реплика помнит конфигурацию и применяет её ко всем слотам. Конфигурация меняется при "применении" игнорируемой RSM служебной команды Reconfig(C1). Чтобы не начинать множество параллельных кворумов в "старой" конфигурации, её изменение откладывают - Alpha method: применённая в слоте k конфигурация начинает действовать лишь со слота k + alpha, где alpha выбирается единожды и глобально. Заполнение лога приостанавливается, если исчерпано alpha-окно - имеется alpha команд в полёте. В таком решении конфигурацию не изменить, если исчерпано число отказов ("смерть цивилизации Paxos"). При реализации имеется множество частных случаев, и такое решение не работает в Raft из-за откатов лога
- недетерминизм: стоит помнить случайных числах, времени, хэш-таблицах, об обратной совместимости реплик при обновлении кода/конфигурации, чтобы состояния автомата не расходились
- Exactly-Once: не достигается базовыми Paxos/Raft, нужны усилия со стороны клиентов
- Read-only: полезно отделить от модификаций более частые чтения, которые нет смысла пропускать через RSM протокол команд. Однако нельзя просто прочесть состояние Learner-а, ведь оно может отставать - Stale Read. И нельзя читать с лидера, поскольку он может устареть. Нужно собрать кворум чтения, пересекающийся с кворумами Accept, куда гарантированно попадут зафиксированные команды текущего лидера. Это можно делать на любом узле, не только на лидере. Реплика получает пачку чтений, собирает кворум, определяет максимальный номер зафиксированного слота, дожидается, покуда все слоты до него включительно будут применены к состоянию, гарантируя линеаризацию, после чего возвращает ответ
- Batching: уменьшим работу лидера на одну команду и начнём собирать Batch-и. Для этого поставим перед Proposer-ами f + 1 Proxy узел, которые собирают команды в пачки, например 500ms, и отправляют их лидеру. Лог оперирует пачками, а RSM распаковывает их на команды. Proxy узлы по сути ещё одна "инженерная" роль
- переполнение лога: нельзя держать бесконечно растущий лог из-за его заполнения после рестарта, новым лидером или репликой, из-за размера на диске. Начнём хранить зафиксированный префикс в виде Snapshot-а состояния RSM, аннотированного номером последнего слота. Пусть снимок большой, но помещается в память, поэтому вместо глобальной блокировки, надолго останавливающей лидера, используем Copy-on-Write (персистентность) в ОС. Syscall fork() копирует поток с таблицой страниц, ссылающейся на те же физические, в новый процесс. Fuzzy Snapshots - выполняем fork() и продолжаем работать, чуть медленнее из-за CoW, а новый процесс read-only сохраняет состояние RSM на диск. Лог хранится в списке файлов - **сегментов**. Вставка в конец лога есть вставка в последний сегмент. При исчерпании сегмента добавляется новый, сразу большого размера, например 64Мб, поэтому нужно уметь определять логический размер последнего сегмента. Благодаря такой схеме, чтобы сбросить лог на диск достаточно использовать fdatasync(), который в отличие от fsync() пишет только критические метаданные, которые здесь не меняются в большинстве случаев, что сокращает число записей на диск (позиционирований) до одной. Для сброса префикса лога нужно просто скинуть несколько сегментов в начале

Ускорение RSM

RSM при добавлении реплик становится медленнее, поскольку кворумы растут, конфигурация усложняется, а шансы на медленные узлы и проблемы с сетью возрастают. Нужно оптимизировать кворумы. Paxos требует пересечения кворумов разных фаз (Prepare + Accept), значит их размеры могут отличаться

Две техники ускорения кворумов
- Striping
- Grid Quorums

Фаза Prepare происходит значительно реже фазы Accept, поэтому сделаем частый кворум меньше. Пусть на 10 узлов кворумы будут размером 8 и 3. Так число отказов не изменилось (10 - 8 = 2), но и быстрее особо не стало. Может система стала чуть стабильнее, однако нагрузка на отдельный узел не изменилась. Воспользуемся идеей RAID 0 - Striping: будем отправлять сообщения не всем Acceptor-ам, а ровно трём, чередуя их. Так задержка отдельных команд увеличится, но пропускная способность вырастет в разы благодаря меньшей нагрузке на реплики

Уменьшим фазу Prepare, использовав Grid Quorums. Зафиксируем размер кворума Accept и выстроим реплики сеткой с высотой по этому размеру. Тогда можно собирать первый кворум по любой строке, а второй - по любому столбцу. Однако при построении сетки нужно помнить о разных Failure Domain, чтобы в кворуме оказывались узлы из нескольких доменов одного уровня (Пример: консенсус в LogDevice)

Для Multi-Paxos существуют вариации, которые жертвуют линеаризацией ради одного RTT для использования в геораспределённых системах

Открытые вопросы
- как Alpha method работает с batching-ом
- как разные лидеры согласуют Striping множества
- что делать при переполнении лога
- что делать при рестарте системы
- что делать при непомещении лога в одну машину

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 7](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-7.md)
