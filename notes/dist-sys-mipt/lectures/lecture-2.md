## Lecture 2. Линеаризуемость. Репликация регистра, алгоритм ABD

#### Изучаем KV Store

В этой лекции начнём проектировать KV Store со следующим API
- $Set \left( key, value \right)$
- $Get \left( key \right)$
- (диапазоны, транзакции и другие операции в последующих лекциях)

и продолжим это делать на протяжении большей части курса

Почему этот класс распределённых систем? Первой хотели сделать распределённую базу данных, но это оказалось слишком сложно. Из неё выкинули фичи (транзакции, запросы, таблицы), и оставшуюся часть ($key \to value$) удалось сделать распределённой и причём хорошо. Уже после поверх отказоустойчивого и распределённого слоя KV реализовали привычный интерфейс базы данных, получив т.н. **SQL over KV**. К тому же доступные документы о `Google BigTable` и `Amazon Dynamo` с интерфейсом KV были опубликованы одними из первых

#### Слои KV Store

KV должен быть масштабируемым и отказоустойчивым, поэтому в нём много слоёв
- **1 уровень - Local Storage**: построим в пределах одного узла эффективную систему с произвольным доступом (API) поверх последовательного диска (Пример: `LevelDB`, `RocksDB`). Полагаем, что данные помещаются на один узел, диск используем на случай рестартов (Durability), а отказов не существует. Вопросы больших данных и отказов решаем на следующих уровнях
- **2 уровень - Replication**: организуем отказоустойчивость на случай смерти диска/узла для небольшого объёма данных. Реплицируем данные одного узла на $3-5$ репликах (**replica set**). *Реплика*, потому что одинаковые. Если необходимо реплицировать неизменяемые данные, то можно применить *erasure codes* и сократить количество реплик в два раза без потери надёжности
- **3 уровень - Distribution**: когда данные более не помещаются на одной машине, необходимо распределить их по кластеру (шардировать). Имеем много узлов и большой диапазон ключей, *Key space* |-||-||-||-|, где |-| назовём range (или регион) $\left[ b, d \right]$, где $b$ и $d$ ключи. Ключи в пределах range упорядочены. Каждый range реплицируется независимо на некотором replica set. Replica set разных range могут пересекаться, но в целом произвольны. Так каждый range будто самостоятельный KV Store
- **4 уровень (последний для целей изучения) - Transactions**: для более сложных операций, чем запись одного ключа. С их помощью реализуются транзакции в SQL (для распределённых баз данных). На этом уровне заканчивается KV Store
- **5 уровень (не входит в KV Store, часть базы данных) - Queries**: распределённое выполнение запросов и табличная модель SQL поверх них

#### Репликация регистра

Решаем задачу репликации одной ячейки памяти - регистра (задача моделирования разделяемой памяти в распределённой системе). После реализации Local Storage решение можно будет расширить на весь диск. Задачу будем решать в слещующей модели: асинхронная, неизвестны скорость и порядок доставки сообщений, неизвестна скорость работы узлов, неизвестна скорость дрейфа часов, узлы перезагружаются или отказывают навсегда, нет повреждений памяти (есть коррекция), нет византийских отказов

Операции регистра
- $Write \left( v \right)$
- $Read \left( \right)$

Обобщим задачу синхронизации памяти в процессоре, поскольку он является распределённой системой, хоть и крайне безопасной. Здесь же всё сложнее из-за отказов и ограничений (скорости) сети: на разных узлах в произвольные моменты времени значения ячейки будут отличаться. Система должна скрывать это от пользователя и предоставлять ему набор гарантий, например видимость своей же записи или видимость чужой записи по причинности (happens-before по внешнему каналу)

#### Последовательная и конкурентная истории

**Конкурентная история** - глобальная история во времени конкурентных действий (чтений/записей) с регистром многих пользователей. Операции не атомарны, и в истории визуально отображены их начало и конец, а также результат. Начало означает инициацию операции пользователем, а конец - получение пользователем ответа. При этом не гарантировано, что в момент конца записи в регистре находится записанное значение - оно уже может быть перезаписано. Операции либо одна заканчивается раньше начала другой, и говорят, что они состоят в отношении до/после (**упорядочены во времени, Real-time ordered**), либо пересекаются, и называются **конкурентными**. Первые, это либо последовательные операции одного клиента, либо операции, упорядоченные по happens-before (внешний канал связи). Вторые, все остальные операции. Возможные конкурентные истории порождаются конкретной реализацией регистра с определёнными гарантиями (моделью согласованности) в процессе работы с ним, и для пересекающихся операций можно считать, что они выполняются в произвольном порядке, т.е. мы вольны предположить тот порядок, который соответствует ожидаемым от реализации гарантиям (модели согласованности), и напротив, если ни один из порядков не подходит, реализация, в рамках ожидаемой модели, некорректна

**Последовательная история** - история последовательных действий с регистром, которые не пересекаются во времени, будто все действия запрашиваются одним пользователем в одном потоке и выполняются мгновенно. Пример $w \left( 1 \right) w \left( 2 \right) r \to 2 \hspace{0.5em} w \left( 3 \right) r \to 3$. Сама по себе последовательная история не может быть правильной или нет, например в случае $w \left( 42 \right) r \to 41$, только при рассмотрении в рамках какой-либо модели согласованности. Последовательность действий с регистром каждого отдельного клиента является последовательной историей, которая обязана согласовываться с конкурентной историей

**Спецификация (Spec) регистра** - набор допустимых последовательных историй (или наблюдаемых поведений). Спецификация описывает, как может вести себя регистр (какие последовательности записей/чтений возможны), если к регистру нет конкурентных обращений. Спецификацию задают при проектировании регистра, например *Atomic* или *Regular*

**Consistency Model (модель согласованности)** - отвечает на вопрос: какие (корректные) конкурентные истории может порождать реализация с заданной спецификацией

Объединяя всё вместе, спецификация говорит, как ведёт себя регистр в идеальной среде (один пользователь, синхронные операции, нет сбоев), а модель согласованности говорит, как регистр с такой спецификацией будет выглядеть в реальной среде (с конкурентным доступом и сбоями). Т.о. спецификация и модель независимы, но обе влияют на поведение

Случаи нарушения гарантий/модели реализацией регистра
- нарушение внешней причинности (hb), когда модель требует обратного
- невозможность упорядочить конкурентные операции в конкурентной истории
- невозможность последовательной истории клиента по модели
- несогласованность последовательной истории клиента и конкурентной
- несогласованность последовательных историй клиентов в совокупности, когда модель требует обратного

#### Линеаризуемость

Модель согласованности, которую хотим от регистра, это **Linearizability** (**линеаризуемость, External Consistency** - намёк на внешнюю коммуникацию) - модель, в которой для любой конкурентной истории операций с регистром существует допустимая по его спецификации последовательная история, называемая *Линеаризацией*, которая построена из тех же операций с теми же результатами, и которая уважает Real-time ordering любой пары упорядоченных операций $o_1$ и $o_2$

Линеаризуемость не предъявляет требований к пересекающимся во времени операциям, поэтому если пользователь сам не упорядочил операции по Real-time ordering, то он не должен ожидать от них какого-либо порядка в линеаризации. Линеаризуемость является более общей моделью, чем Sequential consistency, т.е. даёт больше гарантий. Одновременно может существовать несколько линеаризаций для одной истории, что не является проблемой

**Регистр линеаризуем (atomic register)**, если его реализация порождает только линеаризуемые истории. Для пользователя это означает, что он может думать о системе как о не распределённой, будто она действует атомарно, операции случаются мгновенно одна за одной, и конкуренции нет

Линеаризация по сути единый логический порядок операций, которым клиенты могут объяснить изменения в истории системы, как если бы она не была распределённой и конкурентной, а операции выполнялись мгновенно. При линеаризуемости история изменений, которую видит любой клиент, является подмножеством линеаризации. Если же видение клиентами системы противоречиво, линеаризуемость невозможна

Грубые объяснения линеаризуемости
- любая операция системы выполняется под мьютексом
- любая операция произошла в точке на её отрезке в конкурентной истории, и линеаризация суть множество этих точек
- в графе порядков отсутствуют циклы

#### Каким образом линеаризуемость уважает Real-time ordering

Ни система, ни клиенты не могут использовать $rt$, поскольку доступа ко времени нет, а часы не идеальны и не синхронизируемы. Система выставляет операциям монотонные timestamp-ы, которые увеличиваются со временем, $ts \Rightarrow rt$. Клиенты в свою очередь думают о **причинности (happens-before)** (порядок локальных действий, реакции на внешние сообщения), $hb \Rightarrow rt$ (от противного, действие не может стать причиной события в прошлом), и ожидают, что система это учтёт. И хотя система не знает о $hb$, а клиенты не знают о $ts$ (в некоторых моделях знают), два эти порядка согласованы через общее время $rt$, и следовательно система учитывает $hb$. Так мы без времени знаем какие события следуют до каких

#### Задача репликации регистра

Atomic Register
- $Write \left( v, ts \right) $, $ts$ - timestamp
- $Read$

Операции блокирующие, Asynchronous model, Crash/Restart

Отказоустойчивость (или допустимое число отказов) зависит от размера replica set. Пока неизвестно как, поэтому интуитивно возьмём три реплики (затем алгоритм можно будет обобщить). Дополнительные упрощения
- конфигурация статична - не умеем заменять отказавшие узлы, добавлять новые. Устраняется в 8 лекции
- клиенты знают количество и адреса машин, и общаются с ними напрямую. Реплики не общаются между собой. Устраняется во 2 семинаре
- пишет только один конкретный клиент и строго последовательно, после его смерти писателей нет, читателей произвольное число. Устраняется после построения алгоритма

Writer посылает запрос на три реплики и ждёт подтверждения с двух (одно не отказоустойчиво, трёх можем не дождаться). Каждая реплика хранит значение регистра, которые в моменте не синхронны. Записи могут задерживаться и обгонять друг друга, и чтобы реплика не перетёрла новое значение старым, writer снабжает записи $ts$ (счётчик операций). Реплика последовательно обрабатывает приходящие записи, при большем $ts$ записывает значение, иначе отбрасывает (в MVCC системах (`YDB`) всё равно записывается), и безусловно посылает подтверждение. **Blind write** - безусловная запись без сравнения с текущим значением

При чтении аналогично, посылаем запрос на три реплики и ждём двух ответов, из которых выбираем значение с большим $ts$

#### Quorum System

При $3$ репликах необходимы $2$ ответа, и допустим $1$ отказ, причём необходимое число ответов не уменьшается при отказах. В случае $n$ реплик число отказов $f \lt \lceil \frac{n}{2} \rceil$, и необходимо $\lfloor \frac{n}{2} \rfloor + 1$ подтверждений. Как видно, множества подтверждений чтения и записи имеют общие узлы по принципу Дирихле, т.е. пересекаются, поэтому чтение видит актуальную запись. Это обобщается в **Quorum System**: $P$ - множество всех узлов, $Q$ - система кворумов, $Q \subseteq 2^P$, если $\forall A, B \in Q: A \cap B \neq \emptyset$. В алгоритме, $A$ и $B$ это наборы реплик, отправившие подтверждения чтения и записи, а используемая система кворумов **Majorities** - минимальные строгие большинства. **Quorum** означает минимальное количество голосов для принятия решения. Говорят операция чтения или записи собирает кворум (нужное число подтверждений). Если запись $rt$ ordered before чтения, ожидаем, что кворум чтения увидит узлы из кворума записи. Если собрать кворум невозможно, система блокируется (нужно использовать таймауты). Т.о. Quorum System - это инструмент сокрытия отказов

#### Отказоустойчивость в QS

Не стоит выбирать чётное число реплик, поскольку при добавлении реплики размер кворума вырастает на единицу, а отказоустойчивость не изменяется. При этом есть гибкость выбора кворумов. Множества чтения и записи не обязаны быть равными. Можно уменьшить кворум чтения, тем самым ускорив операцию и увеличив число отказов за счёт большего кворума и более медленной операции записи, и меньшего числа отказов (например, 2 + 4 на 5 репликах с 3 + 1 отказами). Если число отказов превысит допустимый предел, кворум не соберётся, а система перестанет отвечать (или сообщит о недоступности по таймауту), но главное не нарушит гарантий

Quorum System позволяет также избежать split brain. В нечётной конфигурации при partition будет меньшая часть, которая не сможет собирать кворумы и отключится. Либо, если кворум чтения меньше, обе части смогут обрабатывать операции чтения, но не записи. Примечательно то, что после восстановления partition система может продолжать функционировать без внешней синхронизации по построению алгоритма, поскольку значения в отключённой части будут отбрасываться (по $ts$), пока она не актуализируется

Также система кворумов поясняет, почему нельзя просто добавлять реплики. На новых будут кворумы корректного размера, но на старых кворумы относительно уменьшатся и смогут не собирать актуальные данные

#### Нарушение линеаризации

Описанный алгоритм не гарантирует линеаризуемость. Пусть неоконченная запись попала на реплику 1, и клиент (или разные клиенты) делает два чтения, получая ответы сперва с реплик 1 и 2, затем с 2 и 3. Сначала он увидит новое значение, затем старое. Это не линеаризуемая история, и регистр не атомарен. Однако алгоритм и в таком виде используется в проде, например в `Apache Cassandra`, где решаемым задачам линеаризуемость не требуется, и за счёт этого упрощения можно сделать больше оптимизаций в другом слое

Суть в том, что кворум собирается не мгновенно, это продолжительное действие. Кворум чтения вместо пересечения с кворумом записи может захватить отдельный узел, а клиент, сам того не понимая, наблюдать отдельные локальные изменения и видеть незавершённую запись. Необходимо обеспечить "видимость" лишь собранных кворумов

#### Обеспечение линеаризуемости

Добавим *helping* и сделаем чтение двухфазным. После кворума чтения собираем кворум записи с тем же $ts$. Так по окончанию чтения, последующие в $rt$ order чтения гарантированно не увидят более ранние значения. Получили линеаризуемость за счёт усложнения/замедления операции чтения. И хотя исходный сценарий всё ещё возможен, теперь в нём две операции являются конкурентными, а значит их можно переупорядочить. Возможная оптимизация: не пишем на реплики с тем же $ts$, считая, что подтверждение записи от них уже есть

Итого, если клиент обеспечил $hb$, значит операции не пересекаются во времени, а значит они пересекаются по состоявшимся кворумам. Если же $hb$ нет, операции могут пересекаться, и порядок во времени может быть любым (при линеаризации выберем тот, что нам удобнее)

#### Доказательство линеаризуемости

Нужно найти линеаризацию для каждой возможной конкурентной истории. Построим последовательную историю, в которой записи разместим в порядке выполнения единственным писателем. Чтения поставим после прочитанных ими записей в произвольном порядке, но не раньше других чтений, которые $rt$ ordered before них. В получившейся истории операции выставлены по $ts$. Докажем, что этот порядок и есть линеаризация, то есть что он уважает $rt$. Рассмотрим две операции $o_1$ и $o_2$. Если они конкурентные, их порядок может быть любым. Пусть $o_1 \prec_{rt} o_2$. Если $o_2$ запись, то у неё самый новый $ts$ в системе. Если $o_2$ чтение, у $o_1$ обязательно есть фаза записи, от которой чтение получило $ts$. В обоих случаях $ts \left( o_1 \right) \le ts \left( o_2 \right)$, а значит в линеаризации операции стоят в таком же порядке, ч.т.д.

Для нескольких писателей нужно уметь распределённо выбирать единый монотонный timestamp. Путь ts = Now(). В реальности клиент посылает запись, она попадает на узел координатор (один из), который вызывает Now(), собирает кворумы и подтверждает операцию клиенту (т.е. алгоритм реализован на стороне сервера, не в клиенте). Но что делать с тем, что у разных координаторов несинхронизированы часы (нужна монотонность)
Делаем запись двухфазной. Сначала через фазу чтения получаем timestamp и используем его увеличенное на 1 значение для фазы записи. Теперь операции чтения и записи равны по стоимости. Снова получили свойство, что кворумы на чтение и запись РАЗНЫХ операций записи пересекаются в процессе их выполнения. Но теперь возможна новая проблема - одинаковые timestamp-ы
Чтобы не допустить одинаковые timestamp-ы делаем их лексикографическими: парой (ts, id), где id - уникальный идентификатор реплики или клиента (в зависимости от алгоритма генерации timestamp-ов), задающийся при установке конфигурации. Нас не волнует неравнозначность реплик, поскольку без hb пользователь и так ничего не ожидает
(см. Hybrid Logical Clock (YDB) как аналог, где нет фазы чтения)

В Google используется TrueTime.Now(), который заменяет дорогие коммуникации узлов (лишняя фаза чтения с целью узнать прошлое) на ожидание и упорядочение по атомным часам. Это называется Commit Wait (Spanner): для транзакции берётся метка ts = TT.Now().latest, после чего ждём, покуда TT.Now().earliest >= ts, и тогда посылаем подтверждение. Теперь никакие транзакции не выберут более ранний timestamp, а значит произойдут позже

Как поддержать рестарты? Сперва реплика надёжно пишет регистр на диск, после отправляет подтверждение. В случае рестарта реплика восстанавливает регистр и оказывается в состоянии как после partition (необходимо догнать актуальные timestamp-ы). По идее таким же образом, но крайне неэффективно, можно заменять отказавшие узлы: считай что заменённый узел был в partition со старта работы регистра

Как оценить эффективность алгоритма? Нужна модель стоимостей. Два основных расхода времени (влияющие на latency): RTT (RoundTrip Time) - время между отправкой сообщения и получением ответа, Disk - время записи на диск вместе с flush (fsync - отправить данные из страничного кэша на диск). В зависимости от удалённости узлов и вида диска может превалировать одно из двух. Вся операция внутри системы (обе фазы) называется Quorum flush - инициировать кворум, каждой репликой обработать запрос со сбросом диска, собрать ack-и

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Семинар 1](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-1.md)
[Семинар 2 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-2.md)
