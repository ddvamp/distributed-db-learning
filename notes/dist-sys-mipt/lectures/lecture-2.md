## Lecture 2. Линеаризуемость. Репликация регистра, алгоритм ABD

В этой лекции начнём проектировать KV Store со следующим API
- $Set \left( key, value \right)$
- $Get \left( key \right)$
- (диапазоны, транзакции и другое в последующих лекциях)

Почему этот класс распределённых систем? Первой хотели сделать распределённую базу данных, но это оказалось слишком сложно. Из неё выкинули фичи (транзакции, запросы, таблицы), и оставшуюся часть ($key \to value$) удалось сделать распределённой и причём хорошо. После поверх отказоустойчивого и распределённого слоя KV реализовали привычный интерфейс базы данных, получив т.н. **SQL over KV**

#### Слои KV Store

KV должен быть масштабируемым и отказоустойчивым, поэтому в нём много слоёв
- **1 уровень - Local Storage**: построим в пределах одного узла эффективную систему с произвольным доступом (API) поверх последовательного диска (Пример: `LevelDB`, `RocksDB`). Полагаем, что данные помещаются на один узел, диск используем на случай рестартов (Durability), а отказов не существует. Вопросы больших данных и отказов решаем на следующих уровнях
- **2 уровень - Replication**: организуем отказоустойчивость на случай смерти диска/узла. Реплицируем данные одного узла на $3-5$ репликах (**replica set**). *Реплика*, потому что одинаковые. Если необходимо реплицировать неизменяемые данные, то можно применить *erasure codes* и сократить количество реплик в два раза без потери надёжности
- **3 уровень - Distribution**: когда данные более не помещаются на одной машине, необходимо распределить их по кластеру (шардировать). Имеем много узлов и большой диапазон ключей (*Key space*) |-||-||-||-|, где |-| назовём range (или регион) $\left[ b, d \right]$, где $b$ и $d$ ключи. Ключи в range упорядочены. Каждый range реплицируется независимо на некотором replica set. Replica set разных range могут пересекаться, но в целом произвольны. Теперь каждый range будто самостоятельный KV Store
- **4 уровень (последний для целей изучения) - Transaction**. С их помощью реализуются транзакции в SQL (для распределённых баз данных). На этом уровне заканчивается KV Store
- **5 уровень (не входит в KV Store, часть базы данных)** - распределённое выполнение запросов и табличная модель SQL поверх них

Решаем задачу репликации одной ячейки памяти - регистра (задача моделирования разделяемой памяти в распределённой системе). После реализации Local Storage решение можно будет расширить на весь диск. Задачу будем решать в слещующей модели: асинхронная, неизвестны скорость и порядок доставки сообщений, неизвестна скорость работы узлов, неизвестна скорость дрейфа часов, из отказов лишь рестарты и смерть узлов (нет повреждений отдельных битов оперативной памяти космическими лучами, приводящих к каскадным эффектам), есть коррекция памяти, нет византийских отказов

Операции регистра
- $Write \left( v \right)$
- $Read \left( \right)$

Поскольку процессор есть распределённая система, хоть и крайне безопасная, обобщим задачу синхронизации пямяти в нём. Здесь всё сложнее из-за отказов и ограничений (скорости) сети: на разных узлах в произвольные моменты времени значения ячейки будут отличаться, и система будет пытаться скрыть это от пользователя. Значит мы хотим получить от системы набор гарантий, например видимость своей же записи или видимость чужой записи по причинности (happens-before по внешнему каналу)

**Конкурентная история** - глобальная история конкурентных действий с регистром (чтений/записей) во времени, в которой операции либо состоят в отношении до/после (**упорядочены во времени, Real-time ordered**), и на схеме одна заканчивается раньше начала другой, либо не состоят, на схеме пересекаются, и можно считать, что они выполняются в произвольном порядке, который мы сами вольны для себя выбрать. Помимо начала и конца на схеме отображается результат операции. Возможные конкурентные истории порождаются конкретной реализацией регистра с разными гарантиями в процессе работы с регистром

**Последовательная история** - история последовательных действий с регистром, которые не пересекаются во времени (будто все действия мгновенно выполняются одним пользователем в одном потоке). Пример $w \left( 1 \right) w \left( 2 \right) r \to 2 \hspace{0.5em} w \left( 3 \right) r \to 3$. Сама по себе последовательная история не может быть правильной или нет, например в случае $w \left( 42 \right) r \to 41$. Последовательность действий с регистром каждого отдельного клиента является последовательной историей, которая обязана согласовываться с конкурентной историей

**Спецификация (Spec) регистра** - набор всевозможных допустимых (правильных) последовательных историй (или наблюдаемых поведений). Спецификация описывает, как может вести себя регистр (какие последовательности записей/чтений возможны), если к регистру нет конкурентных обращений. Спецификацию задают при проектировании регистра, например *Atomic* или *Regular*

**Consistency Model (модель согласованности)** - отвечает на вопрос: какие конкурентные истории может порождать реализация с заданной спецификацией

Объединяя всё вместе, спецификация говорит, как ведёт себя регистр в идеальной среде (один пользователь, синхронные операции, нет сбоев), а модель согласованности говорит, как регистр с такой спецификацией будет выглядеть в реальной среде, с конкурентным доступом и сбоями. Т.е. спецификация и модель независимы, но обе влияют на поведение

Хотим от регистра Linearizability (линеаризуемости, External Consistency - намёк на внешнюю коммуникацию), что является более общей моделью, чем Sequential, т.е. даёт больше гарантий
Линеаризуемость - для любой конкурентной истории, порождаемой реализацией некоторого регистра, существует допустимая (по спецификации) последовательная история, называемая Линеаризацией, построенная из тех же операций с теми же результатами, такая, что для любой пары операций o1 и o2, если в конкурентной истории операция o1 завершилась в физическом времени (абсолютном) до начала операции o2 (Real-time ordering, отсутствует в процессоре), то в последовательной истории операция o1 следует до операции o2 (требований к пересекающимся во времени операциям нет)
Линеаризация по сути единый для всех клиентов логический порядок операций, соответствующий спецификации, которым клиенты могут объяснить изменения в истории, как если бы система не была конкурентной, а операции выполнялись мгновенно. Так каждый клиент считывая регистр должен получить некоторую историю, которая является частью линеаризации. Линеаризация едина для всех клиентов, поскольку она строится по истории во времени, а время для всех одинаково. В случае возможности линеаризации клиенты обязаны видеть одну и ту же последовательность изменений, и наоборот, если истории с точки зрения клиентов противоречат друг другу, линеаризации невозможна. То есть, линеаризация означает, что результаты чтений разных клиентов, выраженные их личные последовательными историями, непротиворечивы и могут быть объединены в единую последовательную историю. Одновременно может существовать несколько линеаризаций для истории, но нас это не тревожит, нас устраивает наличие хотя бы одной (любой)
Регистр линеаризуем (atomic register), если его реализация порождает только линеаризуемые истории. Для пользователя это означает, что он может не думать о системе как о распределённой, что она действует атомарно, операции случаются мгновенно одна за одной, и конкуренция не важна
Грубые объяснения линеаризуемости: 1. как будто внутри системы существует мьютекс на любое взаимодействие с ней 2. на отрезке действия в конкурентной истории найдётся точка, выбрав которую можно думать, что операция произошла в ней атомарно; эти точки будут отражать линеаризацию 3. отсутствуют циклы в графе порядков 4. то, как конкретный пользователь видит систему

Но как мы может оперировать понятием Real-time ordering, если доступа ко времени нет, а часы не идеальны и не синхронизируемы. Установить RT ordering конца-начала конкурентных действий ни система, ни клиенты не могут. Вместо времени клиенты думают о причинности (happens-before)(порядок локальных действий, реакции на внешние сообщения). hb => RT ordering (действие не может стать причиной события в прошлом). Система об hb не знает (он снаружи), но должна выставить операциям некоторый порядок (timestamps). Поскольку timestamps выдаются последовательно (это нужно обеспечить), то из их порядка => RT ordering. Итого, система не знает о hb, клиенты не знают о timestamps (в некоторых системах знают), но оба этих порядка согласованы через общее время (RT ordering), значит система учитывает hb
Теперь мы без времени знаем какие события следуют до каких
Линеаризуемость ничего не говорит о пересекающихся операциях, поскольку пользователь не удосужился установить между ними hb, потому как сам не ожидает порядка от этих операций

Atomic Register
- Write(v, t - timestamp)
- Read
- операции блокирующие
Asynchronous model
Crash/Restart

Какой размер у replica set (от этого некоторым образом зависит отказоустойчивость, или допустимое число отказов)? Интуитивно возьмём три реплики (затем алгоритм можно будет обобщить). Упрощения
- первое - конфигурация статична (не умеем заменять отказавшие узлы, добавлять новые)
- второе - клиенты знают количество и адреса машин, и общаются с ними напрямую. Реплики не общаются (устраним после построения алгоритма)
- третье - пишет только один конкретный клиент (после его смерти писателей нет) и строго последовательно, читателей произвольное число (устраним после построения алгоритма)
Алгоритм
Пишем на три реплики, ждём подтверждения с двух (одно не отказоустойчиво, трёх можем не дождаться). Каждая реплика хранит копию регистра, и значения в них не синхронны. Поскольку записи могут обгонять друг друга, чтобы реплика не перетёрла новое значение старым, писатель снабжает записи timestamp-ами (временные метки, счётчик операций). Реплика последовательно обрабатывает приходящие записи, при большем timestamp-е записывает значение, иначе отбрасывает (в MVCC системах (YDB) всё равно записывается), и безусловно посылает подтверждение. Blind write - безусловная запись без сравнения с текущим значением
При чтении аналогично, дожидаемся двух ответов и выбираем с большим timestamp-ом

При 3-х репликах необходимы два ответа, и допустим один отказ. В случае n реплик число отказов, f <= n/2, и необходимо n/2 + 1 подтверждений (эти числа не меняются при отказах). Множества подтверждений на запись и чтение должны пересекаться, т.е. иметь общие узлы по принципу Дирихле. Это обобщается в Quorum System: P - множество всех узлов, Q - система кворумов (подмножеств этих узлов) и принадлежит 2^P, и для любых множеств A и B из Q, A и B пересекаются. В случае регистра A и B это наборы реплик, отправившие подтверждения записи и чтения. Система кворумов Majorities - минимальные строгие большинства. Quorum означает минимальное количество голосов для принятия решения. Говорят операция чтения или записи собирает кворум (нужное число подтверждений). Если запись real-time ordered before чтения, ожидаем, что кворум чтения увидит узлы из кворума записи. Система блокируется, когда невозможно собрать любой кворум (нужно использовать таймауты). Т.о. Quorum System - это инструмент сокрытия отказов
Не стоит выбирать чётное число реплик, поскольку размер кворума вырастает на единицу, а отказоустойчивость не изменяется
Также есть гибкость выбора кворума. Необязательно стремиться к равным значениям на чтение и запись: на 5 репликах можно дожидаться 4 записей и 2 чтений, и чтения стали быстрее за счёт более медленных записей, также число отказов на чтение стало больше за счёт меньшего числа отказов на запись
Ни система, ни клиент не видят отказов. Но что делать, если число отказов превысило допустимый предел. Самое простое, перестать отвечать (кворум не собирается, клиент таймаутится)
Quorum System также позволяет избежать split brain: поскольку для кворумов A и B должны пересекаться, то выбрав некоторое множество узлов, оставшиеся не смогут собрать кворум. Так, либо обе части смогут собирать кворум лишь на чтение, либо меньшая часть станет недоступной (если же кворум записи меньше кворума чтения, возможна ситуация, когда в обе части можно лишь писать). Замечательно то, что после восстановления partition система может продолжать функционировать без внешней синхронизации по построению алгоритма: значения в отключённой части будут отбрасываться (по timestamp-у), пока она не актуализируется. Также система кворумов поясняет, почему нельзя добавлять машины: на новых будут бОльшие кворумы, что хорошо, но на старых станут мЕньшие, которые могут не учесть актуальные данные

В такой системе линеаризуемость отсутствует. Пусть неоконченная запись попала на реплику 1, и клиент делает два чтения (или разные клиенты), сперва с реплик 1 и 2, затем с 2 и 3. Он увидит сперва новое, затем старое значения. Это не линеаризуемая история, и регистр не атомарен. Прим: Cassandra пользуется таким алгоритмом, поскольку решаемые ею задачи не требуют такой гарантии, при этом она на другом слое делает оптимизации за счёт упрощения здесь
Суть в том, что кворум собирается не мгновенно, это продолжающееся действие. По сути, клиент сам того не понимая наблюдает отдельные локальные изменения и видит незавершённую запись (он не пересёкся с кворумом, а захватил отдельный узел). Необходимо обеспечить "видимость" лишь собранных кворумов

Ситуацию чинит helping. Сделаем чтение двухфазным. Сперва выполняем обычное чтение, затем с той же временной меткой выполняем запись, собирая кворум (оптимизация: одинаковые timestamp-ы -> кворум есть, запись не нужна). Теперь по окончанию нашего чтения, последующие в real-time order чтения гарантированно увидят по крайне мере то же, что и мы. Так мы получаем линеаризуемость за счёт усложнения/замедления чтения. Исходный сценарий всё ещё возможен, однако теперь первое чтение гарантированно пересекается со вторым во времени, значит их можно переупорядочить

Итого, если клиент обеспечил hb, значит операции не пересекаются во времени, а значит они пересекаются по состоявшимся кворумам. Если же hb нет, операции могут пересекаться, и порядок во времени может быть любым (при линеаризации выберем тот, что нам удобнее)

Док-во линеаризуемости построенного алгоритма. Между собой записи упорядочены единственным писателем, чтения упорядочены по timestamp-ам (т.е. здесь невозможно прочитать или записать более старое значение). Поставим чтения после прочитанных ими записей, но не раньше других чтений, которые real-time ordered before них. Если операции пересекаются во времени, мы от них ничего не требуем, иначе всё хорошо по кворумам. Доказательство по монотонности timestamp-ов

Для нескольких писателей нужно уметь распределённо выбирать единый монотонный timestamp. Путь ts = Now(). В реальности клиент посылает запись, она попадает на узел координатор (один из), который вызывает Now(), собирает кворумы и подтверждает операцию клиенту (т.е. алгоритм реализован на стороне сервера, не в клиенте). Но что делать с тем, что у разных координаторов несинхронизированы часы (нужна монотонность)
Делаем запись двухфазной. Сначала через фазу чтения получаем timestamp и используем его увеличенное на 1 значение для фазы записи. Теперь операции чтения и записи равны по стоимости. Снова получили свойство, что кворумы на чтение и запись РАЗНЫХ операций записи пересекаются в процессе их выполнения. Но теперь возможна новая проблема - одинаковые timestamp-ы
Чтобы не допустить одинаковые timestamp-ы делаем их лексикографическими: парой (ts, id), где id - уникальный идентификатор реплики или клиента (в зависимости от алгоритма генерации timestamp-ов), задающийся при установке конфигурации. Нас не волнует неравнозначность реплик, поскольку без hb пользователь и так ничего не ожидает
(см. Hybrid Logical Clock (YDB) как аналог, где нет фазы чтения)

В Google используется TrueTime.Now(), который заменяет дорогие коммуникации узлов (лишняя фаза чтения с целью узнать прошлое) на ожидание и упорядочение по атомным часам. Это называется Commit Wait (Spanner): для транзакции берётся метка ts = TT.Now().latest, после чего ждём, покуда TT.Now().earliest >= ts, и тогда посылаем подтверждение. Теперь никакие транзакции не выберут более ранний timestamp, а значит произойдут позже

Как поддержать рестарты? Сперва реплика надёжно пишет регистр на диск, после отправляет подтверждение. В случае рестарта реплика восстанавливает регистр и оказывается в состоянии как после partition (необходимо догнать актуальные timestamp-ы). По идее таким же образом, но крайне неэффективно, можно заменять отказавшие узлы: считай что заменённый узел был в partition со старта работы регистра

Как оценить эффективность алгоритма? Нужна модель стоимостей. Два основных расхода времени (влияющие на latency): RTT (RoundTrip Time) - время между отправкой сообщения и получением ответа, Disk - время записи на диск вместе с flush (fsync - отправить данные из страничного кэша на диск). В зависимости от удалённости узлов и вида диска может превалировать одно из двух. Вся операция внутри системы (обе фазы) называется Quorum flush - инициировать кворум, каждой репликой обработать запрос со сбросом диска, собрать ack-и

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Семинар 1](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-1.md)
[Семинар 2 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-2.md)
