## Lecture 11. Транзакции, изоляция транзакций, 2PL и SI

#### Мотивация

Цель курса - построить распределённую отказоустойчивую базу данных. К этому моменту мы уже можем построить бесконечно большое горизонтально масштабируемое линеаризуемое отказоустойчивое KV хранилище. Настала пора добавить транзакции

*Когда нужны транзакции*
1. Транзакции необходимы для реализации операций, задействующих несколько таблеток. Например, изменение диапазона ключей, попадающего на границу. Сейчас такая логически единая операция состоит из двух истинных, по одной на каждую таблетку. Две конкурентные операции превращаются в четыре, по паре на каждую таблетку, и эти пары могут выполниться в разном порядке, хотя и глобально линейном ($` A_1 \prec B_2 \prec B_1 \prec A_2 `$)
2. Другой пример, поддержка переименования и конкатенации файлов в DFS из [лекции 9](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-9.md#dfs)
3. Также транзакции нужны для реализации в Message Queue **Exactly-Once Processing** - атомарно получить сообщение из очереди, обработать и положить результат в другую
4. И конечно, транзакции нужны для реализации транзакций базы данных

**Транзакции** суть операции, состоящие из нескольких, но упорядочивающиеся друг относительно друга как атомарные целые. Необходимость транзакций возникает в распределённых системах, в многопользовательских программах и даже в конкурентном коде. Поэтому рассмотрим общую теорию транзакций, связанную с concurrency. Об аспектах транзакций в контексте распределённых систем см. лекцию 12 и лекцию 13

#### Постановка задачи

Имеем **Data Store** - абстрактное линеаризуемое хранилище с *операциями*
- $` Set \left \lparen k, v \right \rparen `$
- $` Get \left \lparen k \right \rparen \to v `$

над которым хотим выполнять транзакции

**Транзакция** - интерактивная программа, которая начинается со служебной операции $StartTx$, состоит из произвольной последовательности операций $Get$ и $Set$ и, либо фиксирует результаты операцией $CommitTx$ в Data Store, либо вызывает $AbortTx$ и отменяется. Транзакция сама отменяет себя, если в процессе не выполнен какой-то её инвариант

**Scheduler (планировщик)** - находится между клиентом и Data Store и обслуживает транзакции. Он получает чтения/записи, последовательно для одной транзакции и конкурентно для разных, и перенаправляет их в хранилище, возможно, для разных транзакций параллельно. Благодаря алгоритму планирования в хранилище возникает некоторая конкурентная история, которую можно объяснить последовательной, называемой **Schedule (расписание)**. Можно сказать, что планировщик порождает некоторый класс последовательных расписаний

*Две задачи*
1. Требования к планировщику - как описать класс хороших расписаний (*Модель изоляции транзакций*)
2. Построение планировщика - как порождать расписания из заданного класса

#### Модель изоляции транзакций

**Serial Schedule (серийное расписание)** - расписание, в котором транзакции не смешиваются (будто под мьютексом). Очевидно хорошее, но использовать лишь его неразумно. Хотелось бы, чтобы Планировщик порождал больше расписаний, каждое из которых с точки зрения пользователся неотличимо от серийного. Тогда транзакции казались бы атомарными операциями

**View-Equivalence** - отношение двух расписаний $` S \stackrel{view}{\sim} S^{'} `$, когда содержащиеся в них чтения возвращают одни и те же значения, а сами расписания переводят хранилище в одно и то же итоговое состояние. Т.е. view-equivalent расписания неотличимы по результатам чтений извне

**View-Serializability** - свойство расписания $` S: \hspace{0.25em} S \stackrel{view}{\sim} S^{*} `$, где $` S^{*} `$ - serial. Т.е. view-serializable расписание неотличимо внешним наблюдателем от последовательного выполнения транзакций. Это свойство не требует, чтобы серийный порядок уважал $rt$ порядок

**View-serializable планировщик** порождает только view-serializable расписания. Если пользователю дать такой планировщик, то, вместо размышлений о конкуренции транзакций под капотом, ему достаточно думать о корректности каждой транзакции по отдельности, что очень удобно

**Strict View-Serializability** - свойство, аналогичное view-serializability, дополнительно уважающее $rt$ порядок

Вышеописанное составляет **Модель изоляции транзакций** и определяет семантику конкурирующих транзакций. Это *Isolation (ACID)*. Имея view-serializability транзакций можно обеспечить *Consistency (ACID)* данных в хранилище. (Примечание, *AID (ACID)* есть свойства транзакций, которые позволяют обеспечить *C (ACID)* - свойство данных)

#### Связь Strict View-Serializability и Linearizability

Linearizability есть свойство, объясняющее поведение конкурентных операций. Strict view-serializability объясняет поведение конкурентных транзакций - множеств операций. И если представить операции как вырожденные транзакции, то линеаризуемость является частным случаем строгой видимой сериализуемости, а все модели согласованности частными случаями моделей изоляции транзакций

С другой стороны, как обеспечение линеаризации требовало от регистра некоторых свойств - спецификации, так и реализация модели транзакций требует некоторых свойств от системы. В случае строгой видимости сериализуемости необходима линеаризуемая система. В Sequential Consistency же системе можно реализовать лишь View-Serializable транзакции

Итого, strict view-serializable планировщик это алгоритм, в результате работы которого с линеаризуемой системой порождаются только такие конкурентные истории, каждую из которых можно объяснить некоторой последовательной историей, сохраняющей $rt$ порядок, которая с точки зрения операций чтения неотличима от серийной истории, а пользователи при этом могут не думать о concurrency

#### Недоступность view-serializability

Цена обеспечения view-serializability слишком высока, поскольку она устроена очень непонятным образом. Возможны сценарии, когда добавление в конец истории ещё одной операции делает возможным объяснить её серийным расписанием. Но ни один планировщик не может видеть будущее. Более того, задача определения, является ли расписание view-equivalent серийному является NP-полной

Упростим задачу и выделим в классе view-serializable расписаний подкласс просто устроенных расписаний, и построим Планировщик, порождающий их

**Conflict (конфликт)** в расписании - ситуация, когда две операции из двух разных транзакций обращаются к одному и тому же ключу, и по крайней мере одна из них запись. Сами операции называются *конфликтующими*. Пару соседних неконфликтующих операций можно поменять местами, получив view-equivalent расписание

**Conflict-Equivalence** - отношение двух расписаний $` S \stackrel{conflict}{\sim} S^{'} `$, когда одно можно получить из другого, выполнив множество переворотов соседних неконфликтующих операций

**Conflict-Serializability** - свойство расписания $` S: \hspace{0.25em} S \stackrel{conflict}{\sim} S^{*} `$, где $` S^{*} `$ - serial. Это нужный нам подкласс view-serializable расписаний

#### Простота conflict-serializability

Критерий конфликтной сериализуемости довольно прост. Пусть в некотором расписании операции $` \forall o \in T, \forall o^{'} \in T^{'}, T \not = T^{'}: \hspace{0.25em} o \prec o^{'} `$ конфликтуют. Это означает, что операции нельзя поменять местами, сохранив view-equivalence, из чего следует, что в возможной conflict-equivalent сериализации расписания гарантированно $` T \prec T^{'} `$. Т.о. любой конфликт задаёт жёсткое ограничение на порядок транзакций

Возьмём множество всех конфликтов (= ограничений) расписания $S$ и построим по нему $` CG\left \lparen S \right \rparen `$ - **Conflict Graph**

*Утверждение.* $S$ - conflict-serializable $` \Leftrightarrow CG \left \lparen S \right \rparen`$ - acyclic. Доказательство. Напрямую, переворот двух неконфликтных операций не влияет на граф, так что граф расписания эквивалентен графу некоторого conflict-equivalent серийного расписания, в котором все рёбра направлены вперёд, а значит невозможны циклы. И обратно, в ацикличном графе всегда есть вершина без входящих рёбер, все операции которой можно множествами переворотов вынести вперёд и отбросить, что в результате применения для каждой вершины даст серийное расписание

#### Планировщик Strict 2PL

Необходимо построить планировщик, гарантированно порождающий лишь расписания, в графе конфликтов которых нет циклов, и, по возможности, параллельный. И поскольку транзакции интерактивные, планировщик не знает их заранее и управляет на лету. Назовём такой планировщик **Strict 2PL Scheduler**, а его протокол - **Two-Phase Locking**

Имеем Data Store. Свяжем с каждым ключом блокировку. При получении любой операции над некоторым ключом захватываем его блокировку перед использованием хранилища. После выполнения операции блокировка не освобождается, поэтому они копятся в процессе выполнения транзакции. Когда по команде $CommitTx$ все изменения надёжно зафиксированы в журнале, блокировки освобождаются

Очевидно, что транзакции с общими ключами будут упорядочены блокировками, но независимые транзакции могут выполняться параллельно

*Доказательство корректности.* От противного. Предположим в графе конфликтов некоторого порождённого протоколом расписания есть цикл. Ребро в графе означает, что две конфликтующие операции упорядочены во времени. Эти операции брали одну и ту же блокировку, для которой освобождение в первой операции упорядочено во времени до захвата во второй. Два подряд идущих ребра в общем случае обозначают разные блокировки, но по свойству протокола, что освобождаются все блокировки разом, освобождение блокировки в первом ребре происходит во времени до взятия блокировки во втором. Таким образом, если идти по циклу и выстраивать взятия и освобождения блокировок во времени обнаружим, что в конце цикла происходит взятие блокировки уже после её освобождения, что невозможно

В качестве оптимизации можно брать разделяемые блокировки на чтение

TODO: Почему он называется двухфазным? Есть ещё трёхфазный, значит здесь должно быть два действия

#### Deadlocks

Поскольку порядок команд, а значит взятия блокировок определяется пользователем, при работе планировщика возможны дедлоки, которые необходимо уметь избегать. Пусть транзакция на старте получает timestamp по локальным часам. Если блокировка свободна, захватываем её и помечаем выданным timestamp-ом. Если же блокировка занята, сравниваем timestamp-ы и применяем одну из *двух стратегий*
- **Wound-Wait** - если timestamp текущей транзакции больше, отменяем её и перезапускаем с тем же timestamp-ом, иначе ждём освобождения блокировки
- **Wait-Die** - симметрична предыдущей

Остановимся на первой. Она корректна, гарантирует прогресс и справедлива
- *Safety.* В ней невозможны циклы в графе ожиданий, а значит и дедлоки
- *Liveness.* Eventually долго перезапускаемая транзакция станет самой старшей и гарантированно возьмёт блокировку

#### Snapshot Isolation

2PL из-за блокировок на отдельные ключи является очень пессимистичным. Можно построить более оптимистичный планировщик. Пусть в распределённой системе выполняют точечные модификации и чтения огромных таблиц через MapReduce. Очевидно, что чтение выполняется долго и блокирует таблицу целиком, препятствуя любым изменениям, чего хотелось бы избежать

Применим **Snapshot Isolation**. Пусть хранилище мультиверсионное. При команде $CommitTx$ будем порождать новую *immutable* версию хранилища, снабжённую timestamp-ом и которая переиспользует все данные предыдущей за исключением изменённых. При команде $StartTx$ транзакция получает **read timestamp (rts)**, по которому выбирает одну версию хранилища для чтения. Операции записи транзакции буферизуются и используются при команде $CommitTx$, порождая версию с **commit timestamp (cts)**. Для завершения транзакции используется правило **First Committer Wins** - если между rts и cts были изменения по тем же ключам, которые транзакция хочет обновить, коммит невозможен

Открытые вопросы
- как устроены версии и как они хранятся
- как выбирать глобально монотонные timestamp-ы
- как атомарно применить все записи, т.е. проверить, что нет конфликтов с последней версией, и поместить изменения в хранилище

Для поддержания версий можно расширить идею снимков из [семинара 6](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-6.md#snapshot-в-lsm). Возьмём хранилище с `LevelDB` API и вместо последовательных номеров версии будем снабжать ключи timestamp-ами. Сделаем чтение возможным по произвольному timestamp-у, а для записи потребуем timestamp указать. Атомарные обновления, разумеется, будут как-то реализованы через отдельные записи с одним и тем же timestamp-ом. Более подробно этот и другие вопросы рассмотрим в следующих лекциях

#### Аномалия Write Skew

Алгоритм изоляции снимкой не гарантирует сериализуемых изменений. В ней возможна аномалия **Write Skew**. Пусть в системе $x = 0, y = 0$, и пусть две транзакции начались из одной версии, первая читает $x$ и, если он равен $0$, пишет $1$ в $y$ , а вторая делает симметрично. Конфликтов по записям нет, однако в результате в системе изменены оба ключа, что не может быть объяснено никаким серийным исполнением транзакций

С ней связан неловкий момент. В теории баз данных описан и стандартизирован ряд аномалий и придуманы уровни изоляции транзакций, которые от слабейшей к самой сильной исключают всё большее подмножество этих аномалий. Загвоздка в том, что Write Skew не входит в их число, поэтому с точки зрения теории Snapshot Isolation обеспечивает уровень изоляции Serializable, хотя с математической сериализуемым не является. **Поэтому важно, читая документацию по базе данных, понимать, что подразумевается под словом Serializable**

#### Заключение

Два подхода, Strict 2PL и Snapshot Isolation, не являются взаимоисключающими друг друга и согут сосуществовать. 2PL гарантирует сериализацию, но является пессимистичным. Снимки являются оптимистичными и более производительны, но могут порождать несериализуемые истории. Например, `Spanner` комбинирует в себе оба подхода, что описано в статье про эту систему. В `YDB` же используется протокол **Calvin**

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 10](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-10.md)
[Семинар 8 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/seminars/seminar-8.md)
