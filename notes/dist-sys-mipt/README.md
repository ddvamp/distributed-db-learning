# Теория отказоустойчивых распределённых систем (Theory of fault-tolerant distributed systems)

Конспект лекций и семинаров курса Романа Липовского 2020 и 2021 годов прочтения\
[Лекции 2021](https://www.youtube.com/playlist?list=PL4_hYwCyhAvaYKF6HkyCximCvlExxxnrC), 
[Семинары 2021](https://www.youtube.com/playlist?list=PL4_hYwCyhAvZd6B5fN3yAB0zOCjhgpfgg)\
[Лекции 2020](https://www.youtube.com/playlist?list=PL4_hYwCyhAvZaJ3CJlGo9FxOTA2bS1YyN), 
[Семинары 2020](https://www.youtube.com/playlist?list=PL4_hYwCyhAvZTjajkPpwgR29jyx81lMCl)

## Lecture 1. Модель распределённой системы

Все современные сетевые сервисы используют распределённые системы различных типов
- KV Store - гигантская распределённая хэш-таблица с типовыми операциями set(k,v)/get(k), упорядоченностью элементов (like std::map) и со случайным доступом поверх дисков (весь диапазон ключей отображается хэш-функцией, или другим способом, на шарды, в пределах которых ключи упорядочены). Прим: Cassandra, BigTable
- File System - распределённая система хранения файлов огромных размеров или количеств, не умещающихся на дисках одной машины, с последовательным доступом к данным. Отличаются от KV паттерном доступа. Прим: GoogleFS, HDFS
- Coordination Service - распределённая система для построения других распределённых систем (предоставляет примитивы синхронизации и согласованности: выбор лидера, lease и т.д.). По сути пачка атомиков: for distr. system as atomic for concurrency. Прим: ZooKeeper, Chubby
- Message Queues - (асинхронный) транспорт данных, когда много продъюсеров и консъюмеров. Прим: Apache Kafka
- Databases - таблицы/колонки, индексы, транзакции. Самый сложный класс DDS. Прим: Google Spanner, CockroachDB

В первую очередь всё это разные модели данных, и лишь во вторую имеют некоторые различия в построении. Также все эти модели имеют нераспределённые варианты

Причины использовать распределённые системы
- масштабируемость (Scalability) - возможность свободно добавлять новые вычислительные ресурсы/дисковые ёмкости, или же расти горизонтально, адаптируясь к нагрузке (когда стало слишком много данных/операций для одной машины). Также свобода от vendor-lock - способность системы работать на обычном оборудовании
- отказоустойчивость (Durability + Reliability + Fault-tolerance + Availability) - машина/диск может сбоить или даже отказать, поэтому в большом кластере всегда есть машины на обслуживании. Необходимо переживать сбои узлов, автоматически (без помощи человека) адаптироваться и скрывать это от пользователя. Также возможны проблемы сети/электросети и человеческие ошибки
  - Durability (Надёжность хранения) - успешно завершённые операции не будут потеряны, даже при сбое системы. Надёжная запись на диск
  - Reliability (Надёжность работы) - система выполняет свои функции корректно и стабильно в течение длительного времени. Думаем как предотвратить сбой узла
  - Fault-tolerance (Устойчивость к сбоям) - система продолжает работу (хотя бы частично) при отказах компонентов (узлов, сети, дисков и т.п.). Если сбой узла/сети произошёл, думает как не пострадать
  - Availability (Доступность) - система отвечает на запросы в разумное время — даже если часть компонентов недоступна. 4/5 девяток - сколько времени (процент) в год сервер может простаивать и не отвечать на запросы
- безопасность (Safity) - нельзя доверять машине/группе машин, поскольку они могут оказаться злоумышленниками и обманывать другие машины. Особенно важно в блокчейне

Эти причины независимы. К примеру, отказоустойчивость может быть необходима там, где данные помещаются на одну машину (ZooKeeper). Аналогично с безопасностью (биткоин)
- согласованность (Consistency) - ещё одно свойство распределённая систем, гарантировать в той или иной степени, что геораспределённые клиенты будут способны конкурентно взаимодейтсовать с системой и видеть непротиворечивые данные

Описание модели\
Чтобы говорить о распределённых алгоритмах, нужно ввести модель, отражающую реальный мир без излишних подробностей. Её цель не в точности описать реальность, но позволить строить эффективные, полезные и отказоустойчивые алгоритмы. Доказав алгоритмы в такой модели можно утверждать, что они будет верны и в реальном мире\
Система состоит из
- узлов
- стрелочек (сеть + сообщения)
- клиентов

Узел (node, process в старых статьях/книгах) - отдельный компьютер (процесс/сервер) с запущенными сетевыми сервисами. Узлы умеют общаться между собой через отправку односторонних сообщений (message) без ожидания ответов (reply). Для простоты, между каждой парой узлов есть абстракция прямого и надёжного провода (link, TCP), по которому движутся сообщения. Узлы видят только её. Сеть суть совокупность проводов\
От модели ждём не стоимостей, а гарантий. Например, выполнили асинхронную отправку сообщение m узлу p Send(m, p). Как долго сообщение будет доставляться и будет ли доставлено вообще не знаем. В реальности, вместо ответов применяется механизм Retry-ев, сети более сложные чем прямой провод, а скорость передачи ограничена физически (скорость света + среда/материал) и географически (удалённые узлы, локальность), что вносит задержки, и распределённые системы должны это учитывать\
Узлы общаются через Message Passing, клиенты взаимодействуют с узлами по модели Client-Server - посылают запросы и ждут ответы (в реальности, узлы также могут использовать модель клиент-сервер, обычно RPC). Способы взаимодействия клиентов с сервисом: http api, RPC (Remote Procedure Call), client library (плюсовые обёртки над другими API + полезная функциональность, как например ASIO). Клиенты (почти) не знают об узлах. За это отвечают модели согласованности. Есть единый сетевой адрес для клиентов, запросы в который через proxy напрявляются на конкретные узлы (либо балансировка на уровне клиента)\
Клиент выполняет запрос Set(k, X) и ждёт подтверждение ack. Если затем он сообщит об этом другому клиенту по внешнему каналу связи, тот ожидает увидеть X, т.е. наличие причинности. Клиенты не наблюдают распределённость и не думают о ней. Для них система выглядит как бесконечно ёмкий, бесконечно мощный, бесконечно устойчивый компьютер. В коде клиента система выглядит как конкурентный между пользователями объект, поэтому внутри системы удобно использовать модель Message Passing, а снаружи модель Shared Memory (Consistency models). При наличии причинности система должна учитывать возможность внешней коммуникации при обеспечинии согласованности. Также возможны распределённые системы, которые заставляют пользователей передавать между собой служебную информацию для обеспечения причинности по side-channel (за пределами системы, Прим: Cassandra, Dynamo)

Гарантии факта доставки сообщения
- Fair-loss link - модель честного канала: если узел A будет бесконечно отправлять сообщение узлу B, то второй будет бесконечно его получать. Нет момента, когда сообщение навечно перестало доходить. Однако, нет порядка сообщений, нет гарантии доставки, возможны дубликаты. С такой абстракцией неудобно работать
- Reliable link - модель надёжного канала: каждое однократно отправленное сообщение будет доставлено, но могут быть дубликаты, и нет порядка
- Perfect link - модель идеального канала: каждое однократно отправленное сообщение будет доставлено и ровно один раз, но нет порядка. Прим: TCP - отслеживание дублей, повторная отправка до подтверждения, упорядочение на стороне доставки. Используется в литературе
Эти модели не рассматривают реальность (выход из строя узлов/сети), но требуют, чтобы на какой-то период времени сеть стала "идеальной". Также эти модели не допускают сообщений из воздуха
- FIFO link - модель канала, гарантирующая порядок. Ортогональна другим

Будем работать в немного изменённой Reliable модели, где сообщение либо доставится, либо соединение сообщит о разрыве

Гарантии времени доставки сообщения
- Synchronous model - синхронная модель: задержка доставки сообщений ограничена сверху (delay(m) <= delta); непрактична (невозможна), поскольку в реальности система ненадёжна (разрывы, баги в коммутаторах, трафик, обслуживание, человечиские ошибки и т.д.)
- Asynchronous model - асинхронная модель: границы доставки сообщений нет. В этой модели доказывается корректность алгоритмов, т.е. что система обладает свойством Safity - не уходит в некорректные состояния (не делает плохого). Но такие алгоритмы/системы бесполезны
- Partially Synchronous model - частично синхронная модель: модель асинхронная (произвольно долго), но в некоторый момент времени t* гарантированно станет синхронной (произвольно коротко) (например, сеть перестала сбоить). Алгоритм должен быть устойчив к худшему сценарию, он не знает t*, но ожидает его. Когда сеть начинает работать стабильно, алгоритм совершает прогресс. В этой модели доказывается, что система делает что-то хорошее, т.е. обладает свойством Liveness - eventually система совершит прогресс (что-то хорошее)\
Говоря иначе, partially synchronous model означает, что сеть работает непредсказуемо и не даёт никаких гарантий о времени доставки, но время от времени такие гарантии появляются

Моделирование сети
- Partition - разбиение системы в результате изменений в сети (отказ link-ов) на два связных кластера, полностью изолированых друг от друга. Также разделяются и пользователи (по модулю проксирования запросов на узлы)
- Split Brain - ситуация, когда кластеры по partition начинают работать самостоятельно как отдельные системы. Эту ситуацию нужно предотвращать, поскольку она ненаблюдаема до (из-за локальности) и невосстановима после починки сети: пользователь пронаблюдает наличие узлов. Вариант решения: одна из частей должна остановиться до починки (CAP теорема), или обе части дожны перестать работать на запись (?но кажется здесь возможна проблема отсутствия последних записей)

Текущая модель учитывает partition - случай бесконечно медленных link-ов на некотором срезе

Моделирование узлов\
Узел для целей модели есть однопоточный автомат/актор, который принимает сообщения из сети, вызывает обработчики, меняет состояние и, возможно, отвечает сообщениями в сеть/систему. Многопоточность в реализации лишь добавляет производительность и удобство построения, но не влияет на свойства системы. Для доказательства теорем/алгоритмов можно думать, что все действия узла атомарны, что не повлияет на корректность реальных (неатомарных) систем\
Полагаем, что скорость работы узлов произвольная (началась сборка мусора/удаление большой структуры объектов). Узел может выйти из строя (Crash) ("взорваться", отказ диска). Узел может перезагрузиться (Restart), что требует добавить персистентное состояние (оперативная память + жесткий диск). Также возможны Византийские отказы (Byzantine), когда узел ведёт себя произвольным/непредсказуемым образом, например является злоумышленником и отвечает всем по разному. Последнее сильно усложняет алгоритмы (закладываются на заданный процент отказных узлов), но и учитывается лишь в безопасных системах (например, криптовалюты)\
В зависимости от алгоритма/архитектуры узлы могут быть как равноправными, так и иметь роли и меняться ими

Время\
Считаем время (time) абсолютным для всех узлов и представим в виде общей оси времени, на которой любое событие в системе имеет координату. Время не влияет на Safity, но влияет на Liveness. Однако вместо времени узлы имеют доступ к часам (clock), которые возвращают timestamp-ы. В идеале это тождество c(t) -> t, но в действительности (коде) Now() -> t, например кварцевые (quartz, в ПК) или атомные (atomic, точные и дорогие, на спец. машинах) часы. Часы строят из объектов реального мира, поэтому они имеют погрешности, что необходимо учитывать в алгоритмах\
Принципиально все использования часов сводятся к двум действиям
- t1 < t2 - сравнение timestamp-ов для упорядочивания событий (согласование порядка действий с внешней причинностью)
- t2 - t1 - вычисление интервала времени для установки timeout-а или организации failure-detection (время ожидания подтверждения доставки сообщения до узла, время отчёта лидера об его активности - heartbeat-ы)

Погрешности часов
- skew - рассинхронизация часов на разных узлах; проблема для упорядочивания (порядки по времени и по часам не сходятся). Прим: клиент A выполнил set(k, X) на узле со временем 12:00, после чего сообщил это клиенту B, который выполнил set(k, Y) на узле со временем 11:59 - записи переупорядочились
- drift (дрейф, скитание) - явление нестабильности скорости измерения времени часами (неединичность производной); проблема для точных интервалов. ppm (parts per million, миллионные доли) - на сколько микросекунд часы могут отклониться за одну секунду. У кварцевых часов это 20 ppm (~1.7 секунд в сутки)

Конец описания модели

Синхронизация часов (невозможна)\
drift неизбежен, но можно ли устранить skew - свести часы к одному показанию, не обязательно к реальному времени
Запишем на первом узле t1 = Now() и запросим время у другого узла, получим ответ t3 и запишем t2 = Now(). Примерная задержка d ~= (t2 - t1) / 2. Значит на другом узле сейчас t3 + d (если сеть симметрична)\
Положим для сообщений delay(m) in [dt - u, dt], где u - uncertainty (неопределённость), и дрейфа нет. Синхронизировать часы означает выбрать для всех узлов некоторую поправку O к их времени: SCi(t) = Ci(t) + Oi. Предположим есть произвольный алгоритм, который после запуска за некоторое время даст |SCi(t) - SCj(t)| = |Oi - Oj| <= E. Определим нижнюю оценку E, т.е. насколько точно часы можно синхронизировать\
Пусть мы управляем миром, в том числе работой сети (задержками) и начальным отклонением часов (иными словами, можем вообразить любые возможные исполнения, полезно применять эту технику при доказательстве алгоритмов). Построим два различных исполнения, в которых поправки вычисляются независимо, и в общем случае должны различаться, но таких, что алгоритм не сможет уловить различий и выберет в обоих случаях одинаковые поправки\
Положим сообщения 1 -> 2 идут dt, максимально долго, обратно 2 -> 1 идут dt - u, максимально быстро. На такой конфигурации запускаем алгоритм и вычисляем E. Затем делаем shift - оставляем события на первом узле на тех же местах, но переворачиваем сеть (1 -> 2 dt - u, 2 -> 1 dt), и также немного переводим часы n2 (на + u), чтобы ответы остались такими же. Оба узла разницы не видят, значит алгоритм выберет одну и ту же поправку O, но для второго узла C'2(t) = C2(t) + u => SC'2(t) = SC2(t) + u. Поскольку разница между двумя часами (узлов) может быть от -E до E, оба измерения вторых часов должны лежать в этом диапазоне (относительно SC1(t), который не изменился), ровно как и u: 2E >= u, E >= u/2. В случае n узлов E >= (1 - 1/n) * u\
Т.о. мы можем померить лишь время roundtrip, но не асимметрию сети. Стоит отказаться от синхронизации времени, поскольку время это разделяемое состояние (нечто общее) между узлами, которое не является когерентным. Именно поэтому safety не зависит (не должно зависить) от времени, лишь liveness\
Если отказаться от асимметрии (например, синхронизировать одни часы по другим, но не обратно), всё становится лучше

Почему GPS синхронизирует часы\
Для определения положения, кажется, достаточно 3 спутников (пересечение трёх сфер спутников и "сферы" земли даёт две точки, одна из которых находится в космосе). Расстояние до спутника определяется так |X - Xi| = Di. Однако часы на устройстве имеют погрешность d, так что на самом деле |X - Xi| = v * (deltaTi + d), что складывается в 4 неизвестных и 4 уравнения (навигационные уравнения GPS). Поэтому побочным результатом решения системы является определение погрешности часов. Часы на спутниках являются точными (в том смысле, что у них очень маленький drift, и их постоянно синхронизируют)

Google.TrueTime - локальный сервис по синхронизации часов. TT.Now() обращается к локальному состоянию на узле и получает [e(arliest), l(atest)], в пределах которого находится истинное время. Гарантируется, что [t0, t1], где t0 - истинное время отправки запроса и t1 - истинное время получения ответа (например, внутри Spanner), пересекается с [e, l] хотя бы в одной точке, и что l - e <= 6ms. Для этого каждые 30 секунд демон сервиса TT на узле синхронизирует локальное состояние (интервал [e, l]) с одним из тайм мастеров кластера (в пределах одного ДЦ), а ppm устанавливается в 200, что заведомо больше, чем в часах машины. Тогда при запросе Now в некоторый момент времени сервис сдвигает e по монотонным часам от точки синхронизации c ppm = -200, а l с ppm = 200 (таким образом интервал с настоящим дрейфом заведомо попадает в вычисленный)\
Тайм мастеров два вида, либо узел с GPS антеной (для синхронизации своих часов по GPS), либо Armagedon Master с атомными часами. Они независимы и имеют разные сценарии отказа\
TrueTime используется для замены долгого межконтинентального общения (~10-100ms) ожиданием не дольше 6ms (описание в след. лекции)\
Этот сервис был построен для использования в Google Cloud Spanner

Safety - без учёта времени\
Liveness - с учётом времени, не нарушая safety

Seminar 1. Среда исполнения распределённой системы

Геораспределённая система - система, узлы которой расположены на большом расстоянии друг от друга

Leap second (добавочная секунда) - поправка ко времени из-за неравномерного вращения Земли\
В мире существует несколько временных осей, которые возникли из-за leap second
- TAI - время без leap seconds
- GPS - время в одноимённой системе (учитывает leap секунды до момента запуска)
- UTC - "настоящее" время (со всеми leap seconds)

Unix time не монотонно, как из-за наличия leap seconds, так и в результате синхронизация с gps (протокол ntp). Поэтому возможно, что две секунды подряд идёт одинаковый timestamp, либо даже timestamp уменьшается. Нельзя в распределённых системах полагаться на монотонность течения (общего) времени (по временным осям). Поэтому в компьютерах существует двое часов
- wall clock (настенные, календарные, std::system_clock) - имеют общую точку отсчёта на разных узлах, но не монотонны (могут быть скорректированы). Могут быть использованы для сравнения времени на разных узлах
- monotonic clock (std::steady_clock) - не имеют единой точки отсчёта, но монотонны (например, монотонность устанавливается от запуска системы). Позволяют измерять диапазоны локально

Важно понимать, что независимо от вида часов, они подвержены дрифту. Их различие в наличие/отсутствии корректировки времени

Узлы (сервера) собираются в стойки (rack) под объединением коммутатора (top-of-rack/ToR switch), стойки собираются в кластера (cluster) под объединением кластерного коммутатора (cluster switch), кластера собираются в датацентры, датацентры собираются в геораспределённую сеть\
В правильно организованной инфраструктуре отсутствуют single point of failure (SPF, единые точки отказов) - точки, отказ в которых приводит к падению всей системы. Для этого необходимо размещать части системы (в том числе по репликам) в разных доменах отказа. Failure domain (домен отказа) - часть системы, выходящая из строя из-за одного локального отказа. Диск -> узел, коммутатор -> стойка, коммутатор -> кластер. Необходимо применять rack awareness - реплики в разных стойках. Также следует стремиться уменьшать количество самих точек отказа, напрмер, через избыточность (запасные коммутаторы, несколько путей в сети между двумя узлами, raid-массивы и т.д.). Вариант уменьшения рисков отказа - резервация коммутаторов от разных производителей (разные баги и условия выхода из строя). Но в месте с ростом надёжности увеличивается расстояние между узлами и издержки, поэтому растёт latency, так что важно находить балланс. Прим: фабрики google или facebook

Недоутилизация - ситуация простаивания избыточных мощностей/ресурсов, имеющихся для целей пережить пик нагрузки и отказоустойчивости. Или же когда ты разделяешь пользователей локально по частям системы, которая становится активной лишь в определённое время (например, когда пользователи не спят)\
Именно поэтому вместо большого количества маленьких кластеров под отдельные сервисы стали строить один большой кластер на все сервисы сразу. В нём должна быть избыточность маршрутов (для предотвращения потери связности), высокая пропускная способность любого разреза, а также он должен легко масштабироваться без необходимости использовать специальное оборудование (достаточно того, что на рынке)

Vendor lock - зависимость от производителя, когда определённым товаром или товаром с конкретными свойствами торгует единственный производитель. Также возникает при экстримальном вертикальном масштабировании (процессор с самым большим числом ядер, самый вместительный диск и т.д.). Уменьшает надёжность системы

Почему между датацентрами на разных концах мира есть задержки\
Идеал - скорость света. Материал оптоволокна неидеален, преломления, затухания сигнала, точки сварки, коррекция ошибок, разрыв кратчайших путей, соображения надёжности (не прямой путь), экономические соображения (провода к промежуточной точке в стороне, постройка датацентра и аренда кабеля вместо своего кабеля), географические особенности (реки, населённые пункты, существующие ж/д пути)

TCP является распределённой системой, и эта абстракция соединения (потока байт) существует лишь на уровне ОС двух участников (TCP стек находится в ядре, и ОС управляет соединениями, не на уровне сети/проводов/коммутаторов/маршрутизаторов, IP работает с Datagrams), т.е. TCP не имеет физического представления. Проблема TCP в том, что абстракция прямого провода (машина-машина) не соответствует физической действительности (в сети много пар машин, которые пользуются проводами сообща), поэтому нельзя писать сколько захочешь: перегрузка сервера, перегрузка сети, о которых не знают TCP концы (Flow control, Congestion control). Например, для отправки пакетов существует окно подтверждений, которые ещё не получены, и это окно при получении таймаута уменьшается в 2 раза. Также TCP подвержен проблеме синхронизации часов: при гибели процесса ОС закроет соединение и пошлёт сигнал fin; если на другом конце машина перегрузится, она ответит, что не знает нас (соединение разорвано); если машина упадёт, мы будем сперва успешно писать в буфер, затем при его заполнении долго ждать отправки с ретрансмитами до таймаута. Так нельзя понять, что происходит в остальном мире прямо сейчас\
TCP медленный, т.е. из-за flow и congestion control он начинает с маленьких скоростей прощупывать сеть и разгоняется\
В действительности существует множество десятков версий протокола TCP

Современные распределённые системы начинают работать в облаках, поэтому для высокой производительности должны учитывать виртуализацию, контейнеры, планировщики кластеров и разделение ресурсов, и т.п.
