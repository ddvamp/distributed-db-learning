## Seminar 4. Распределённая файловая система, GFS

Две причины хранить файлы в DFS
- отказоустойчивость - DFS позволяет не думать об отказах дисков/машин
- данные не помещаются на диск одной машины из-за огромных размеров или количеств
В последнем в зависимости от случая используются разные решения, оба на RSM. И в отличие от LFS (локальная файловая система) размер файлов неограничен

KV Storage vs DFS. Важно понимать, что обе распр. системы хранят данные, но
- KV - просто структура данных словарь
- DFS - настоящая файловая система

Откуда возникает необходимость работы с огромными объёмами данных. К примеру статистика, которую аналитики хотят обрабатывать, что невозможно на одной машине, и чтобы не думать о распределении и отказах, используют MapReduce с декларативным описанием. Сам MapRecude не хочет думать о хранении данных и использует DFS (GFS, затем Colossus). Или облако предоставляет вместе с сервером Persistent Disk - диск, не умирающий вместе с машиной; интерфейс диска, эмулированного в DFS для локальной FS

Нельзя написать POSIX совместимую систему, поскольку не каждую операцию можно легко сделать распределённой. Необходим простой API, и его выбор влияет на многое
- Create(p(ath))
- Delete(p)
- Copy(d(st), s(rc))
- PRead(p, o(ffset), s(ize)) - для простоты вместо fd используем path
x Write(p, o, d(ata)) - оффсет выбирает пользователь
- Append(p, d) - оффсет выбирает система
Реализация одной или обоих из Write/Append колоссально влияет на дизайн и построение DFS. Здесь легко ошибиться. Упростим систему и уберём Write

DFS вместо физического устройства хранения (HDD/SSD) работает с Block Device (блочное устройство) - абстракция достаточно большого блочного хранилища. Оно является для DFS как KV Storage для распр. DB. Блок (Block, B) есть единица чтения/записи и хранения фиксированного размера, он реплицирован, и все операции выражены через него. В распр. случае блочное устройство есть кластер, конкатенация дисков машин. DFS укладывает файлы в блоки, не обязательно подряд (особенно при разорванных во времени Append-ах; paging, ограничения хранения, фрагментация), поддерживает иерархию путей/имён и для каждого файла хранит информацию Inode: список блоков, их размер, аттрибуты. Метаданные (иерархия + Inodes) так же хранятся в блоках
Разделим данные и метаданные на два логических уровня. За них будут отвечать разные подсистемы
- Chunk Store - данные
  - Put(d) -> id - положить данные в чанк и получить его id
  - Get(id) - вычитать чанк
- Meta Store - иерархия путей + inodes
  - AppendChunk(p, id) - добавить чанк в список
  - ListChunks(p, o, s) -> [id1, id2,...] - получить список чанков с началом по смещению
Чанк, поскольку в распр. системах вместо блоков оперируют реплицированными чанками (размера ~1GB). Подсистемы будут размещены на разных машинах и иметь разные гарантии. Отметим, что Meta Store по сути дерево, в узлах которого лежат списки блоков (Inode)

Read = ms.ListChunks + cs.Get - последовательно вычитывает чанки с префетчем
Append = cs.Put + ms.AppendChunk - записывает данные в чанк
Поскольку есть только Append, заполненные чанки становятся иммутабельными, и при записи в файл лишь добавляются новые чанки. Если в конце файла чанк заполнен не полностью, при последующей записи он заменится новым чанком. Создание/удаление работает лишь с метаданными, копирование копирует список чанков и последний чанк

Данные первичны, и их объём может достигать ~10-100 PB. Объём метаданных зависит от
- # файлов - повлиять трудно
- # чанков - можно регулировать размером
Чем меньше чанков, тем эффективнее работа TCP, но выше фрагментация. Выберем размер чанка ~1GB. Нужно заставить клиента делать большие Append, пусть он копит данные (иначе будет плохой Write Amplification)
Данных много, и Chunk Store должен быть неограниченно масштабируемым. Meta Store сильно отличается от него, что видно из API. Операции Chunk Store либо независимы между собой (пара Put или пара Get), либо data dependency ordered (hb), т.е. индифферентны к переупорядочиванию. Операции Meta Store API напротив, чувствительны к переупорядочиванию, что способствует нарушению инвариантов. Поэтому Meta Store реализуется через RSM, а для Chunk Store достаточно G-Set (структура типа CRDT, conflict-free replicated data types) с моделью Eventual Consistency (для которой CRDT специально и придумали)

Представим наивную реализацию. Есть пул машин Block Device, и есть спец. машины Meta Store на RSM, реплицирующие дерево. Chunk Store имеет выделенный узел - контроллер (в GFS Chunk Master). Он обрабатывает входящие операции и хранит в памяти карту кластера (отображение id чанка в список его реплик, аналог преобразования логической адресации в физичискую для дисков). При Put контроллер выбирает id и пишет данные на 3 реплики, 2 синхронно и 1 асинхронно (чуть сложнее, он мог бы возвращать список реплик, куда писать данные). По rack awareness выбираем реплики в разных стойках. На случай рестарта (потери карты) каждая реплика периодически отсылает ему список своих чанков
Устраним единую точку отказа, сделав несколько контроллеров. Операции независимы, логический порядок им не нужен, значит они могут приходить на любой контроллер. Тогда возможна ситуация, что запись чанка и его чтение попали на разные контроллеры, и во втором ещё нет отображения. Пусть контроллеры взаимодействуют, и реплики отсылают информацию им всем, так что eventually каждый контроллер будет знать всю карту. Операция завершится успешно, возможно, с более долгим ожиданием (таймауты и ретраи)
Схема получается простой, поскольку мы реплицируем растущее множество без удалений - G-Set. Реплики не общаются между собой (нет Broadcast-а). Это повышает отказоустойчивость: и среди контроллеров, и в replica set, могут отказать все узлы кроме одного. Это упрощает масштабирование: если карта перестала помещаться в память контроллеров, нужно продублировать систему - шардировать. Здесь важно понимать, что Chunk Store это совокупность шардов, и именно он занимается отображением id в шард. Когда на протяжении некоторого продолжительного времени реплика перестаёт отвечать, контроллер заменяет её новым узлом. Также можно добавить Check Sums, например id is hash
Ещё одно улучшение: Erasure Codes. Это применение кодов Рида-Соломона, для уменьшения количества реплик при той же отказоустойчивости, но ценой оверхеда 50% на объём данных. В проде применяется LRC (коды с локальной реконструкцией) с оверхедом 33%, но ценой того, что иногда прихоится пересчитывать данные (tradeoff disk <-> cpu)

Google BigTable - LSM поверх DFS

В GFS много плохих решений, которые исправили в Colossus, где нет Write, только Append

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 4](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-4.md)
[Лекция 5 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-5.md)
