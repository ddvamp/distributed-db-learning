## Seminar 4. Распределённая файловая система, GFS

#### Мотивация использовать DFS

Будем говорить о задаче реализации DFS. И KV Store, и DFS, обе системы для хранения данных, но KV просто словарь, а DFS полнофункциональная файловая система

Причины использовать DFS
- *отказоустойчивость* - позволяет не думать об отказах дисков/машин. Виртуальный жёсткий диск
- *большой объём данных* - позволяет работать с файлами огромных размеров или количеств, не умещающихся на диске одной машины. В зависимости от случая решение будет отличаться, но они оба используют RSM. И в отличие от **LFS** (локальная файловая система) нет ограничения на размер файлов

Первую DFS написали в `Google` для реализации системы **MapReduce** - фреймворк с декларативным описанием задач по обработке больших объёмов данных, например статистики аналитиками, где не нужно думать об отказах и заниматься ручным разбиением и распределением задач, а просто описать композицию (Map + Reduce). MapReduce занимается распределением вычислений, но не хочет думать о хранении данных, поэтому использует DFS (изначально `GFS`, затем `Colossus`)

Другой пример применения DFS это **Persistent Disk** - абстракция отказоустойчивого диска, эмулированного в DFS, интерфейс которого предоставляется для LFS сервера, арендуемого в облаке. Persistent, потому что он не умирает при смерти машины сервера

Разработаем дизайн DFS

#### Продумываем API

Нельзя написать POSIX совместимую систему, поскольку не каждую операцию можно легко сделать распределённой. Необходим простой API
- $Create \left \lparen p \right \rparen, \hspace{0.25em} p - path $
- $Delete \left \lparen p \right \rparen $
- $Copy \left \lparen d, s \right \rparen, \hspace{0.25em} d - dst, \hspace{0.25em} s - src $
- $` PRead \left \lparen p, o, s \right \rparen, \hspace{0.25em} o - offset, \hspace{0.25em} s - size `$ - для простоты вместо fd используем path. fd реализуются на уровне выше, который не рассматриваем
- ❌ $` Write \left \lparen p, o, d \right \rparen, \hspace{0.25em} d - data `$ - оффсет записи выбирает пользователь
- $` Append \left \lparen p, d \right \rparen `$ - оффсет записи выбирает система

Выбор, реализовать $Write$, $Append$ или обе, колоссально влияет на дизайн и построение DFS. Здесь легко ошибиться. Упростим систему и уберём $Write$, мотивируя это в конце

#### Архитектура LFS

Чтобы понять, как можно построить распределённую файловую систему, нужно взглянуть на локальную. LFS использует физические устройства хранения (HDD/SSD) не напрямую, а работает с **Block Device (блочное устройство)** - абстракция достаточно большого хранилища, поделённого на блоки. **Блок (Block, B)** есть единица чтения/записи и хранения фиксированного размера, и через него выражены все операции. Содержимое файла укладывается в блоки, не обязательно подряд (особенно при разрозненных во времени $Append$). Также для файлов нужно поддерживать иерархию путей/имён и хранить для каждого информацию **Inode** (список блоков, размер, аттрибуты). Всё перечисленное, **метаданные**, тоже хранится в блоках в отдельной части FS. Дополнительно могут храниться карта свободных блоков, лог изменений и другие вспомогательные структуры

DFS можно строить так, чтобы в ней использовался уровень распределённого Block Device, который отвечает за отказоустойчивое хранение блоков как шардов, играя схожую роль что и KV Store для DDB. Мы пойдём другим путём и не будем так делать

#### Данные и метаданные

Выделим два логических уровня, данных и метаданных. За них будут отвечать разные подсистемы DFS, имеющие разные гарантии и размещённые на разных машинах
- **Chunk Store** - отвечает за хранение данных. **Чанк** суть синоним блока в распределённых системах
  - $` Put \left \lparen d \right \rparen \to id `$ - положить данные в чанк и получить его id
  - $` Get \left \lparen id \right \rparen `$ - вычитать чанк
- **Meta Store** - отвечает за хранение метаданных. По сути дерево с Inode в узлах
  - $` AppendChunk \left \lparen p, id \right \rparen `$ - добавить чанк в список
  - $` ListChunks \left \lparen p, o, s \right \rparen \to \begin{bmatrix} id1 & id2 & \dots \end{bmatrix} `$ - получить список чанков с началом по смещению

#### Последствия отказа от Write

- $Read = ms.ListChunks + cs.Get$ - последовательно вычитывает чанки с префетчем
- $Append = cs.Put + ms.AppendChunk$ - записывает данные в чанк

Поскольку есть только $Append$, записанные в Chunk Store чанки становятся иммутабельными, и при записи в файл лишь добавляются новые чанки. Если чанк в конце файла заполнен не полностью, при последующей записи он заменяется новым чанком. Создание/удаление работают лишь с метаданными, возможно, с очисткой данных в фоне. Копирование копирует Inode и последний чанк

#### Дизайны Chunk и Meta Store

Данные первичны, и их объём может достигать $` \sim 10-100PB `$. Объём метаданных зависит от
- числа файлов - повлиять трудно
- числа чанков - можно регулировать размером

Чем меньше чанков, тем эффективнее работа TCP, но тем выше фрагментация. Выберем размер чанка $` \sim 1GB `$. Чтобы уменьшить фрагментацию (и улучшить Write Amplification), нужно заставить клиентов делать большие $Append$. Пусть они копят данные и при записи (почти) заполняют чанки

Данных много, поэтому Chunk Store должен быть неограниченно масштабируемым. Meta Store сильно от него отличается, как видно из API. В силу иммутабельности чанков операции Chunk Store либо независимы, либо зависят по данным (нельзя прочитать чанк не имея $id$), и т.о. индифферентны к переупорядочиванию. Операции Meta Store, напротив, могут менять файлы и должны выполняться в заданном пользователем порядке, который при распределении должен едино поддерживаться на всех узлах. Поэтому Meta Store реализуется через RSM, а для Chunk Store достаточно **G-Set (Growing Set)** - структура типа **CRDT (conflict-free replicated data types)** с моделью Eventual Consistency

Наивная реализация. Есть пул машин в качестве огромного масштабируемого "диска". Есть специальные машины Meta Store (RSM), которые самостоятельно хранят метаданные в виде реплицированного дерева. И есть Chunk Store, который имеет выделенный узел - "*контроллер* диска" (**Chunk Master** в `GFS`). Контроллер обрабатывает входящие операции и хранит у себя в памяти карту кластера (отображение $id$ чанка в список его реплик, аналог преобразования логической адресации в физичискую для дисков). При $Put$ контроллер выбирает $id$ и надёжно сохраняет данные, в самом простом варианте на $3$ реплики, $2$ синхронно и $1$ асинхронно (в реализации ждёт $2$ первые фьючи). Чтобы не пропускать данные через себя, контроллер мог бы возвращать список реплик для записи (вопрос - кому?). По rack awareness реплики находятся в разных стойках. На случай рестарта (потери карты) каждая реплика периодически отсылает контроллеру список своих чанков

Один контроллер это единая точка отказа, поэтому сделаем их несколько. Поскольку операции независимы и им не нужен логический порядок, можно посылать их на любой контроллер. Тогда возможно, что операция чтения чанка попадёт не на тот контроллер, что его писал, и о нём не будет информации в карте. Это не проблема, поскольку реплики отсылают отображения всем контроллерам, и ещё можно как-нибудь заставить контроллеры общаться между собой, так что eventually каждый контроллер будет знать всю карту, а операция завершится, возможно, с более долгим ожиданием

Благодаря отсутствию $Write$ и G-Set схема получилась простой. Репликам не нужно взаимодействовать, благодаря чему выше отказоустойчивость. Система способна пережить отказ всех контроллеров кроме одного. Благодаря тому, что чанки иммутабельны, и на реплике чанк либо пуст, либо в конечном состоянии, можно также пережить отказ всех реплик кроме одной. Когда реплика не отвечает на протяжении некоторого продолжительного времени (отвечают $2$ из $3$), контроллер заменяет её новым узлом. На случай ошибок дисков можно применять CheckSum. Также здесь простое масштабирование. Если карта перестала помещаться в память контроллеров, или запросов стало слишком много, можно просто продублировать систему (пул машин + контроллеры)

Для уменьшения числа реплик при той же отказоустойчивости можно применить **Erasure Codes**, например коды **Рида-Соломона** с оверхедом $` 50\% `$ на объём данных. В проде применяются **LRC (коды с локальной реконструкцией)** с оверхедом $` 33\% `$, но ценой того, что иногда приходится пересчитывать данные (tradeoff disk $` \leftrightarrow `$ cpu)

Используя DFS с последовательными записями в конец как абстракцию распределённого отказоустойчивого устройства хранения, можно построить поверх него LSM и, добавив координирующий слой, реализовать KV Store. Именно так устроена `BigTable`

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 4](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-4.md)
[Лекция 5 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-5.md)
