## Seminar 2. Реализация регистра в коде

#### Роли узлов

Реальные системы состоят из многих узлов, и их конфигурация постоянно меняется (отказы, обслуживание, расширение, проблемы сети), поэтому клиенту неудобно использовать конкретные адреса реплик и самому собирать кворумы (с некоторыми исключениями, см. `Apache Cassandra`). Вместо этого клиент посылает запрос на некоторый узел координатор, который выполняет операцию

Реплика с одной стороны является копией регистра, а с другой координирует работу с регистром в целом, т.е. имеет две **роли**
- **Coordinator** - *активная роль (алгоритмическая*), методы $Get \left \lparen \right \rparen /Set \left \lparen v \right \rparen $: при запросе от клиента инициирует работу с регистром как единым объектом (выбирает $ts$, собирает кворумы и т.д.)
- **Replica** - *пассивная роль (структурная)*, методы $LocalRead \left \lparen \right \rparen /LocalWrite \left \lparen v, ts \right \rparen $: при запросе от координатора работает со своей копией регистра (сравнивает $ts$, обновляет значение, отвечает координатору и т.д.)

Для удобства координатор также отправляет команду сам себе (*loopback интерфейс*) как реплике. В коде разумно декомпозировать роли в два интерфейса (базовых класса)

#### RPC

Вместо двух операций $Send \left \lparen m, p \right \rparen , HandleMessage \left \lparen \right \rparen $ и модели Message Passing удобно реализовать роли в отдельных сервисах и общаться с ними едиными запросами по модели Client-Server как снаружи, так и внутри. В коде вместо callback-ов используется **RPC**, т.е. общаемся с репликой через объект и его методы. Таким образом работают библиотечные клиенты на С++ различных распределённых систем, $client = KVStore \left \lparen \right \rparen; client.Set \left \lparen k, v \right \rparen $. Цель RPC - убрать из кода особенности сетевого взаимодействия, а также связать запрос и ответ

#### Concurrency

Одновременно на координатор приходит много запросов, поэтому в коде они представлены файберами/корутинами. Внутри файбера асинхронно собираются два последовательных кворума через параллельную композицию фьюч, полученных от RPC запросов к репликам. Клиенты также посылают RPC запросы и работают с фьючами

#### Cancellation

Сеть ненадёжна, поэтому необходимо снабдить запросы механизмом Retry-ев. Это делается на уровне абстракции канала (реализация Reliable link-а), например, спрятать внутри служебный файбер с циклом асинхронного ожидания комбинации фьюч запроса и таймаута с перепланированием при неудаче

Поскольку Retry-и могут продолжаться вечно, например, при смерти целевого узла, также необходим механизм Cancellation-а для прерывания запроса по сбору кворума. В коде это может быть *context* или *stop token*, передаваемый в RPC запрос и используемый в конце scope-а, при смерти корутины/файбера или вручную. Главное использовать таймауты (или иметь аналогичные механизмы отмены) в асинхронных системных вызовах, чтобы помимо ресурсов программы можно было освободить ресурсы ОС

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 2](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-2.md)
[Лекция 3 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-3.md)
