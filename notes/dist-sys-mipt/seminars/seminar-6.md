## Seminar 6. TrueTime. Снимки в LSM

TrueTime

Вернёмся к алгоритму ABD - репликации диапазона ячеек памяти. Для устранения лишней фазы в его операциях нужно уметь выбирать глобально монотонные timestamp-ы. Локальные часы ненадёжны, поэтому рассмотрим, как можно использовать TrueTime без коммуникации реплик так, чтобы для real-time ordered записей w1 < w2 => ts(w1) < ts(w2). В реальности коммуникация идёт асинхронно и опосредованно через Тайм-мастеров

Не нужно думать о двух операциях, своей и чужой, и как их упорядочить по логическим часам. Достаточно думать лишь о своей, поскольку по определению линеаризации нужно выбрать ts в пределах интервала записи. Но мы не можем выбирать, ведь начало записи нам неизвестно, а TT даёт конкретные ts ([e, l]). Поэтому сделаем наоборот - выберем такое время окончания операции, чтобы туда попала известная нам метка

Для выполнения операции вызовем TT.Now() -> [e, l] и возьмём ts. e - не подходит, поскольку неизвестен его порядок с началом операции. Остаётся l, и гарантированно t1 <= tt <= l, которого нужно дождаться - Commit Wait (Spanner). Нельзя делать SleepFor(l - e) из-за skew/drift. Нужно спать в цикле, дожидаясь l <= TT.Now().e. Как только этот момент наступил, можно начать собирать кворум

Это имеет смысл делать лишь в геораспределённых системах, поскольку в пределах датацентров быстрее сходить по сети

Мы дожидаемся l, чтобы использовать этот ts в записи. В действительности можно ждать и собирать кворум параллельно, поскольку RT-ordering обеспечивается тем, что об операции сообщают по факту её окончания, когда оба события произошли. Так, любая последующая RT-o операция получит l' > l

TODO: приведённая схема не работает, где-то будет нарушаться линеаризуемость. В чём проблема?

LSM

Оптимизируем код реплики, которая хранит данные на disk. Координатор приходит в хранилище с Update(k, v, ts): атомарно проверить, что на данном узле ts ключа меньше, в случае чего выполнить перезапись. Отдельные read-check-write не сработают. Первое решение - взять mutex. Крайне неэффективно, поскольку мы сериализуем работу с разными, независимыми ключами, а в самой распр. системе уже есть конкурентность операций read/write. Второй вариант - шардировать mutex по ключам, например по остатку от деления на 128

Snapshot - иммутабельный снимок текущего состояния распределённой системы, который не меняется от последующих изменений в системе. Непонятно как создаётся даже когда в системе ~100GB распределённых данных. Будем строить его для некоторых операций
Интерфейс LSM
- Put(k, v)
- Delete(k)
- Get(k)
- Snapshot()
Структура LSM - Log + SSTable + MemTable, где SSTable - иммутабельное хранилище большей части данных. Сделаем хранилище мультиверсионным, добавив к ключу Sequence Number s. Он глобальный на всю систему и увеличивается при каждой операции Put (в случае banch-а увеличивается на его размер). Взять Snapshot означает запомнить Sequence Number. Get(k, s = current()) начинает принимать Sequence Number аргументом

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 6](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-6.md)
[Лекция 7 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-7.md)
