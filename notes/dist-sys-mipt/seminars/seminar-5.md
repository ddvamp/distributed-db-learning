## Seminar 5. RPC-фреймворк

#### Мотивация использовать RPC

Вспомним KV Store и алгоритм репликации ячейки из лекции $2$. Есть узлы системы и клиенты. Когда клиент посылает запрос, он попадает на некоторый узел, и тот берёт активную роль Координатора - инициирует коммуникацию и собирает два кворума, на чтение $ts$ и запись значения. Узел, который становится участником кворума, берёт пассивную роль Реплика - представляет копию ячейки и выполняет команды Координатора. При этом узел может брать обе роли сразу

Обе роли можно реализовать на коллбэках с обработкой в едином месте кода. Однако, если заметить, что ожидаемые ролями сообщения и производимые ими реакции различны, удобнее обработку разделить, и реализовать роли в виде двух интерфейсов, каждый со своим API

По модели, координатор общается с репликами через отправку асинхронных сообщений, но в алгоритме это общение структурировано в виде запрос-ответ. Т.е. в действительности имеем модель клиент-сервер. Координатор является сервером для внешних пользователей и клиентом для Реплик

Так, из-за разделения ролей и модели клиент-сервер в коде неудобно использовать асинхронные вызовы и коллбеки. Больше подходит **RPC**

#### Описание RPC

*Верхний уровень.* **RPC (Remote Procedure Call)** позволяет скрыть детали сети и рассуждать в терминах объектов. С RPC сетевой запрос начинает выглядеть как локальный вызов функции, который возвращает результат исполнения на удалённой машине. Для этого нужны *два представления* одного объекта
- **Service** - удалённый объект-исполнитель
- **Stub** - локальный объект-представление

которые реализуют один интерфейс, наследуя общий базовый класс. Клиент в коде вызывает метод Stub-а, и после на сервере RPC-фреймворк вызывает тот же метод Service-а с теми же аргументами, что и клиент. Для этого метод и аргументы сериализуются и посылаются сетевым запросом. Stub/Service есть верхний уровень всей схемы, который оперирует понятиями метод, аргументы и типы. Заметим, что и на Stub-е, и на Service-е мы хотим осуществлять вызовы конкурентно, поэтому Stub/Service потокобезопасны. Однако на этом уровне понятия конкурентности нет, и синхронизация осуществляется уровнем ниже

*Уровень транспорта.* Взаимодействие Stub-Service осуществляется через двухстороннюю отправку сообщений по сети. Их доставкой занимается нижний уровень Transport, обязательный для любого RPC-фреймворка. Он является единым для всех RPC сервисов. На этом уровне сообщения есть нетипизированные строки байт. Нет ни семантики отдельных сообщений (методы, аргументы,...), ни связи запрос-ответ. Транспорт предоставляет *два механизма (метода в интерфейсе)* для установления двустороннего общения
- **Connect** - для установления клиентом соединения по адресу
- **Serve** - для прослушивания сервером порта

Формат адреса/порта зависит от реализации, и это не обязательно числа. Метод Connect возвращает интерфейс-сокет, через который можно асинхронно посылать сообщения, и регистрирует обработчик. Обработчик позволяет обрабатывать входящие сообщения, передаваемые в его метод вместе с сокетом обратной записи, а также обрабатывать разрыв соединения. Метод Serve возвращает интерфейс-сервер и также регистрирует обработчик. Реализация Transport может использовать любой протокол. TCP, HTTP, свой протокол, например эмуляция TCP без обращений в сеть

*Уровень канала.* Между внешним слоем и транспортом существует промежуточный уровень - пара Channel/RPC Server. Channel пользуется интерфейс-сокетом, RPC Server - интерфейс-сервером. На этом уровне выполняется синхронизация, и появляется concurrency. Со стороны клиента это возможность конкурентных обращений к Stub/записей в канал и Future<Message> + Await для ответа (Message a.k.a. std::string). Со стороны сервера это возможность конкурентно обрабатывать многих клиентов, т.е. Concurrent Handlers (Fibers/Coroutines + Thread Pool). Также канал наделяет сообщения семантикой request/response, на стороне клиента через отслеживание активных запросов (RequestId $` \to `$ ActiveRequest), и на стороне сервера напрямую через обработку запросов и отправку ответов

Для удобства Channel и RPC Server реализуют интерфейс обработчика, чтобы их можно было зарегестрировать в методах транспорта. Так в Channel конкурентно приходят запросы от многих Stub-ов и ответы на них из сети. Поэтому внутри Channel удобно сериализовать все обращения к нему через Strand

#### Примерная схема запроса

Вызывается метод Stub-а с набором аргументов. Типы стираются, и переданные значения сериализуются в Message. Stab вызывает метод Call у Channel с передачей Method (service name + method name) и Message, получая Future<Message> на результат (или свободный метод Call с кастомизацией вызова, который обращается к Channel внутри). Channel формирует структуру request, сериализует её и пишет в интерфейс-сокет. Транспорт доставляет сообщение RPC Server-у, который десериализует его в структуру request, определяет сервис и метод, находит сервис и вызывает у него метод Invoke, где в зависимости от метода десериализуются аргументы. Метод Invoke выполняет вызов нужного метода и возвращает сериализованным его результат, который в итоге становится доступен через Future, откуда его получает Stub, десериализует и возвращает наружу

#### Сериализация, транспорт, дедлайны

**Protobuf** - протокол сериализации, позволяющий, чтобы участвующие стороны были написаны на разных языках. Использует **varint encoding** - кодирует не типы, а значения, тратя на них столько байт, сколько они занимают. Альтернативой ему является **zero-copy buffers**, которые используются например в **Cap'n Proto**. Суть в том, чтобы класть в сеть ровно то, что содержится в байтах для сериализации. Трейдоф - количество данных, посылаемых в сеть не уменьшить

Транспорт должен быть производительным, т.е. большое количество сообщений в обе стороны должно быть упаковано в малое количество TCP соединений. Также нужна кроссплатформенность языков. Этими свойствами обладает **HTTP/2**. HTTP/2 для TCP всё равно что fiber для thread, буквально. Файберы суть упаковка маленьких задач в потоки под тяжеловесным управлением ОС. Поверх логического TCP соединения есть много независимых конкурентных потоков данных из отдельных фреймов уровня HTTP/2. Протокол склеивает все фреймы в один поток байт, поэтому flow control должен быть обеспечен для каждого потока фреймов в отдельности. Удобно использовать HTTP/2 как транспорт в RPC, где отдельные вызовы будут упаковываться во фреймы

Как правильно реализовывать deadline/timeout на RPC вызов - добавить его к запросу и отправить на сервис, который этим будет заниматься

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 5](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-5.md)
[Лекция 6 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-6.md)
