## Seminar 7. CASPaxos

#### Замечания по RPC при сборе кворумов

В строгой реализации сбора кворума, мы собираем кворум фьюч, предусматривая случай максимального числа отказов. Оставшиеся фьючи, если есть, отменяются, чтобы уровень канала не повторял бесконечно запросы к недоступным узлам. Если хоть одна из фьюч содержит $nack$, кворум неудачен

*Эффективнее* при наличии отказов будет подождать ещё некоторое время, надеясь на дополнительные положительные ответы. Это можно реализовать через комбинатор с таймером. Альтернативный вариант, сделать менее сильную абстракцию канала, котора сообщает о разрыве сама или по таймеру, и добавить несколько ретраев каждой фьюче

#### Виртуальный лог Delos

В распределённых системах часто выделяют *две плоскости*
- **Data plane** - компоненты системы, задеваемые на пути чтения/записи. Это обработка запросов, репликация, транзакции и хранение. Работает постоянно
- **Control plane** - компоненты системы, отвечающие за её (само)управление. Это управление таблетками, мониторинг состояния узлов, балансировка нагрузки, хранение метаданных любых сущностей. Работает относительно редко. Частично затрагивается на медленном пути операций

Консенсус нужен не для упорядочивания команд, а для организации смены лидеров. Control plane занимается консенсусом, когда как data plane ответственен за репликацию. И если в Raft плоскости не разделимы, то в Paxos их можно выразить независимыми частями кода

`Facebook` в `Delos`(сервис координации) в реализации RSM ввёл дополнительную абстракцию - **Virtual Consensus**, которая состоит из двух частей, представляющих плоскости
- **VirtualLog** - представление лога для RSM
- **Loglet** - кусочек виртуального лога

Loglet отвечает за упорядочивание команд, его можно читать/писать и запечатать. В нём нет консенсуса, только репликация. Когда лидер переизбирается, Loglet запечатывается, и его можно, например, поместить в холодное хранилище. Старому лидеру же говорят прекратить работать с Loglet-ом. Более того, астракция Loglet-а позволяет на лету менять его реализацию, а значит и поведение всего лога. Например, поменять размер alpha-окна или в принципе весь алгоритм репликации

VirtualLog используется в представлении control plane - **MetaStore**. Он выстраивается из Loglet-ов, склеивая их в одну цепочку. В VirtualLog-е находится единственный в системе консенсус (MultiPaxos), а работа осуществляется при достаточно редкой смене Loglet-ов

Используемый в логе Multi-Paxos есть версия без лидеров и оптимизаций с консенсусом в каждом слоте (см. первую половину [лекции 6](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-6.md#построение-лога)). При этом MetaStore представляет собой версионированный регистр с операцией $CAS$, в которой хранится текущий Loglet, и его можно реализовать даже через Single-Decree Paxos, что и рассмотрим далее

#### CAS через Single-Decree Paxos

Рассмотрим произвольное исполнение алгоритма Paxos. В нём возникают различные $n$. Свяжем их в орграф следующим образом. Узел $0$ является корнем. Успешная фаза $Prepare$ с номером $n_k$, что получила голос $` \left \lparen n_a, v_a \right \rparen `$, порождает узел $n_k$ и ребро $` n_a \to n_k `$. И выделим все узлы, для которых фаза $Accept$ также была успешна. Ещё можно для только что начавшейся фазы $Prepare$ представить всевозможные исходы в текущей конфигурации и обозначить для каждого пунктиром виртуальные узел и ребро

*Утверждение.* Все выделенные узлы имеют один выделенный корень. Доказательство аналогично оному для свойства agreement в алгоритме Paxos (см. [лекцию 5](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-5.md#доказательство-свойств)). Какой бы выделенный узел мы не взяли, возвращаясь назад, мы всегда вернёмся к узлу первой успешной фазы $Accept$

Так первый выделенный узел "отрезает" прочие ветви, берущие начало от его предков. И ещё этот узел ознаменует выбор значения. Заметим, что граф не зависит от значений $v$. Тогда в фазе $Accept$ можно передавать произвольное значение и даже результат произвольной операции над значением, полученным из $Prepare$. Введём следующие *три операции*
- **initialize:** $` x \to `$ if $` x = \emptyset `$ then $` \left \lparen 0, val_0 \right \rparen `$ else $x$
- **update:** $` x \to `$ if $` x = \left \lparen k, * \right \rparen `$ then $` \left \lparen k + 1, val_1 \right \rparen `$ else $x$
- **read:** $` x \to x `$

Значение, хранящееся в регистре, снабжено версией $k$, т.е. является составным. $x$ в каждой операции есть результат успешной фазы $Prepare$, т.е. значение из $Vote$ с наибольшим $n_a$. В случае неудачи второй фазы $update$, в котором обновление может быть применено постфактум (благодаря helping-у в алгоритме Paxos), необходимо повторять операцию $read$, пока та не будет успешной. После этого предшествующие операции гарантированно завершены. Для того, чтобы далее узнать результат $update$, на репликах нужно хранить историю всех изменений, из которой и определить результат

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 9](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-9.md)
[Лекция 10 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-10.md)
