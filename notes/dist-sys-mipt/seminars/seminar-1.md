## Seminar 1. Среда исполнения распределённой системы

**Геораспределённая система** - система, узлы которой расположены на большом расстоянии друг от друга, в масштабе земного шара

**Leap second (добавочная секунда)** - поправка ко времени из-за неравномерного вращения Земли. В мире существует несколько временных осей, которые возникли из-за leap second
- **TAI** - время без добавочных секунд
- **GPS** - время в одноимённой системе. Учитывает добавочные секунды до момента запуска
- **UTC** - "настоящее" время. Включает все добавочные секунды

Unix time не монотонно, как из-за наличия leap seconds, так и в результате синхронизация с GPS (протокол *NTP*). Поэтому возможно, что в следующую секунду будет тот же timestamp, либо даже меньший. Нельзя в распределённых системах полагаться на монотонность течения "общего" времени (по временным осям). Поэтому в компьютерах существует *двое часов*
- **wall clock (настенные, календарные)**, `std::system_clock` - имеют единую точку отсчёта на разных узлах, но не монотонны. Могут быть использованы для сравнения времени на разных узлах
- **monotonic clock**, `std::steady_clock` - не имеют единой точки отсчёта, но монотонны (например, монотонность устанавливается от запуска системы). Позволяют измерять диапазоны локально

Независимо от вида часов, они подвержены дрифту. Их различие в наличии/отсутствии корректировки времени

Узлы (сервера) собираются в *стойки (rack)* под объединением *коммутатора (top-of-rack/ToR switch)*, стойки собираются в *кластера (cluster)* под объединением *кластерного коммутатора (cluster switch)*, кластера собираются в *датацентры*, датацентры собираются в геораспределённую сеть. В правильно организованной инфраструктуре отсутствуют **single point of failure (SPF, единые точки отказов)** - точки, отказ в которых приводит к падению всей системы. Для этого необходимо размещать части системы (например, реплики) в разных доменах отказа. **Failure domain (домен отказа)** - часть системы, выходящая из строя из-за одного локального отказа. Диск $\to$ узел, коммутатор $\to$ стойка, коммутатор $\to$ кластер. Необходимо применять *rack awareness* - размещать реплики в разных стойках. Также следует стремиться уменьшать количество самих точек отказа, напрмер, через *избыточность* (запасные коммутаторы, несколько путей в сети между двумя узлами, raid-массивы и т.д.). Вариант уменьшения рисков отказа - резервирование коммутаторов от разных производителей (разные баги и условия выхода из строя). Но в месте с ростом надёжности увеличивается расстояние между узлами и издержки, поэтому растёт latency, так что важно находить балланс. Пример: фабрики Google или Facebook

**Недоутилизация** - ситуация простаивания избыточных мощностей/ресурсов, имеющихся с целью пережить пик нагрузки и для отказоустойчивости. Возникает также, когда пользователи по географическому признаку распределяются по частям системы, которые становятся активными лишь в определённое время. Именно поэтому вместо большого количества маленьких кластеров под отдельные сервисы стали строить один большой кластер на все сервисы сразу. В нём должна быть избыточность маршрутов (для предотвращения потери связности), высокая пропускная способность любого разреза, а также он должен легко масштабироваться без необходимости использовать специальное оборудование (должно быть достаточно того, что есть на рынке)

**Vendor lock** - зависимость от производителя, когда определённым товаром или товаром с конкретными свойствами торгует единственный производитель. Также возникает при экстримальном вертикальном масштабировании (процессор с самым большим числом ядер, самый вместительный диск и т.д.). Уменьшает надёжность системы

#### Почему между датацентрами на разных концах мира есть задержки

Идеал - скорость света. Материал оптоволокна неидеален, преломления, затухания сигнала, точки сварки, коррекция ошибок, разрыв кратчайших путей, соображения надёжности (не прямой путь), экономические соображения (провода к промежуточной точке в стороне, постройка датацентра и аренда кабеля вместо своего кабеля), географические особенности (реки, населённые пункты, существующие ж/д пути)

TCP является распределённой системой, и эта абстракция соединения (потока байт) существует лишь на уровне ОС двух участников (TCP стек находится в ядре, и ОС управляет соединениями, не на уровне сети/проводов/коммутаторов/маршрутизаторов, IP работает с Datagrams), т.е. TCP не имеет физического представления. Проблема TCP в том, что абстракция прямого провода (машина-машина) не соответствует физической действительности (в сети много пар машин, которые пользуются проводами сообща), поэтому нельзя писать сколько захочешь: перегрузка сервера, перегрузка сети, о которых не знают TCP концы (*Flow control*, *Congestion control*). Например, для отправки пакетов существует окно подтверждений, которые ещё не получены, и это окно при получении таймаута уменьшается в $2$ раза. Также TCP подвержен проблеме синхронизации часов: при гибели процесса ОС закроет соединение и пошлёт сигнал fin; если на другом конце машина перегрузится, она ответит, что не знает нас (соединение разорвано); если машина упадёт, мы будем сперва успешно писать в буфер, затем при его заполнении долго ждать отправки с ретрансмитами до таймаута. Так нельзя понять, что происходит в остальном мире прямо сейчас

TCP медленный, т.е. из-за flow и congestion control он начинает с маленьких скоростей прощупывать сеть и разгоняется

В действительности существует множество десятков версий протокола TCP

Современные распределённые системы начинают работать в облаках, поэтому для высокой производительности должны учитывать виртуализацию, контейнеры, планировщики кластеров и разделение ресурсов, и т.п.

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 1](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-1.md)
[Лекция 2 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-2.md)
