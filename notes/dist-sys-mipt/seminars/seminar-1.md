## Seminar 1. Среда исполнения распределённой системы

**Геораспределённая система** - система, узлы которой расположены на большом расстоянии друг от друга в масштабе земного шара

#### О времени и часах

**Leap second (добавочная секунда)** - поправка ко времени из-за неравномерного вращения Земли. В мире существует несколько временных осей, которые возникли из-за leap second
- **TAI** - время без добавочных секунд
- **GPS** - время в одноимённой системе. Учитывает добавочные секунды на момент запуска (19)
- **UTC** - "настоящее" время. Включает все добавочные секунды

В распределённых системах нельзя полагаться на монотонность течения "общего" времени (по временным осям). Unix time не монотонно, как из-за наличия leap seconds, так и в результате синхронизация с GPS (протокол *NTP*). Возможно, что в следующую секунду timestamp будет тот же или даже меньший. Поэтому в компьютерах существует *двое часов*
- **wall clock (настенные, календарные)**, `std::system_clock` - имеют единую точку отсчёта на разных узлах, но не монотонны. Могут быть использованы для сравнения времени на разных узлах
- **monotonic clock**, `std::steady_clock` - не имеют единой точки отсчёта, но монотонны (например, монотонность устанавливается от запуска системы). Позволяют измерять диапазоны локально (таймеры, профилирование)

Независимо от вида часов, они подвержены дрифту. Их различие в наличии/отсутствии корректировки времени

**Leap Smear** - решение `Google` на уровне аппаратуры для устранения немонотонности времени

#### О датацентрах

Узлы (сервера) собираются в *стойки (rack)* под объединением *коммутаторов (top-of-rack/ToR switch)*, стойки собираются в *кластера (cluster)* под объединением *кластерных коммутаторов (cluster switch)*, кластера собираются в *зоны доступности (availability zone)*, например, датацентры, зоны собираются в *регионы (region)*, регионы собираются*геораспределённую сеть*

**Availability zone** - часть системы, которая не делит важные инфраструктурные компоненты (вычислительные мощности, охлаждение, электропитание и т.д.) с другими зонами. Обычно локально удалены друг от друга

**Failure domain (домен отказа)** - часть системы, выходящая из строя из-за одного локального отказа. Каждый уровень в иерархии выше является доменом отказа. Диск $\to$ узел, коммутатор $\to$ стойка, коммутатор $\to$ кластер

**Коррелированный отказ** - одновременный выход из строя части системы из-за отказа некоторого компонента и попадания в его домен отказа. Чем выше уровень иерархии, тем ниже вероятность коррелированного отказа по нему, но тем больше последствия

**Single point of failure (SPF, единая точка отказа)** - точка, отказ в которой приводит к падению всей системы целиком. В этом случае вся система расположена в одном домене отказа

Отказоустойчивость, в первую очередь, достигается за счёт аппаратуры. В правильно организованной инфраструктуре не должно быть единых точек отказа путём разнесения системы по разным доменам отказа. Необходимо сводить к минимуму количество коррелированных отказов, например, применять *rack awareness* - размещать реплики в разных стойках, чтобы replica set не попала в одну стойку. Также следует стремиться уменьшать количество самих точек отказа, напрмер, через *избыточность* (запасные коммутаторы, несколько путей в сети между двумя узлами, raid-массивы и т.д.). Вариант уменьшения рисков отказа - резервирование оборудования от разных производителей (разные баги и условия выхода из строя)

Существует несколько уровней доменов отказа, и чем он выше, тем сложнее обеспечивать отказоустойчивость. Вместе с ростом надёжности увеличивается расстояние между узлами и издержки, поэтому растёт latency, так что важно находить балланс. Пример: фабрики `Google` или `Facebook`

**Недоутилизация** - простаивание избыточных мощностей/ресурсов, имеющихся с целью пережить пик нагрузки и для отказоустойчивости. Возникает также, когда пользователи по географическому признаку распределяются по частям системы, которые становятся активными лишь в определённое время. Именно поэтому вместо большого количества маленьких кластеров под отдельные сервисы стали строить один большой кластер на все сервисы сразу. В нём должна быть избыточность маршрутов (для предотвращения потери связности), высокая пропускная способность любого разреза, а также он должен легко масштабироваться без необходимости использовать специальное оборудование (должно быть достаточно того, что есть на рынке)

**Vendor lock** - зависимость от производителя, когда определённым товаром или товаром с конкретными свойствами торгует единственный производитель. Возникает при экстримальном вертикальном масштабировании (процессор с самым большим числом ядер, самый вместительный диск и т.д.). Уменьшает надёжность системы

#### Почему между датацентрами на разных концах мира есть задержки

Идеал - скорость света. Неидеальный материал оптоволокна, преломление света, затухания сигнала, точки сварки соединений проводов и повреждений, влияние окружающей среды, коррекция ошибок, разрыв кратчайших путей, соображения надёжности (не прямой путь), экономические соображения (провода к промежуточной точке в стороне, постройка датацентра и аренда кабеля вместо своего кабеля), географические особенности (реки, населённые пункты, существующие ж/д пути). Всё это снижает скорость передачи сообщений

И хотя в реальности задержки большие, TCP скрывает это

#### Слова о TCP

TCP как абстракция соединения (потока байт) является хорошим примером reliable канала. Также TCP является распределённой системой и существует лишь на уровне ОС двух участников (TCP стек находится в ядре, и ОС управляет соединениями). Т.е. TCP не имеет физического представления (на уровне сети/проводов/коммутаторов/маршрутизаторов, IP работает с Datagrams). В действительности существует множество десятков версий протокола TCP

Проблема TCP в том, что абстракция прямого провода (машина-машина) не соответствует физической действительности. В сети много пар машин, которые пользуются проводами сообща, и если позволить TCP писать неограниченно, он может перегрузить сеть и/или сервера не зная об этом. Механизмы *Flow control*, *Congestion control* призваны предотвратить это. Например, для отправки пакетов существует окно подтверждений, которые ещё не получены, и это окно при получении таймаута уменьшается в $2$ раза

Также TCP подвержен проблеме синхронизации часов. Если на другом конце умрёт процесс, то ОС закроет соединение и пошлёт сигнал fin (всё хорошо). Если машина перезагрузится, при получении сообщения она ответит, что не знает нас (соединение разорвано). Если же машина упадёт, мы будем сперва успешно писать в буфер до его заполнения и затем до таймаута долго ждать отправки с ретрансмитами. Т.о. нельзя понять, что происходит в остальном мире прямо сейчас (отличить сбой узла от проблем сети), и это нужно учитывать при написании кода

TCP медленный, т.е. из-за flow и congestion control он начинает с маленьких скоростей прощупывать сеть и разгоняется

Современные распределённые системы начинают работать в облаках, поэтому для высокой производительности должны учитывать виртуализацию, контейнеры, планировщики кластеров и разделение ресурсов, и т.п.

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 1](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-1.md)
[Лекция 2 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-2.md)
