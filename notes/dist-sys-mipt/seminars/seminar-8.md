## Seminar 8. Crash consistency

Напомним из чего состоит отказоустойчивость с точки зрения возможных *нарушений*
- **Отказы.** Необходимо строить систему в предположении, что произвольный узел внезапно может перестать посылать сообщения, через долгое время может вернуться и даже действовать будто ничего не произошло
- **Злоумышленники.** Узел или группа узлов могут пытаться нарушить протоколы общения системы, о чём поговорим в последней части курса
- **Рестарты.** Есть проблема **Crash Consistency** - как пережить рестарт с точки зрения самого узла, то есть гарантировать согласованность хранимых данных

Рассмотрим как обеспечить crash consistency, детали работы с файловой системой диска и ОС. Хотим обеспечить выполнение *AD (ACID)*
- **Durability** - результаты подтверждённой операции не исчезнут после рестарта
- **Atomicity** - операция, прерванная рестартом, либо завершится после него, либо полностью откатится

#### Персистентный лог

Нюансы реализации описаны в [лекции 8](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-8.md#rsm-в-проде)

#### Crash-consistent pwrite

Рассмотрим как перезаписать на диске произвольный фрагмент файла. Для этого существует syscall $pwrite$, оставляющий размер файла неизменным
- *Durability.* По окончанию $pwrite$ не гарантируется, что изменения попали на диск. Они вполне могли оказаться в кэше ОС или в кэше устройства хранения, которые сбрасывается вызовом $fsync$ или $fdatasync$. На некоторых ядрах и фс эти вызовы не умеют сбрасывать кэш устройства, так что его нужно отключать. Также $fsync$ может возвращать ошибки, и в первом приближении здесь нужно упасть
- *Atomicity.* Данные пишутся на диск блоками. И в диске, и с точки зрения ОС, запись одного блока является атомарным действием, но запись нескольких - нет. Поэтому рестарт во время записи в конец файла, что может потребовать изменений метаданных, во время перезаписи большой части файла, а также во время многих других операций может привести к неконсистентному состоянию

Научимся откатывать частичные следы $pwrite$ при помощи служебного **Undolog**. Перед операцией создадим лог и запишем в него "обрантую" нашей команду, т.е. $pwrite$ с исходными данными, а после окончания лог удалим через syscall $unlink$ - удалить файл. Если случится рестарт, система увидит лог, применит записанную в нём команду и восстановит данные

Лог также должен создаваться с учётом рестартов. Запись в логе состоит из заголовка и самой команды. Заголовок содержит магическое число, размер команды и checksum. Магическое число находится в начале записи и предназначено для быстрой проверки, была ли запись завершена или нет. Checksum используется для проверки корректности записанной команды при восстановлении. После создания нужно использовать $fsync$ как для лога, так и для его директории, а также $fsync$ директории после удаления. Так частично созданный лог проигнорируется, а корректный можно применить многократно

И есть ещё много открытых вопросов. Как откатывать другие операции, действительно ли undolog создаётся на одну операцию, как делать докатывание операции и др.

#### Checksum

Данные в программах дискретны, однако физический мир непрерывен. Записывая и считывая данные мы ожидаем, что хранилище является identity функцией, то есть не меняет данные при прямом-обратном преобразовании. Однако это не так, поскольку возможно множество различных ситуаций от особенностей реального мира, нарушающих работу хранилища. Например космические лучи, заряжающие конденсаторы в памяти и переворачивающие тем самым биты. На этот случай применяют коды коррекции ошибок. В дисках подобного механизма проверки валидности записанных данных нет. Из-за этого повсеместно применяются **checksum**

Можно создать фоновый поток на реплике, который периодически сравнивает сохранённые данные с checksum, и в случае несоответствия сообщает об этом лидеру, который заменяет реплику

#### Сеть

При использовании RPC сообщение проходит весь стек TCP/IP и упаковывается в TCP segment в IP datagram в Ethernet frame, каждый из которых имеет checksum или CRC для обратного преобразования. Однако их недостаточно, нужны проверки и на уровне Application. Нужно реализовывать checksum на уровне RPC (дополнительное поле в структуре request с checksum всего запроса)

На уровне Ethernet frame есть 32-bit CRC, которые используются для отслеживания ошибок в проводах. Однако элементы сети, например, маршрутизаторы, при пересылке фреймов могут их редактировать и пересчитывать CRC. Если элемент неисправен, CRC может быть вычислен неправильно, либо сами данные могут быть испорчены

IP datagram содержит checksum лишь для заголовка, проверки передаваемых данных нет. IP протокол отвечает за маршрутизацию, не транспортировку

В TCP checksum является крайне слабой и ненадёжной. Условно это сумма 16-bit кусочков, и переворот одинакового бита в двух кусочках не изменит результата. Более того, сам протокол TCP может быть реализован с багами

И вот Ethernet сам породил ошибку в данных, IP проверять не собирается, а TCP её не увидел. И сообщение было испорчено

#### Две оптимизации хранения данных

1. Не стоит для каждой таблетки узла заводить отдельный Log в LSM, которых будет очень много, ровно как и параллельных операций работы с ними всеми. Следует хранить все логические логи в одном **Tablet Log**
2. Если вся система состоит из одних таблеток, стоит выстраивать её поверх слоя распределённого хранилища иммутабельных логов, как, например, сделано в `YDB`

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 11](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-11.md)
