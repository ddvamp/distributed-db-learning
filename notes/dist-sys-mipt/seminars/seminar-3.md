## Seminar 3. Local Storage, LSM и B+, HDD и SSD

Чтобы завершить репликацию данных объёма одной машины, необходимо научиться эти данные на машине хранить и эффективно ими управлять. Нужен **Local Storage**

Будем разрабатывать **Storage Engine** - библиотеку для узлов распределённых систем разных классов. Хотим следующее API с произвольным доступом к ключам поверх медленного последовательного диска
  - $Put \left \lparen k, v \right \rparen $
  - $Get \left \lparen k \right \rparen $
  - $Delete \left \lparen k \right \rparen $
  - $` ScanRange \left \lparen l, h \right \rparen `$ - возвращает итератор на отсортированный диапазон ключей. Используется в слоях транзакций и SQL

Ожидания при использовании LS в распределённой базе данных
- данные не помещаются в память и заполняют диск узла целиком
- данные должны переживать рестарты узла
- данные должны храниться упорядоченно
- должны обеспечиваться гарантии AD из ACID
  - **Atomicity** - операции, заставшие рестарт, либо подтверждаются, либо полностью откатываются. Частичные следы в хранилище невозможны
  - **Durability** - результат подтверждённых системой операций не теряется при рестартах
- операции должны быть максимально быстрыми по модулю устройства хранения

Архитектура и эффективность движка зависят от используемого устройства хранения. Их два основных вида с разными устройством и моделями стоимостей
- **HDD** - крутящийся диск
- **SSD** - твёрдотельный накопитель

#### HDD

Жёсткий диск это механическая и самая ненадёжная часть системы. Он имеет следующее строение: диск $` \to `$ блины $` \to `$ дорожки $` \to `$ сектора. Сектор, как и кэш линия в памяти, суть единица чтения/записи в диске размером $512-4096B$. Для обращения к сектору нужно позиционирование на него. Оно занимает $` \sim 5-10 ms `$ и состоит из **seek** (смена трека/поворот иглы) и **rotational latency** (поворот диска). Если seek не успевает за поворотом диска, понадобится ещё один круг. Случайные доступы позволяют прочесть $` \sim 50-100 KB `$ в секунду, когда последовательное чтение в $` \sim 100 `$ раз быстрее, поэтому для эффективной работы с диском необходимо учитывать его геометрию и избегать произвольного доступа

#### Классические структуры

Хотим иметь упорядоченный контейнер и избежать произвольного доступа в диске. Наивный подход, не избегаем и строим *RB-tree*. Для работы с ним отображаем необходимые страницы в память (mmap), после сбрасываем (fsync). Не будет хорошо работать из-за слишком большой глубины и большого числа прыжков (= seek-ов)

Ускоряем при помощи *B+-tree* за счёт широких узлов с ключами и большей степени ветвления, т.е. меньшего числа переходов/прыжков. Промежуточные узлы с ключами скорее всего поместятся в память (кэш файловой системы), поэтому поиск и изменения структуры не будут затрагивать диск, кроме моментов синхронизации. Чтобы не терять не сброшенные на диск обновления ключей, используем **Write-Ahead Log (WAL)** - лог изменений на диске. Сперва пишем в WAL, затем обновляем данные в памяти. При рестарте восстанавливаем состояние из WAL. Для сканирования диапазона ключей провяжем листья в список

#### Log-Structured Merge Tree

Альтернативное и более простое решение - **Log-Structured Merge Tree (LSM)**. Для его построения решим две подзадачи

**Первая подзадача**
- Immutable датасет (нет $Put$)
- быстрые $Get + ScanRange$
- Durability

Построим **Sorted String Table** (**SSTable**, всегда относится к LSM). Сколь угодно большой файл с данными отсортируем по ключам и мысленно поделим на блоки. Блок характеризуется оффсетом и начальным ключом. К файлу приложим **Index** - упорядоченное отображение ключа в оффсет блока, с которого ключ начинается. Размер блока выберем таким, чтобы индекс помещался в страничный кэш. $Get$ получает адрес блока из индекса и считывает его в память для поиска. $ScanRange$ считывает последовательность блоков. Можно использовать сжатие и ценой скорости чтения уменьшить занимаемый объём/увеличить количество данных

**Вторая подзадача**
- быстрые $Put$
- $Get$, скорость не важна
- Durability

Построим append-only **Log** в виде файла на диске. При $Put$ кладём данные в конец лога (с fsync). Записи можно объединять в пачки и сокращать число записей на диск. При $Get$ последовательно проходим по всему логу для поиска последнего значения

**LSM**
- $Get$
- $Put$
- Durability

Ограничим размер Log-а, чтобы его можно было дублировать в **MemTable** - упорядоченный контейнер в памяти, реализованный через *SkipList*. При $Put$ сперва пишем в Log, затем в MemTable. Когда MemTable переполнился, асинхронно дампим его в виде новой SSTable и очищаем вместе с Log-ом. При $Get$ сперва ищем в MemTable, и если там нет, во всех SSTable. Количество индексов и поисков по диску пропорционально числу SSTable, поэтому при появлении двух SSTable схожих размеров в фоне сливаем их в один. При $Delete$ вставляем специальное значение - **tombstone**

#### Технические нюансы LSM

*Проблемы реализации*
- ключ может храниться в нескольких SSTable
- $Get$ требует чтения из нескольких файлов
- частые чтения одних и тех же блоков (горячие блоки или частые ключи)

Для решения первой храним в наибольшем SSTable не менее $` 90\% `$ всех данных на диске, так что избыточность не более $` 10\% `$. Для решения второй для каждого SSTable заводим в памяти *Bloom Filter*. Для решения третьей используем **Block Cache** - храним в памяти популярные блоки, что достаточно просто в силу иммутабельности SSTable

*Является ли полученное решение последовательным*. Чтения осуществляют произвольный доступ лишь в памяти. Записи на диск бывают
- добавление в конец Log
- сброс MemTable
- слияние SSTable

все последовательные

*SkipList в MemTable*. Используется он, а не, например, RB-tree из-за concurrency. Обычно чтений сильно больше, чем записей, поэтому они должны быть параллельными и желательно не блокироваться записями. Деревья поиска так не умеют, со списком гораздо проще. Во-вторых, можно сделать так, чтобы в MemTable писал ровно один поток (для этого записи собираются, например, в конкурентной очереди), и использовать семантику acquire-release, что бесплатно на x86 (пример: LevelDB - каноническая реализация LSM, RocksDB - fork LevelDB от Facebook)

*Почему LSM, а не B+-tree*. В первую очередь из-за **Write Amplification** - отношение данных, физически записанных на устройство хранения, к данным, логически помещённым в базу (пара ключ + значение). Например, запись $100B$ данных потребует сбросить на диск как минимум одну страницу в $4096B$. Также судят по **Read Amplification** - количество прочтений диска для считывания одного значения (либо альтернативно, также отношение) и **Space Amplification** - отношение объёма данных на устройстве хранения к объёму логических данных в базе. У LSM лучше WA, но хуже RA, в сравнении с B+-tree. Также у LSM есть качественные open-source решения (`LevelDB`, `RocksDB`)

#### SSD

SSD суть диск, который имеет быстрые операции с произвольным доступом, чтение за $10us$, запись за $100us$, что в $` \sim 1000 `$ раз быстрее HDD. Он основан на *Flash Memory* со структурой cell $` \to `$ page ($4KB$) $` \to `$ block ($` \sim 100 `$ pages), и имеет следующие операции
- $` Read \left \lparen page \right \rparen `$ - прочесть страницу, $` \sim 100us`$
- $` Program \left \lparen page \right \rparen `$ - переписать страницу, но без возможности перевести $` 0 \to 1 `$. Каждый бит можно либо оставить как есть, либо перевести $` 1 \to 0 `$
- $` Erase \left \lparen block \right \rparen `$ - возможность перевести $` 0 \to 1 `$, но с перезаписью целого блока

Из-за такого API поверх FM есть *Flash Translation Layer*, целью которого служит стирать как можно реже ценой записи в случайные страницы. При записи возможно, что страница будет перезаписана в новую физическую в свободном блоке, а старая помечена для переиспользования. Возникает фрагментация, и нужна сборка мусора в фоне, что замедляет диск. При этом, чем больше случайных записей, тем быстрее фрагментируется диск

Ещё одна проблема SSD это ограниченное детерминированное число циклов перезаписи (P/E cycle) конкретного блока. Пусть FTL и распределяет логические страницы по блокам равномерно, если все реплики в replica set одновременно запущены на одинаковых SSD, все они откажут примерно в одно и то же время. Необходимо брать диски разного типа и от разных вендоров

Оба вида дисков имеют два типа адресации
- **логическая** - абстракция Block Device, представляет собой последовательность блоков фиксированного размера, в терминах которых диск предоставляет API
- **физическая** - реальная структура диска, в которую контроллер диска, например FTL, отображает логическую

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 3](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-3.md)
[Лекция 4 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-4.md)
