## Seminar 3. Local Storage, LSM и B+, HDD и SSD

Будет разрабатывать Storage Engine - библиотеку для узлов распр. системы
Хотим следующее API произвольного доступа поверх медленного последовательного диска
  - Put(k, v)
  - Get(k)
  - Delete(k)
  - ScanRange(l, h) - возвращает итератор на отсортированный диапазон ключей. Используется в слое транзакций KV хранилища
Поскольку хотим использовать LS в распр. базе данных, то
- данных много, они заполняют диск узла целиком и шардированы
- данные должны переживать рестарты
- данные должны храниться упорядоченно
- операции должны быть максимально быстрыми (по модулю устройства хранения)
- должны обеспечиваться гарантии AD из ACID (в хорошем LS)
  - Atomicity - если после начала записи, но до её окончания, система перезагрузилась, то запись либо откатилась, либо выполнилась целиком. Частичные следы в хранилище невозможны
  - Durability - если система подтвердила запись, её результат гарантируется (данные надёжно сохранены и не теряются при рестарте)

Как обеспечить AD? Насколько быстрые операции возможны?
Эффективность и архитектура движка зависят от устройства хранения, которых два основных вида с разным устройством и разными моделями стоимостей
- HDD - крутящийся диск
- SSD - твёрдотельный накопитель

HDD
диск -> блин -> дорожка -> сектор. Сектор суть единица чтения в 512-4096 байт. Время позиционирования seek (смена трека/поворот иглы) + rotational latency (поворот диска) ~5-10mc. Случайные доступы ~50-100KB в секунду, последовательное чтение в ~100 раз быстрее
Поскольку диск механический, это самая ненадёжная часть системы
Для эффективной работы с диском необходимо учитывать его геометрию и избегать произвольного доступа

Наивный подход: игнорируем устройство, полагаемся на случайный доступ и строим RB-tree. Для работы с ним отображаем необходимые страницы mmap-ом в память, после сбрасываем (fsync). Здесь слишком большая глубина и много прыжков

Ускоряем при помощи B+-tree: за счёт широких узлов с ключами и большей степени ветвления уменьшаем количество переходов/прыжков. Так промежуточные узлы с ключами скорее всего поместятся в память (или кэш файловой системы), и на диске будут лишь данные. Для сканирования данных провяжем листья в список. На случай рестартов и потери обновлений ключей в памяти используем WAL (Write-Ahead Log): перед модификацией дерева пишем в лог на диске

Альтернативное и более простое решение: LSM (Log-Structured Merge Tree)
Сперва решим подзадачу
- Immutable датасет (k, v)
- быстрые произвольные Get + ScanRange
- Durability
Построим SSTable (Sorted String Table, всегда относится к LSM). Сколь угодно большой файл с данными отсортируем по ключам и мысленно поделим на блоки. Блок характеризуется оффсетом и начальным ключом. Приложим к файлу Index, где по ключу хранится оффсет блока. Размер блока выберем таким, чтобы индекс помещался в память. Get читает адрес блока из индекса, затем считывает блок в память и ищет в нём. Можно использовать сжатие и ценой скорости чтения уменьшить занимаемый объём/увеличить количество данных

Вторая подзадача
- Put
- Get, скорость не важна
- Durability
Построим Log, append-only. При Put кладём данные в конец лога/файла (с fsync), к тому же их можно батчить. При Get последовательно проходимся по всему логу

Третья подзадача (финал)
- быстрые Get
- Put
- Durability
Дублируем Log в упорядоченный контейнер в памятии - MemTable (SkipList): Put = Put(log) + Put(memtable) (порядок важен на случай рестартов). При переполнении MemTable (ограниченный размер, или память кончилась) асинхронно дампим его в виде новой SSTable, очищаем Log и MemTable. При появлении двух SSTable схожих размеров в фоне сливаем их в одну (1 1 2 4 32 -> 16 32). При поиске идём в MemTable, затем по всем SSTable (?как правильно хранить все индексы). При удалении вставляем спец. значение: Delete(k) = Put(k, None)
Проблемы:
- избыточность: ключ может хранится много раз в разных SSTable
- один Get требует нескольких чтений SSTable-ов с диска
- частые чтения одних и тех же блоков (горячие блоки или частые ключи)
Решение первой: инвариант - храним в самом большом SSTable не менее 90% всех данных на диске
Решение второй: Bloom Filter в памяти для каждого SSTable
Решение третьей: Block Cache в памяти - храним там популярные блоки. Удобно в силу иммутабельности SSTable

Почему SkipList - из-за concurrency. Обычно чтений сильно больше, чем записей. Чтения должны быть параллельными и желательно не блокироваться записями. RB-tree так не умеет, со списком гораздо проще. Пусть в MemTable пишет ровно один поток (для этого записи собираются, например, в конкурентной очереди), тогда можно использовать семантику acquire-release, что бесплатно на x86 (Прим: LevelDB - каноническая реализация LSM, RocksDB - fork LevelDB от Facebook)

Почему LSM, не B+-tree - в первую очередь из-за Write Amplification
Write Amplification - отношение данных записанных на диск к логически записанным в структуру данным (ключ + значение). К примеру запись пары на 100 байт потребует как минимум сброс 4096 байт страницы. Аналогично высчитывается Read Amplification. Space Amplification - отношение задействованного пространства на диске к объёму логических данных. У LSM лучше WA, то есть экономнее пишут, но хуже RA по сравнению с B+-tree
Также у LSM (в отличие от B+-tree) есть качественные open-source решения (LevelDB, RocksDB)

SSD
Случайные доступы за 10us, запись за 100us. Просто возьмём этот диск? Нет, даже SSD работает лучше с последовательными записями
SSD суть диск поверх Flash Memory: cell -> page (4KB) -> block (~100 pages)
Операции
- Read(page) - прочесть страницу, ~10us
- Program(page) - перевести какой-то набор бит страницы 1 -> 0 (0 -> 1 невозможно)
- Erase(block) - перевести все биты блока в 1
Из-за такого API поверх FM есть Flash Translation Layer, целью которого служит стирать как можно реже ценой записи в случайные страницы. При записи в страницу возможно она пометится грязной и перепишется в страницу свободного блока. Нужна "сборка мусора", и возникают моменты дефрагментации, добавляющие фоновую работу, которая замедляет диск
Также проблема SSD в ограниченном числе циклов перезаписи Erase, и хотя FTL распределяет их равномерно, если replica set одновременно запущен на одинаковых дисках, они откажут примерно в одно и то же время. Необходимо брать диски разного типа и от разных вендоров

Оба вида дисков имеют два типа адресации
- логическая - абстракция Block Device, представляет собой последовательность блоков фиксированного размера, в терминах которых диск предоставляет API
- физическая - реальная структура диска, в которую контроллер (напр. FTL) диска отображает логическую

[↑ Содержание ↑](https://github.com/ddvamp/distributed-db-learning/tree/main/notes/dist-sys-mipt#содержание)\
[← Лекция 3](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-3.md)
[Лекция 4 →](https://github.com/ddvamp/distributed-db-learning/blob/main/notes/dist-sys-mipt/lectures/lecture-4.md)
